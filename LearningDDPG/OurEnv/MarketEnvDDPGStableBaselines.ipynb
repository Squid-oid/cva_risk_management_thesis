{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Environment\n",
    "[Is there some way to visualize our env in a good way? Overlay all the Swaptions and Qs as semi transparent red and blue then add CVA as solid?]\n",
    "\n",
    "Our environment is much like the Mountain Car environment in many key ways, it is also continous in input and output, it is an optimal control problem, it is markov. But there are a few key differences to catch. First of all we have an 18 dimensional action space, and a 37 dimensional input space, so it is a much larger space. Another difference to note is that our environment has a fixed termination date after 9 (10) years, so we should keep this in mind when we think about discounting and planning.\n",
    "\n",
    "#### Action/Obs Space \n",
    "##### Observation\n",
    "The observation is a `ndarray` with shape `(37,)` where the elements correspond to the following:\n",
    "|Num  |Observation                                                       |Min   |Max     |Unit              |\n",
    "|-----|------------------------------------------------------------------|------|--------|------------------|\n",
    "|0-8  |Fraction of portfolio value in Swaptions expiring in year 1 to 9  | 0    | 1      | float - fraction | \n",
    "|9-17 |Fraction of portfolio in defaulting after year 1 to 9             | 0    | 1      | float - fraction |\n",
    "|18-26|Swaptions expiring in year 1 to 9 with strike at Swap strike      | 0    | Inf*   | float - $ value  | \n",
    "|27-35|Probability of defaulting after year 1 to 9                       | 0    | 1      | float - $ value  |\n",
    "|36   |Current interest rate                                             | -Inf** | Inf**| float - %        |\n",
    "\n",
    "*In practice very small \\\n",
    "**In practice between around 10 to -3\n",
    "\n",
    "##### Action\n",
    "The action is a `ndarray` with shape `18` where the elements correspond to the following:\n",
    "|Num  |Action                                                            |Min   |Max    |Unit              |\n",
    "|-----|------------------------------------------------------------------|------|-------|------------------|\n",
    "|0-8  |Fraction of portfolio value in Swaptions expiring in year 1 to 9  | 0    | 1     | float - fraction | \n",
    "|9-17 |Fraction of portfolio in defaulting after year 1 to 9             | 0    | 1     | float - fraction |\n",
    "\n",
    "\n",
    "#### Dynamics\n",
    "The Dynamics obey our random market simulation, most prices will move continously and in a locally bounded manner. Some prices drop to 0 after expiry, but if the market attempts to buy them it is taken care of by the environment. There's more to say here\n",
    "\n",
    "#### Goal and Rewards\n",
    "The goal of the model is to minimize some loss metric related to risk. I chose to approximate this as being minimizing the sum of squared stepwise P&L (Price and Loss) as a standin for variance. We could (as in mountain car) add a punishment term for buying expired assets which might needlessly complicate training, or might reduce variance and improve it...\n",
    "$$r_i = -(\\mathrm{CVA_i} - \\mathcal{P}_{\\mathrm{hedge}})^2$$\n",
    "\n",
    "#### Initial State\n",
    "The initial state of the market is an ATM swaption and derivatives to coincide, there is some constant initial default risk. The intial hedge is an even spread in value across all of the assets but this should probably be changed to a delta hedge once we have that.\n",
    "\n",
    "#### Episode End\n",
    "The episode ends after year 9 when the naive CVA is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import pickle\n",
    "\n",
    "import path_datatype\n",
    "import sys\n",
    "\n",
    "from env import tradingEng\n",
    "\n",
    "\n",
    "# Define environment\n",
    "with open(\"1.6kRunDemo.pkl\",\"rb\") as fp:\n",
    "    paths = pickle.load(fp)\n",
    "env = tradingEng(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "n_actions = 18\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean = np.zeros(n_actions), sigma = 0.15*np.ones(n_actions), theta = 0.3)\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\cva_risk_management_thesis\\LearningDDPG\\OurEnv\\env.py:143: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  scale = value/value_action\n",
      "c:\\Users\\osc16\\cva_risk_management_thesis\\LearningDDPG\\OurEnv\\env.py:146: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"Swaption Position\" : action[\"Swaption Position\"]*scale,\n",
      "c:\\Users\\osc16\\cva_risk_management_thesis\\LearningDDPG\\OurEnv\\env.py:147: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"Q Position\" : action[\"Q Position\"]*scale,\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000, log_interval=100)\n",
    "model.save(\"ddpg_fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envT = tradingEng(paths)\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "num_eval_episodes = 50\n",
    "\n",
    "env = RecordEpisodeStatistics(env, buffer_length=num_eval_episodes)\n",
    "\n",
    "for episode_num in range(num_eval_episodes):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action, _states = model.predict(obs, deterministic=True)  # replace with actual agent\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "env.close()\n",
    "\n",
    "print(f'Episode time taken: {env.time_queue}')\n",
    "print(f'Episode total rewards: {env.return_queue}')\n",
    "print(f'Episode lengths: {env.length_queue}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
