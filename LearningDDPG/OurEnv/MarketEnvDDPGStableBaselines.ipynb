{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Environment\n",
    "[Is there some way to visualize our env in a good way? Overlay all the Swaptions and Qs as semi transparent red and blue then add CVA as solid?]\n",
    "\n",
    "Our environment is much like the Mountain Car environment in many key ways, it is also continous in input and output, it is an optimal control problem, it is markov. But there are a few key differences to catch. First of all we have an 18 dimensional action space, and a 37 dimensional input space, so it is a much larger space. Another difference to note is that our environment has a fixed termination date after 9 (10) years, so we should keep this in mind when we think about discounting and planning.\n",
    "\n",
    "#### Action/Obs Space \n",
    "##### Observation\n",
    "The observation is a `ndarray` with shape `(37,)` where the elements correspond to the following:\n",
    "|Num  |Observation                                                       |Min   |Max     |Unit              |\n",
    "|-----|------------------------------------------------------------------|------|--------|------------------|\n",
    "|0-8  |Fraction of portfolio value in Swaptions expiring in year 1 to 9  | 0    | 1      | float - fraction | \n",
    "|9-17 |Fraction of portfolio in defaulting after year 1 to 9             | 0    | 1      | float - fraction |\n",
    "|18-26|Swaptions expiring in year 1 to 9 with strike at Swap strike      | 0    | Inf*   | float - $ value  | \n",
    "|27-35|Probability of defaulting after year 1 to 9                       | 0    | 1      | float - $ value  |\n",
    "|36   |Current interest rate                                             | -Inf** | Inf**| float - %        |\n",
    "\n",
    "*In practice very small \\\n",
    "**In practice between around 10 to -3\n",
    "\n",
    "##### Action\n",
    "The action is a `ndarray` with shape `18` where the elements correspond to the following:\n",
    "|Num  |Action                                                            |Min   |Max    |Unit              |\n",
    "|-----|------------------------------------------------------------------|------|-------|------------------|\n",
    "|0-8  |Fraction of portfolio value in Swaptions expiring in year 1 to 9  | 0    | 1     | float - fraction | \n",
    "|9-17 |Fraction of portfolio in defaulting after year 1 to 9             | 0    | 1     | float - fraction |\n",
    "\n",
    "\n",
    "#### Dynamics\n",
    "The Dynamics obey our random market simulation, most prices will move continously and in a locally bounded manner. Some prices drop to 0 after expiry, but if the market attempts to buy them it is taken care of by the environment. There's more to say here\n",
    "\n",
    "#### Goal and Rewards\n",
    "The goal of the model is to minimize some loss metric related to risk. I chose to approximate this as being minimizing the sum of squared stepwise P&L (Price and Loss) as a standin for variance. We could (as in mountain car) add a punishment term for buying expired assets which might needlessly complicate training, or might reduce variance and improve it...\n",
    "$$r_i = -(\\mathrm{CVA_i} - \\mathcal{P}_{\\mathrm{hedge}})^2$$\n",
    "\n",
    "#### Initial State\n",
    "The initial state of the market is an ATM swaption and derivatives to coincide, there is some constant initial default risk. The intial hedge is an even spread in value across all of the assets but this should probably be changed to a delta hedge once we have that.\n",
    "\n",
    "#### Episode End\n",
    "The episode ends after year 9 when the naive CVA is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import pickle\n",
    "\n",
    "import path_datatype\n",
    "import sys\n",
    "\n",
    "from env import tradingEng\n",
    "\n",
    "\n",
    "# Define environment\n",
    "with open(\"1.6kRunDemo.pkl\",\"rb\") as fp:\n",
    "    paths = pickle.load(fp)\n",
    "env = tradingEng(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "n_actions = 18\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean = np.zeros(n_actions), sigma = 0.05*np.ones(n_actions), theta = 0.01)\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1, batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 2        |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 2359     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2258     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 4702     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 6        |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 7038     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6937     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 9373     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9272     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 11722    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11621    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 70       |\n",
      "|    time_elapsed    | 198      |\n",
      "|    total_timesteps | 14074    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13973    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 14       |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 16429    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16328    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 260      |\n",
      "|    total_timesteps | 18782    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18681    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 18       |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 21138    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21037    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.17e+03 |\n",
      "|    ep_rew_mean     | nan      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 23490    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | nan      |\n",
      "|    critic_loss     | nan      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23389    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "Nruns = 10\n",
    "model.learn(total_timesteps=251*10*Nruns, log_interval=2)\n",
    "model.save(\"ddpg_fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_over:\n\u001b[1;32m---> 11\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# replace with actual agent\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     13\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "num_eval_episodes = 1\n",
    "\n",
    "env = tradingEng(paths)\n",
    "\n",
    "episode_over = False\n",
    "rewards = list()\n",
    "actions = list()\n",
    "obs, info = env.reset()\n",
    "while not episode_over:\n",
    "    action, _states = model.predict(obs, deterministic=True)  # replace with actual agent\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    episode_over = terminated or truncated\n",
    "env.close()\n",
    "\n",
    "print(f'Example action taken: {actions[0]}')\n",
    "print(f'Episode rewards: {rewards}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
