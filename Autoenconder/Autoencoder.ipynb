{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde30f52",
   "metadata": {},
   "source": [
    "### Setting up a quick and dirty simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6851c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import os, sys\n",
    "dir = os.path.abspath('')\n",
    "dir = os.path.dirname(dir)\n",
    "sys.path.append(dir)\n",
    "import MarketGeneratingFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8908012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoEncoder import MarketAutoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890e850",
   "metadata": {},
   "source": [
    "### Try Training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef151fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "import pickle\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths3 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths4 = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df91e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = paths1 + paths2 + paths3 + paths4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558fae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1, Batch 1000 loss: 0.2161127008163673\n",
      "Epoch1, Batch 3000 loss: 0.09115282501461017\n",
      "Epoch1, Batch 5000 loss: 0.09117833379866915\n",
      "Epoch1, Batch 7000 loss: 0.09089264083782223\n",
      "Epoch1, Batch 9000 loss: 0.0909280089905899\n",
      "Epoch2, Batch 1000 loss: 0.09425659016745974\n",
      "Epoch2, Batch 3000 loss: 0.0906029310495359\n",
      "Epoch2, Batch 5000 loss: 0.09064874475177696\n",
      "Epoch2, Batch 7000 loss: 0.0905149729458871\n",
      "Epoch2, Batch 9000 loss: 0.0902509220345565\n",
      "Epoch3, Batch 1000 loss: 0.09393893805587097\n",
      "Epoch3, Batch 3000 loss: 0.09018353456099022\n",
      "Epoch3, Batch 5000 loss: 0.09011936370048541\n",
      "Epoch3, Batch 7000 loss: 0.09032118250943627\n",
      "Epoch3, Batch 9000 loss: 0.09011842259043612\n",
      "Epoch4, Batch 1000 loss: 0.09372379595335882\n",
      "Epoch4, Batch 3000 loss: 0.09016848508922008\n",
      "Epoch4, Batch 5000 loss: 0.09011368272677521\n",
      "Epoch4, Batch 7000 loss: 0.09011765799217915\n",
      "Epoch4, Batch 9000 loss: 0.08984572014887317\n",
      "Epoch5, Batch 1000 loss: 0.09355589226343085\n",
      "Epoch5, Batch 3000 loss: 0.09025035432237587\n",
      "Epoch5, Batch 5000 loss: 0.08993984693246845\n",
      "Epoch5, Batch 7000 loss: 0.08990711833684373\n",
      "Epoch5, Batch 9000 loss: 0.08991028213377023\n",
      "Epoch6, Batch 1000 loss: 0.09338868717076383\n",
      "Epoch6, Batch 3000 loss: 0.08993193149463964\n",
      "Epoch6, Batch 5000 loss: 0.09002559552398617\n",
      "Epoch6, Batch 7000 loss: 0.08983965060838023\n",
      "Epoch6, Batch 9000 loss: 0.08979229658775134\n",
      "Epoch7, Batch 1000 loss: 0.09336628004567395\n",
      "Epoch7, Batch 3000 loss: 0.09007264709560464\n",
      "Epoch7, Batch 5000 loss: 0.08989134943424772\n",
      "Epoch7, Batch 7000 loss: 0.08980407619178393\n",
      "Epoch7, Batch 9000 loss: 0.08976850050903819\n",
      "Epoch8, Batch 1000 loss: 0.09337250909098826\n",
      "Epoch8, Batch 3000 loss: 0.08983539483316677\n",
      "Epoch8, Batch 5000 loss: 0.08977021582217465\n",
      "Epoch8, Batch 7000 loss: 0.08988847097995421\n",
      "Epoch8, Batch 9000 loss: 0.08979426616599291\n",
      "Epoch9, Batch 1000 loss: 0.09333876872707796\n",
      "Epoch9, Batch 3000 loss: 0.0898498487690261\n",
      "Epoch9, Batch 5000 loss: 0.08965561169539026\n",
      "Epoch9, Batch 7000 loss: 0.08985211631273507\n",
      "Epoch9, Batch 9000 loss: 0.0897420269898294\n",
      "Epoch10, Batch 1000 loss: 0.09333827010692952\n",
      "Epoch10, Batch 3000 loss: 0.08973350788310956\n",
      "Epoch10, Batch 5000 loss: 0.08992430765845236\n",
      "Epoch10, Batch 7000 loss: 0.08978978155128658\n",
      "Epoch10, Batch 9000 loss: 0.08981168967657502\n",
      "Epoch11, Batch 1000 loss: 0.09331517509792948\n",
      "Epoch11, Batch 3000 loss: 0.08985147945961548\n",
      "Epoch11, Batch 5000 loss: 0.08974643234512918\n",
      "Epoch11, Batch 7000 loss: 0.08971409547425568\n",
      "Epoch11, Batch 9000 loss: 0.08972166267674167\n",
      "Epoch12, Batch 1000 loss: 0.09317210346046668\n",
      "Epoch12, Batch 3000 loss: 0.08975145725515558\n",
      "Epoch12, Batch 5000 loss: 0.08968406211157418\n",
      "Epoch12, Batch 7000 loss: 0.08999366288101807\n",
      "Epoch12, Batch 9000 loss: 0.08982059674614522\n",
      "Epoch13, Batch 1000 loss: 0.09303159116311349\n",
      "Epoch13, Batch 3000 loss: 0.08983953885619698\n",
      "Epoch13, Batch 5000 loss: 0.08967026387275506\n",
      "Epoch13, Batch 7000 loss: 0.08984405259085414\n",
      "Epoch13, Batch 9000 loss: 0.08964515436420564\n",
      "Epoch14, Batch 1000 loss: 0.0931396083302051\n",
      "Epoch14, Batch 3000 loss: 0.08963572120922149\n",
      "Epoch14, Batch 5000 loss: 0.08951436614163549\n",
      "Epoch14, Batch 7000 loss: 0.08980453938308905\n",
      "Epoch14, Batch 9000 loss: 0.08965445239754245\n",
      "Epoch15, Batch 1000 loss: 0.09336630381467632\n",
      "Epoch15, Batch 3000 loss: 0.08966146504976014\n",
      "Epoch15, Batch 5000 loss: 0.08965867242524703\n",
      "Epoch15, Batch 7000 loss: 0.08962043534955329\n",
      "Epoch15, Batch 9000 loss: 0.08970769952836469\n",
      "Epoch16, Batch 1000 loss: 0.09310881283856696\n",
      "Epoch16, Batch 3000 loss: 0.08975065132900062\n",
      "Epoch16, Batch 5000 loss: 0.08964044829726482\n",
      "Epoch16, Batch 7000 loss: 0.08941965010587655\n",
      "Epoch16, Batch 9000 loss: 0.08979663773945576\n",
      "Epoch17, Batch 1000 loss: 0.0933919731672043\n",
      "Epoch17, Batch 3000 loss: 0.08965004124827576\n",
      "Epoch17, Batch 5000 loss: 0.08975307058939211\n",
      "Epoch17, Batch 7000 loss: 0.08948856942048147\n",
      "Epoch17, Batch 9000 loss: 0.08955219977795988\n",
      "Epoch18, Batch 1000 loss: 0.09322967064943317\n",
      "Epoch18, Batch 3000 loss: 0.08958852252035453\n",
      "Epoch18, Batch 5000 loss: 0.08981165434152844\n",
      "Epoch18, Batch 7000 loss: 0.08948603878390542\n",
      "Epoch18, Batch 9000 loss: 0.08960915510730767\n",
      "Epoch19, Batch 1000 loss: 0.09306083973729905\n",
      "Epoch19, Batch 3000 loss: 0.08964693959881224\n",
      "Epoch19, Batch 5000 loss: 0.08971897191826889\n",
      "Epoch19, Batch 7000 loss: 0.08938099915939059\n",
      "Epoch19, Batch 9000 loss: 0.08967127141620596\n",
      "Epoch20, Batch 1000 loss: 0.0930911018082636\n",
      "Epoch20, Batch 3000 loss: 0.08955019767768059\n",
      "Epoch20, Batch 5000 loss: 0.08978613784706067\n",
      "Epoch20, Batch 7000 loss: 0.08956721944631768\n",
      "Epoch20, Batch 9000 loss: 0.08948184059150444\n",
      "Epoch21, Batch 1000 loss: 0.09330718212370584\n",
      "Epoch21, Batch 3000 loss: 0.08956966575446461\n",
      "Epoch21, Batch 5000 loss: 0.08941092675792851\n",
      "Epoch21, Batch 7000 loss: 0.0898807995371788\n",
      "Epoch21, Batch 9000 loss: 0.08955185241899125\n",
      "Epoch22, Batch 1000 loss: 0.09321913067689401\n",
      "Epoch22, Batch 3000 loss: 0.08960004226586174\n",
      "Epoch22, Batch 5000 loss: 0.0895201486270538\n",
      "Epoch22, Batch 7000 loss: 0.0895603129837652\n",
      "Epoch22, Batch 9000 loss: 0.0896304259605658\n",
      "Epoch23, Batch 1000 loss: 0.09327824046350881\n",
      "Epoch23, Batch 3000 loss: 0.08960691545284978\n",
      "Epoch23, Batch 5000 loss: 0.08954516842616651\n",
      "Epoch23, Batch 7000 loss: 0.08961080806937517\n",
      "Epoch23, Batch 9000 loss: 0.08956913555895227\n",
      "Epoch24, Batch 1000 loss: 0.09322998309715269\n",
      "Epoch24, Batch 3000 loss: 0.08965784022632953\n",
      "Epoch24, Batch 5000 loss: 0.08957055271502651\n",
      "Epoch24, Batch 7000 loss: 0.0895366355472944\n",
      "Epoch24, Batch 9000 loss: 0.08963351775592912\n",
      "Epoch25, Batch 1000 loss: 0.0931188252606359\n",
      "Epoch25, Batch 3000 loss: 0.08959832461311482\n",
      "Epoch25, Batch 5000 loss: 0.08962526257334413\n",
      "Epoch25, Batch 7000 loss: 0.08949835423367339\n",
      "Epoch25, Batch 9000 loss: 0.08957215581393928\n",
      "Epoch26, Batch 1000 loss: 0.09326534028717738\n",
      "Epoch26, Batch 3000 loss: 0.08948073497020484\n",
      "Epoch26, Batch 5000 loss: 0.08959476855295755\n",
      "Epoch26, Batch 7000 loss: 0.08948802072372268\n",
      "Epoch26, Batch 9000 loss: 0.08948112851686972\n",
      "Epoch27, Batch 1000 loss: 0.09327828631980942\n",
      "Epoch27, Batch 3000 loss: 0.08960773054016044\n",
      "Epoch27, Batch 5000 loss: 0.08945625519473249\n",
      "Epoch27, Batch 7000 loss: 0.08958289231741855\n",
      "Epoch27, Batch 9000 loss: 0.0895414950987607\n",
      "Epoch28, Batch 1000 loss: 0.09329417796998234\n",
      "Epoch28, Batch 3000 loss: 0.08965785357145857\n",
      "Epoch28, Batch 5000 loss: 0.08955974973320584\n",
      "Epoch28, Batch 7000 loss: 0.08952136207033981\n",
      "Epoch28, Batch 9000 loss: 0.0895280541451869\n",
      "Epoch29, Batch 1000 loss: 0.09322457988576123\n",
      "Epoch29, Batch 3000 loss: 0.08956226394495652\n",
      "Epoch29, Batch 5000 loss: 0.08965720148969337\n",
      "Epoch29, Batch 7000 loss: 0.08936523321829164\n",
      "Epoch29, Batch 9000 loss: 0.08957069250410604\n",
      "Epoch30, Batch 1000 loss: 0.09330424552066197\n",
      "Epoch30, Batch 3000 loss: 0.08950519675938685\n",
      "Epoch30, Batch 5000 loss: 0.08943999633619235\n",
      "Epoch30, Batch 7000 loss: 0.08968769729132915\n",
      "Epoch30, Batch 9000 loss: 0.0893417450740549\n",
      "Epoch31, Batch 1000 loss: 0.0931680706954658\n",
      "Epoch31, Batch 3000 loss: 0.08956902529691374\n",
      "Epoch31, Batch 5000 loss: 0.08956067042948944\n",
      "Epoch31, Batch 7000 loss: 0.08951003436978153\n",
      "Epoch31, Batch 9000 loss: 0.08967494395822727\n",
      "Epoch32, Batch 1000 loss: 0.0929905931807079\n",
      "Epoch32, Batch 3000 loss: 0.08951203285737132\n",
      "Epoch32, Batch 5000 loss: 0.08955219865899529\n",
      "Epoch32, Batch 7000 loss: 0.08963722752411701\n",
      "Epoch32, Batch 9000 loss: 0.08959382711744711\n",
      "Epoch33, Batch 1000 loss: 0.09326373058800214\n",
      "Epoch33, Batch 3000 loss: 0.0896170787745807\n",
      "Epoch33, Batch 5000 loss: 0.08963849669952878\n",
      "Epoch33, Batch 7000 loss: 0.08940552417034309\n",
      "Epoch33, Batch 9000 loss: 0.08970231833154174\n",
      "Epoch34, Batch 1000 loss: 0.09297919837327127\n",
      "Epoch34, Batch 3000 loss: 0.0896155598823155\n",
      "Epoch34, Batch 5000 loss: 0.08937929658339447\n",
      "Epoch34, Batch 7000 loss: 0.08954799291467237\n",
      "Epoch34, Batch 9000 loss: 0.08956873969923654\n",
      "Epoch35, Batch 1000 loss: 0.09307706605708826\n",
      "Epoch35, Batch 3000 loss: 0.0896402056792133\n",
      "Epoch35, Batch 5000 loss: 0.08957996748566696\n",
      "Epoch35, Batch 7000 loss: 0.08963279981968404\n",
      "Epoch35, Batch 9000 loss: 0.08944305090790759\n",
      "Epoch36, Batch 1000 loss: 0.09303335073009159\n",
      "Epoch36, Batch 3000 loss: 0.08960659104467383\n",
      "Epoch36, Batch 5000 loss: 0.08958355609327633\n",
      "Epoch36, Batch 7000 loss: 0.08942165155531874\n",
      "Epoch36, Batch 9000 loss: 0.08964295655334989\n",
      "Epoch37, Batch 1000 loss: 0.09300147013256997\n",
      "Epoch37, Batch 3000 loss: 0.08953502951087955\n",
      "Epoch37, Batch 5000 loss: 0.08949712773946379\n",
      "Epoch37, Batch 7000 loss: 0.08947568165892275\n",
      "Epoch37, Batch 9000 loss: 0.08964760778548689\n",
      "Epoch38, Batch 1000 loss: 0.09326052328777484\n",
      "Epoch38, Batch 3000 loss: 0.08947269265382733\n",
      "Epoch38, Batch 5000 loss: 0.08928919442126501\n",
      "Epoch38, Batch 7000 loss: 0.08955032192993571\n",
      "Epoch38, Batch 9000 loss: 0.08972611319806437\n",
      "Epoch39, Batch 1000 loss: 0.09320708466577797\n",
      "Epoch39, Batch 3000 loss: 0.08940180749840893\n",
      "Epoch39, Batch 5000 loss: 0.08962474041216398\n",
      "Epoch39, Batch 7000 loss: 0.08959541696700764\n",
      "Epoch39, Batch 9000 loss: 0.08954385472280989\n",
      "Epoch40, Batch 1000 loss: 0.09302316375775856\n",
      "Epoch40, Batch 3000 loss: 0.08939619382280004\n",
      "Epoch40, Batch 5000 loss: 0.08954249478277337\n",
      "Epoch40, Batch 7000 loss: 0.08979338825854404\n",
      "Epoch40, Batch 9000 loss: 0.08956339672143128\n",
      "Epoch41, Batch 1000 loss: 0.0931329857458733\n",
      "Epoch41, Batch 3000 loss: 0.08942204725094847\n",
      "Epoch41, Batch 5000 loss: 0.08942342716377198\n",
      "Epoch41, Batch 7000 loss: 0.08958588947608558\n",
      "Epoch41, Batch 9000 loss: 0.08958463335178317\n",
      "Epoch42, Batch 1000 loss: 0.0932065708152823\n",
      "Epoch42, Batch 3000 loss: 0.08954807673353019\n",
      "Epoch42, Batch 5000 loss: 0.0893564710008682\n",
      "Epoch42, Batch 7000 loss: 0.0896278798027138\n",
      "Epoch42, Batch 9000 loss: 0.08943772099046798\n",
      "Epoch43, Batch 1000 loss: 0.09330492840935734\n",
      "Epoch43, Batch 3000 loss: 0.08944244222380322\n",
      "Epoch43, Batch 5000 loss: 0.08958486749766024\n",
      "Epoch43, Batch 7000 loss: 0.08946712075795138\n",
      "Epoch43, Batch 9000 loss: 0.08950718191758424\n",
      "Epoch44, Batch 1000 loss: 0.09311294904727328\n",
      "Epoch44, Batch 3000 loss: 0.08943076388814032\n",
      "Epoch44, Batch 5000 loss: 0.08965599756405072\n",
      "Epoch44, Batch 7000 loss: 0.08941896005272619\n",
      "Epoch44, Batch 9000 loss: 0.08958224264767035\n",
      "Epoch45, Batch 1000 loss: 0.0931566654343184\n",
      "Epoch45, Batch 3000 loss: 0.08944887865173246\n",
      "Epoch45, Batch 5000 loss: 0.08955299743173396\n",
      "Epoch45, Batch 7000 loss: 0.0894498791724686\n",
      "Epoch45, Batch 9000 loss: 0.08968489085734872\n",
      "Epoch46, Batch 1000 loss: 0.09303583563412449\n",
      "Epoch46, Batch 3000 loss: 0.08974898663995441\n",
      "Epoch46, Batch 5000 loss: 0.08970353311726408\n",
      "Epoch46, Batch 7000 loss: 0.08941158328014938\n",
      "Epoch46, Batch 9000 loss: 0.08943018840178509\n",
      "Epoch47, Batch 1000 loss: 0.09291317799917972\n",
      "Epoch47, Batch 3000 loss: 0.08956584948663618\n",
      "Epoch47, Batch 5000 loss: 0.08941809559652848\n",
      "Epoch47, Batch 7000 loss: 0.08967884078657282\n",
      "Epoch47, Batch 9000 loss: 0.08953037766725386\n",
      "Epoch48, Batch 1000 loss: 0.0930728658252464\n",
      "Epoch48, Batch 3000 loss: 0.0897083222712304\n",
      "Epoch48, Batch 5000 loss: 0.08953454850598457\n",
      "Epoch48, Batch 7000 loss: 0.08935633243584956\n",
      "Epoch48, Batch 9000 loss: 0.08930611172622255\n",
      "Epoch49, Batch 1000 loss: 0.09319175469572169\n",
      "Epoch49, Batch 3000 loss: 0.08944747591017832\n",
      "Epoch49, Batch 5000 loss: 0.08956908613964207\n",
      "Epoch49, Batch 7000 loss: 0.08941511572909457\n",
      "Epoch49, Batch 9000 loss: 0.08959086835794537\n",
      "Epoch50, Batch 1000 loss: 0.0931487081108754\n",
      "Epoch50, Batch 3000 loss: 0.08945326124333204\n",
      "Epoch50, Batch 5000 loss: 0.0895141939738274\n",
      "Epoch50, Batch 7000 loss: 0.08937863684063574\n",
      "Epoch50, Batch 9000 loss: 0.08955467738511165\n",
      "Epoch51, Batch 1000 loss: 0.09313509891748568\n",
      "Epoch51, Batch 3000 loss: 0.0896857818237749\n",
      "Epoch51, Batch 5000 loss: 0.08928375147291907\n",
      "Epoch51, Batch 7000 loss: 0.08970543269888068\n",
      "Epoch51, Batch 9000 loss: 0.08942340274458994\n",
      "Epoch52, Batch 1000 loss: 0.09316656938344718\n",
      "Epoch52, Batch 3000 loss: 0.08962161255215642\n",
      "Epoch52, Batch 5000 loss: 0.08971564822001787\n",
      "Epoch52, Batch 7000 loss: 0.08912120131280227\n",
      "Epoch52, Batch 9000 loss: 0.08939003159236465\n",
      "Epoch53, Batch 1000 loss: 0.09325608301478107\n",
      "Epoch53, Batch 3000 loss: 0.08948244033300917\n",
      "Epoch53, Batch 5000 loss: 0.08947646003307166\n",
      "Epoch53, Batch 7000 loss: 0.08943687947699502\n",
      "Epoch53, Batch 9000 loss: 0.08949552658462204\n",
      "Epoch54, Batch 1000 loss: 0.09316711170252917\n",
      "Epoch54, Batch 3000 loss: 0.08949735164766666\n",
      "Epoch54, Batch 5000 loss: 0.08954152856816615\n",
      "Epoch54, Batch 7000 loss: 0.08949269648891298\n",
      "Epoch54, Batch 9000 loss: 0.08946490171027895\n",
      "Epoch55, Batch 1000 loss: 0.09307641236747854\n",
      "Epoch55, Batch 3000 loss: 0.08945390475317551\n",
      "Epoch55, Batch 5000 loss: 0.08958760650337548\n",
      "Epoch55, Batch 7000 loss: 0.08935697145340293\n",
      "Epoch55, Batch 9000 loss: 0.08963517533759713\n",
      "Epoch56, Batch 1000 loss: 0.09306778627611537\n",
      "Epoch56, Batch 3000 loss: 0.08962791725172113\n",
      "Epoch56, Batch 5000 loss: 0.08939318272708464\n",
      "Epoch56, Batch 7000 loss: 0.08948179151191633\n",
      "Epoch56, Batch 9000 loss: 0.08949486805353879\n",
      "Epoch57, Batch 1000 loss: 0.09325921754641225\n",
      "Epoch57, Batch 3000 loss: 0.08958118001545207\n",
      "Epoch57, Batch 5000 loss: 0.08959692223701672\n",
      "Epoch57, Batch 7000 loss: 0.08929266812639326\n",
      "Epoch57, Batch 9000 loss: 0.08947758824256276\n",
      "Epoch58, Batch 1000 loss: 0.0930950886765405\n",
      "Epoch58, Batch 3000 loss: 0.08934336068998763\n",
      "Epoch58, Batch 5000 loss: 0.08962275229884296\n",
      "Epoch58, Batch 7000 loss: 0.08946237841571297\n",
      "Epoch58, Batch 9000 loss: 0.08949587043289195\n",
      "Epoch59, Batch 1000 loss: 0.09312540348750419\n",
      "Epoch59, Batch 3000 loss: 0.08969905561291079\n",
      "Epoch59, Batch 5000 loss: 0.08937534726139898\n",
      "Epoch59, Batch 7000 loss: 0.08946356639262203\n",
      "Epoch59, Batch 9000 loss: 0.08949480283868344\n",
      "Epoch60, Batch 1000 loss: 0.09302575908521396\n",
      "Epoch60, Batch 3000 loss: 0.08952253124791867\n",
      "Epoch60, Batch 5000 loss: 0.08949917651708102\n",
      "Epoch60, Batch 7000 loss: 0.08961097237513767\n",
      "Epoch60, Batch 9000 loss: 0.08930999350373514\n",
      "Epoch61, Batch 1000 loss: 0.09306493712180948\n",
      "Epoch61, Batch 3000 loss: 0.08950415315029234\n",
      "Epoch61, Batch 5000 loss: 0.08954479357530457\n",
      "Epoch61, Batch 7000 loss: 0.08964928256307168\n",
      "Epoch61, Batch 9000 loss: 0.08938491739790116\n",
      "Epoch62, Batch 1000 loss: 0.09304073562543537\n",
      "Epoch62, Batch 3000 loss: 0.0895594746123807\n",
      "Epoch62, Batch 5000 loss: 0.08937985743490286\n",
      "Epoch62, Batch 7000 loss: 0.08973070096950042\n",
      "Epoch62, Batch 9000 loss: 0.08946372853951416\n",
      "Epoch63, Batch 1000 loss: 0.09285560011028351\n",
      "Epoch63, Batch 3000 loss: 0.08958162883285506\n",
      "Epoch63, Batch 5000 loss: 0.08949295997931092\n",
      "Epoch63, Batch 7000 loss: 0.0895073356421679\n",
      "Epoch63, Batch 9000 loss: 0.08945204785861553\n",
      "Epoch64, Batch 1000 loss: 0.09327804428828484\n",
      "Epoch64, Batch 3000 loss: 0.08949886758908956\n",
      "Epoch64, Batch 5000 loss: 0.08954753435813585\n",
      "Epoch64, Batch 7000 loss: 0.0894190273837792\n",
      "Epoch64, Batch 9000 loss: 0.08933092063592223\n",
      "Epoch65, Batch 1000 loss: 0.09308596762059691\n",
      "Epoch65, Batch 3000 loss: 0.08947657148951303\n",
      "Epoch65, Batch 5000 loss: 0.08970227352014283\n",
      "Epoch65, Batch 7000 loss: 0.08937579943599952\n",
      "Epoch65, Batch 9000 loss: 0.089365774817801\n",
      "Epoch66, Batch 1000 loss: 0.09310569828104485\n",
      "Epoch66, Batch 3000 loss: 0.08979078004101591\n",
      "Epoch66, Batch 5000 loss: 0.0893514886315002\n",
      "Epoch66, Batch 7000 loss: 0.08950967982588957\n",
      "Epoch66, Batch 9000 loss: 0.08932770680088302\n",
      "Epoch67, Batch 1000 loss: 0.09291633882750586\n",
      "Epoch67, Batch 3000 loss: 0.08947251778603293\n",
      "Epoch67, Batch 5000 loss: 0.0895942162398475\n",
      "Epoch67, Batch 7000 loss: 0.08950375339712922\n",
      "Epoch67, Batch 9000 loss: 0.0896212812608403\n",
      "Epoch68, Batch 1000 loss: 0.09294210954333036\n",
      "Epoch68, Batch 3000 loss: 0.08947506672492521\n",
      "Epoch68, Batch 5000 loss: 0.08954762813335293\n",
      "Epoch68, Batch 7000 loss: 0.08941122329735389\n",
      "Epoch68, Batch 9000 loss: 0.08944982024343287\n",
      "Epoch69, Batch 1000 loss: 0.09312436635330293\n",
      "Epoch69, Batch 3000 loss: 0.08968216496788282\n",
      "Epoch69, Batch 5000 loss: 0.0892922072012376\n",
      "Epoch69, Batch 7000 loss: 0.08948552904272898\n",
      "Epoch69, Batch 9000 loss: 0.08940791199652043\n",
      "Epoch70, Batch 1000 loss: 0.0932147297661943\n",
      "Epoch70, Batch 3000 loss: 0.08942511618722913\n",
      "Epoch70, Batch 5000 loss: 0.08935781647573532\n",
      "Epoch70, Batch 7000 loss: 0.08955509835297158\n",
      "Epoch70, Batch 9000 loss: 0.08952401353636376\n",
      "Epoch71, Batch 1000 loss: 0.09302876452906533\n",
      "Epoch71, Batch 3000 loss: 0.08950578866248463\n",
      "Epoch71, Batch 5000 loss: 0.0894095383292501\n",
      "Epoch71, Batch 7000 loss: 0.08963036559429009\n",
      "Epoch71, Batch 9000 loss: 0.08933399691652644\n",
      "Epoch72, Batch 1000 loss: 0.09323318496937001\n",
      "Epoch72, Batch 3000 loss: 0.08939519665454923\n",
      "Epoch72, Batch 5000 loss: 0.08940236438580497\n",
      "Epoch72, Batch 7000 loss: 0.08967600181387397\n",
      "Epoch72, Batch 9000 loss: 0.08940897249430649\n",
      "Epoch73, Batch 1000 loss: 0.09293293499315007\n",
      "Epoch73, Batch 3000 loss: 0.08957737594481954\n",
      "Epoch73, Batch 5000 loss: 0.08941974536734439\n",
      "Epoch73, Batch 7000 loss: 0.08947879260041115\n",
      "Epoch73, Batch 9000 loss: 0.08944800887104563\n",
      "Epoch74, Batch 1000 loss: 0.09295309154122859\n",
      "Epoch74, Batch 3000 loss: 0.08951036257732325\n",
      "Epoch74, Batch 5000 loss: 0.08941167376432667\n",
      "Epoch74, Batch 7000 loss: 0.08953856717259233\n",
      "Epoch74, Batch 9000 loss: 0.08953272480054521\n",
      "Epoch75, Batch 1000 loss: 0.09296696878949741\n",
      "Epoch75, Batch 3000 loss: 0.08986099647048126\n",
      "Epoch75, Batch 5000 loss: 0.08925658379396115\n",
      "Epoch75, Batch 7000 loss: 0.08940735359180126\n",
      "Epoch75, Batch 9000 loss: 0.08956505914100373\n",
      "Epoch76, Batch 1000 loss: 0.0929238967886502\n",
      "Epoch76, Batch 3000 loss: 0.08926334259851475\n",
      "Epoch76, Batch 5000 loss: 0.08969265011119858\n",
      "Epoch76, Batch 7000 loss: 0.08950487691891729\n",
      "Epoch76, Batch 9000 loss: 0.08941306125900164\n",
      "Epoch77, Batch 1000 loss: 0.0931933020603243\n",
      "Epoch77, Batch 3000 loss: 0.08943253840513785\n",
      "Epoch77, Batch 5000 loss: 0.08939551809766076\n",
      "Epoch77, Batch 7000 loss: 0.08963397557340125\n",
      "Epoch77, Batch 9000 loss: 0.08942884642829455\n",
      "Epoch78, Batch 1000 loss: 0.09294300490205046\n",
      "Epoch78, Batch 3000 loss: 0.08946757544239144\n",
      "Epoch78, Batch 5000 loss: 0.08951790137524085\n",
      "Epoch78, Batch 7000 loss: 0.08943185800590336\n",
      "Epoch78, Batch 9000 loss: 0.08954422527592702\n",
      "Epoch79, Batch 1000 loss: 0.09312576745365049\n",
      "Epoch79, Batch 3000 loss: 0.08960347029996776\n",
      "Epoch79, Batch 5000 loss: 0.08936147644662723\n",
      "Epoch79, Batch 7000 loss: 0.08940422266746924\n",
      "Epoch79, Batch 9000 loss: 0.08940989527196122\n",
      "Epoch80, Batch 1000 loss: 0.09302881365283967\n",
      "Epoch80, Batch 3000 loss: 0.08948562899791965\n",
      "Epoch80, Batch 5000 loss: 0.08942506503835242\n",
      "Epoch80, Batch 7000 loss: 0.08943393773324722\n",
      "Epoch80, Batch 9000 loss: 0.08949375225061344\n",
      "Epoch81, Batch 1000 loss: 0.09304633354047721\n",
      "Epoch81, Batch 3000 loss: 0.08929487230302495\n",
      "Epoch81, Batch 5000 loss: 0.0897140200655935\n",
      "Epoch81, Batch 7000 loss: 0.0893758891546177\n",
      "Epoch81, Batch 9000 loss: 0.08958130382576678\n",
      "Epoch82, Batch 1000 loss: 0.09290228222380432\n",
      "Epoch82, Batch 3000 loss: 0.08937940512932603\n",
      "Epoch82, Batch 5000 loss: 0.08947981770362236\n",
      "Epoch82, Batch 7000 loss: 0.08952446838146948\n",
      "Epoch82, Batch 9000 loss: 0.08949193350888303\n",
      "Epoch83, Batch 1000 loss: 0.09311779403361481\n",
      "Epoch83, Batch 3000 loss: 0.0893430311807251\n",
      "Epoch83, Batch 5000 loss: 0.08969434474651854\n",
      "Epoch83, Batch 7000 loss: 0.08938755470745473\n",
      "Epoch83, Batch 9000 loss: 0.08944746822293635\n",
      "Epoch84, Batch 1000 loss: 0.09301261367288283\n",
      "Epoch84, Batch 3000 loss: 0.08958520764419592\n",
      "Epoch84, Batch 5000 loss: 0.0893448651577382\n",
      "Epoch84, Batch 7000 loss: 0.08958753833297477\n",
      "Epoch84, Batch 9000 loss: 0.08939221950708581\n",
      "Epoch85, Batch 1000 loss: 0.09313451926778708\n",
      "Epoch85, Batch 3000 loss: 0.08939770760891977\n",
      "Epoch85, Batch 5000 loss: 0.08928840266011692\n",
      "Epoch85, Batch 7000 loss: 0.08947566239744667\n",
      "Epoch85, Batch 9000 loss: 0.08956615610982342\n",
      "Epoch86, Batch 1000 loss: 0.09312813768448398\n",
      "Epoch86, Batch 3000 loss: 0.08926491949208808\n",
      "Epoch86, Batch 5000 loss: 0.08946135175264558\n",
      "Epoch86, Batch 7000 loss: 0.08961522836139613\n",
      "Epoch86, Batch 9000 loss: 0.08944870342751561\n",
      "Epoch87, Batch 1000 loss: 0.09310332424616602\n",
      "Epoch87, Batch 3000 loss: 0.08938122977578251\n",
      "Epoch87, Batch 5000 loss: 0.08967568983103925\n",
      "Epoch87, Batch 7000 loss: 0.08943174889768776\n",
      "Epoch87, Batch 9000 loss: 0.08930111213669649\n",
      "Epoch88, Batch 1000 loss: 0.09304440210435595\n",
      "Epoch88, Batch 3000 loss: 0.08936817662501305\n",
      "Epoch88, Batch 5000 loss: 0.08949845144667859\n",
      "Epoch88, Batch 7000 loss: 0.08942886862647252\n",
      "Epoch88, Batch 9000 loss: 0.08949801493216014\n",
      "Epoch89, Batch 1000 loss: 0.09305575965246767\n",
      "Epoch89, Batch 3000 loss: 0.08951812772343928\n",
      "Epoch89, Batch 5000 loss: 0.08939738546378738\n",
      "Epoch89, Batch 7000 loss: 0.08951878081671806\n",
      "Epoch89, Batch 9000 loss: 0.08947326153037913\n",
      "Epoch90, Batch 1000 loss: 0.09301148879507122\n",
      "Epoch90, Batch 3000 loss: 0.08949472152125384\n",
      "Epoch90, Batch 5000 loss: 0.08941407943197104\n",
      "Epoch90, Batch 7000 loss: 0.0895530003554824\n",
      "Epoch90, Batch 9000 loss: 0.08939737478197435\n",
      "Epoch91, Batch 1000 loss: 0.09293544346783596\n",
      "Epoch91, Batch 3000 loss: 0.08933878834664746\n",
      "Epoch91, Batch 5000 loss: 0.08961666920158184\n",
      "Epoch91, Batch 7000 loss: 0.08955732492275467\n",
      "Epoch91, Batch 9000 loss: 0.0894803641104325\n",
      "Epoch92, Batch 1000 loss: 0.09295718681581641\n",
      "Epoch92, Batch 3000 loss: 0.08963052063145927\n",
      "Epoch92, Batch 5000 loss: 0.08944185330310313\n",
      "Epoch92, Batch 7000 loss: 0.08954541351575046\n",
      "Epoch92, Batch 9000 loss: 0.08936460925605703\n",
      "Epoch93, Batch 1000 loss: 0.09285696865206897\n",
      "Epoch93, Batch 3000 loss: 0.0894985571012429\n",
      "Epoch93, Batch 5000 loss: 0.08952070237586908\n",
      "Epoch93, Batch 7000 loss: 0.08942705019054227\n",
      "Epoch93, Batch 9000 loss: 0.08947232671120868\n",
      "Epoch94, Batch 1000 loss: 0.09301493384966349\n",
      "Epoch94, Batch 3000 loss: 0.0896510614623025\n",
      "Epoch94, Batch 5000 loss: 0.08951931030990434\n",
      "Epoch94, Batch 7000 loss: 0.0894134830282432\n",
      "Epoch94, Batch 9000 loss: 0.08917702187731774\n",
      "Epoch95, Batch 1000 loss: 0.09309055145662881\n",
      "Epoch95, Batch 3000 loss: 0.08937997783747983\n",
      "Epoch95, Batch 5000 loss: 0.08947122364049566\n",
      "Epoch95, Batch 7000 loss: 0.08954234896685877\n",
      "Epoch95, Batch 9000 loss: 0.08943413597500975\n",
      "Epoch96, Batch 1000 loss: 0.09303693766852085\n",
      "Epoch96, Batch 3000 loss: 0.08955335219347109\n",
      "Epoch96, Batch 5000 loss: 0.08944671678572619\n",
      "Epoch96, Batch 7000 loss: 0.08945355655979913\n",
      "Epoch96, Batch 9000 loss: 0.08957481947106259\n",
      "Epoch97, Batch 1000 loss: 0.09282605117857655\n",
      "Epoch97, Batch 3000 loss: 0.08940633505042989\n",
      "Epoch97, Batch 5000 loss: 0.08950158159594525\n",
      "Epoch97, Batch 7000 loss: 0.08938772201776418\n",
      "Epoch97, Batch 9000 loss: 0.08950086876950963\n",
      "Epoch98, Batch 1000 loss: 0.09296590792940422\n",
      "Epoch98, Batch 3000 loss: 0.08932062038853474\n",
      "Epoch98, Batch 5000 loss: 0.08933315209999251\n",
      "Epoch98, Batch 7000 loss: 0.08947606938405638\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "autoencoder = MarketAutoencoder()\n",
    "autoencoder.train(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.0780045  0.076282   0.07531677 0.07525466 0.07611936 0.0778018\n",
      " 0.07969296 0.0808845  0.0803526  0.07704843 0.069941   0.05874882\n",
      " 0.04307726 0.02333897 1.         1.         1.         1.\n",
      " 1.         0.98830882 0.97218843 0.95476932 0.93711246 0.91958668\n",
      " 0.90231896 0.88535058 0.8686924  0.85234448 0.83630307 0.82056316\n",
      " 0.80511934 0.78996614 0.77509812 0.76050993]\n",
      "[-4.60517019 -4.60517019 -4.60517019 -4.60517019 -4.60517019 -4.60517019\n",
      " -2.43036737 -2.45013425 -2.46138423 -0.84949068 -0.84776129 -0.8443964\n",
      " -0.84061408 -0.838231   -0.83929481 -0.84590313 -0.860118   -0.88250236\n",
      " -0.91384548 -0.95332205  1.          1.          1.          1.\n",
      "  1.          0.97661764  0.94437687  0.90953863  0.87422492  0.83917336\n",
      "  0.80463791  0.77070116  0.73738481  0.70468896  0.67260614  0.64112632\n",
      "  0.61023869  0.57993228  0.55019624  0.52101985]\n",
      "---------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x40 and 18x18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m k \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mencoder(th\u001b[38;5;241m.\u001b[39mfrom_numpy(sample))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(k)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x40 and 18x18)"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "example_path = paths[200]\n",
    "t = 252*5\n",
    "Swaptions = [0.0] + [example_path.Swaptions[i][t] for i in range(0,19)]\n",
    "Q_s =  [example_path.Q_s[i][t] for i in range(1,21)]\n",
    "sample = np.concat([Swaptions,Q_s])\n",
    "print(sample)\n",
    "autoencoder.preprocess(sample)\n",
    "print(sample)\n",
    "print(\"---------------------\")\n",
    "k = autoencoder.encoder(th.from_numpy(sample))\n",
    "print(k)\n",
    "print(\"---------------------\")\n",
    "recovered = autoencoder.decoder(k)\n",
    "print(recovered)\n",
    "print(autoencoder.deprocess(recovered.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"autoencoderT\",\"wb\") as fp:\n",
    "    pickle.dump(autoencoder,fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
