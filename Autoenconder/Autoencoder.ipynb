{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde30f52",
   "metadata": {},
   "source": [
    "### Setting up a quick and dirty simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6851c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import os, sys\n",
    "dir = os.path.abspath('')\n",
    "dir = os.path.dirname(dir)\n",
    "sys.path.append(dir)\n",
    "import MarketGeneratingFunctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8908012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoEncoder import MarketAutoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890e850",
   "metadata": {},
   "source": [
    "### Try Training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef151fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "import pickle\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths3 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths4 = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df91e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = paths1 + paths2 + paths3 + paths4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558fae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1, Batch 1000 loss: 0.13684981692247736\n",
      "Epoch1, Batch 3000 loss: 0.08549070596379772\n",
      "Epoch1, Batch 5000 loss: 0.06385182979189012\n",
      "Epoch1, Batch 7000 loss: 0.05093949684679352\n",
      "Epoch1, Batch 9000 loss: 0.04380477621700533\n",
      "Epoch2, Batch 1000 loss: 0.04113102392902849\n",
      "Epoch2, Batch 3000 loss: 0.03884253778498461\n",
      "Epoch2, Batch 5000 loss: 0.038203992902861285\n",
      "Epoch2, Batch 7000 loss: 0.0373877255892007\n",
      "Epoch2, Batch 9000 loss: 0.03620482014912593\n",
      "Epoch3, Batch 1000 loss: 0.03619432991678265\n",
      "Epoch3, Batch 3000 loss: 0.03444255185978065\n",
      "Epoch3, Batch 5000 loss: 0.03318414903119679\n",
      "Epoch3, Batch 7000 loss: 0.03296183907367389\n",
      "Epoch3, Batch 9000 loss: 0.03215357529872301\n",
      "Epoch4, Batch 1000 loss: 0.03239457740043037\n",
      "Epoch4, Batch 3000 loss: 0.031373299316625906\n",
      "Epoch4, Batch 5000 loss: 0.03023727773238375\n",
      "Epoch4, Batch 7000 loss: 0.029810747584892758\n",
      "Epoch4, Batch 9000 loss: 0.02937644669687138\n",
      "Epoch5, Batch 1000 loss: 0.02930295496372408\n",
      "Epoch5, Batch 3000 loss: 0.028278392260220796\n",
      "Epoch5, Batch 5000 loss: 0.027927875094240818\n",
      "Epoch5, Batch 7000 loss: 0.027303174380493834\n",
      "Epoch5, Batch 9000 loss: 0.026886722977169836\n",
      "Epoch6, Batch 1000 loss: 0.027469902778382942\n",
      "Epoch6, Batch 3000 loss: 0.0258138342982018\n",
      "Epoch6, Batch 5000 loss: 0.025632511626147617\n",
      "Epoch6, Batch 7000 loss: 0.02372212235872596\n",
      "Epoch6, Batch 9000 loss: 0.0225624245723138\n",
      "Epoch7, Batch 1000 loss: 0.021151821629784757\n",
      "Epoch7, Batch 3000 loss: 0.018211740993954804\n",
      "Epoch7, Batch 5000 loss: 0.01745493481265837\n",
      "Epoch7, Batch 7000 loss: 0.01656017016440103\n",
      "Epoch7, Batch 9000 loss: 0.015790813020229652\n",
      "Epoch8, Batch 1000 loss: 0.015662524771763862\n",
      "Epoch8, Batch 3000 loss: 0.015081004407673997\n",
      "Epoch8, Batch 5000 loss: 0.014694091053293246\n",
      "Epoch8, Batch 7000 loss: 0.01427571454059381\n",
      "Epoch8, Batch 9000 loss: 0.014067354389973327\n",
      "Epoch9, Batch 1000 loss: 0.014022085557032294\n",
      "Epoch9, Batch 3000 loss: 0.013652430401984255\n",
      "Epoch9, Batch 5000 loss: 0.013628325397708953\n",
      "Epoch9, Batch 7000 loss: 0.013248709585031425\n",
      "Epoch9, Batch 9000 loss: 0.013223287818131766\n",
      "Epoch10, Batch 1000 loss: 0.013130387250801167\n",
      "Epoch10, Batch 3000 loss: 0.012671122820618915\n",
      "Epoch10, Batch 5000 loss: 0.013062890856322762\n",
      "Epoch10, Batch 7000 loss: 0.012688810977285438\n",
      "Epoch10, Batch 9000 loss: 0.012064352822622117\n",
      "Epoch11, Batch 1000 loss: 0.012570626564831575\n",
      "Epoch11, Batch 3000 loss: 0.01178007333898486\n",
      "Epoch11, Batch 5000 loss: 0.012038914844816357\n",
      "Epoch11, Batch 7000 loss: 0.011841993828000448\n",
      "Epoch11, Batch 9000 loss: 0.011973189788671259\n",
      "Epoch12, Batch 1000 loss: 0.012103557387099886\n",
      "Epoch12, Batch 3000 loss: 0.011429119495292564\n",
      "Epoch12, Batch 5000 loss: 0.011371504789412503\n",
      "Epoch12, Batch 7000 loss: 0.011227864444390874\n",
      "Epoch12, Batch 9000 loss: 0.011424868982398941\n",
      "Epoch13, Batch 1000 loss: 0.011336969903676901\n",
      "Epoch13, Batch 3000 loss: 0.010906920426311816\n",
      "Epoch13, Batch 5000 loss: 0.010785360280005039\n",
      "Epoch13, Batch 7000 loss: 0.011088036635901938\n",
      "Epoch13, Batch 9000 loss: 0.010436896055450246\n",
      "Epoch14, Batch 1000 loss: 0.011004850890952083\n",
      "Epoch14, Batch 3000 loss: 0.010405547908364218\n",
      "Epoch14, Batch 5000 loss: 0.010426709656022005\n",
      "Epoch14, Batch 7000 loss: 0.010633810249467539\n",
      "Epoch14, Batch 9000 loss: 0.010203335856027954\n",
      "Epoch15, Batch 1000 loss: 0.010587802477025642\n",
      "Epoch15, Batch 3000 loss: 0.00988255029923106\n",
      "Epoch15, Batch 5000 loss: 0.009979211612783163\n",
      "Epoch15, Batch 7000 loss: 0.009901259882359117\n",
      "Epoch15, Batch 9000 loss: 0.010026757809452905\n",
      "Epoch16, Batch 1000 loss: 0.010073770102288443\n",
      "Epoch16, Batch 3000 loss: 0.00970930507514707\n",
      "Epoch16, Batch 5000 loss: 0.009853300079787004\n",
      "Epoch16, Batch 7000 loss: 0.009444110116246917\n",
      "Epoch16, Batch 9000 loss: 0.009850762132102528\n",
      "Epoch17, Batch 1000 loss: 0.009853871546798123\n",
      "Epoch17, Batch 3000 loss: 0.009548232988068462\n",
      "Epoch17, Batch 5000 loss: 0.009492086391167965\n",
      "Epoch17, Batch 7000 loss: 0.009445976585016443\n",
      "Epoch17, Batch 9000 loss: 0.00905972631657873\n",
      "Epoch18, Batch 1000 loss: 0.009979026988371557\n",
      "Epoch18, Batch 3000 loss: 0.009210483715230757\n",
      "Epoch18, Batch 5000 loss: 0.008955000931478021\n",
      "Epoch18, Batch 7000 loss: 0.009160140229439371\n",
      "Epoch18, Batch 9000 loss: 0.009021695848639582\n",
      "Epoch19, Batch 1000 loss: 0.009395161455931013\n",
      "Epoch19, Batch 3000 loss: 0.009005159180912485\n",
      "Epoch19, Batch 5000 loss: 0.008949210723785999\n",
      "Epoch19, Batch 7000 loss: 0.009134238547210145\n",
      "Epoch19, Batch 9000 loss: 0.008982438580988598\n",
      "Epoch20, Batch 1000 loss: 0.009039802886114136\n",
      "Epoch20, Batch 3000 loss: 0.0087348422190958\n",
      "Epoch20, Batch 5000 loss: 0.008535995028454226\n",
      "Epoch20, Batch 7000 loss: 0.008557481417061889\n",
      "Epoch20, Batch 9000 loss: 0.008599079835230781\n",
      "Epoch21, Batch 1000 loss: 0.00917022752797458\n",
      "Epoch21, Batch 3000 loss: 0.008545402745230706\n",
      "Epoch21, Batch 5000 loss: 0.008316831331533226\n",
      "Epoch21, Batch 7000 loss: 0.008541648552831444\n",
      "Epoch21, Batch 9000 loss: 0.00819011469305481\n",
      "Epoch22, Batch 1000 loss: 0.008469212393359602\n",
      "Epoch22, Batch 3000 loss: 0.00837138616490641\n",
      "Epoch22, Batch 5000 loss: 0.008171524272500164\n",
      "Epoch22, Batch 7000 loss: 0.008243914639742552\n",
      "Epoch22, Batch 9000 loss: 0.00825523281852658\n",
      "Epoch23, Batch 1000 loss: 0.008333077793477287\n",
      "Epoch23, Batch 3000 loss: 0.008016487902230572\n",
      "Epoch23, Batch 5000 loss: 0.00803902387662964\n",
      "Epoch23, Batch 7000 loss: 0.007924426466605928\n",
      "Epoch23, Batch 9000 loss: 0.007920561772632308\n",
      "Epoch24, Batch 1000 loss: 0.008208310745278504\n",
      "Epoch24, Batch 3000 loss: 0.008106645024672357\n",
      "Epoch24, Batch 5000 loss: 0.008018506118531178\n",
      "Epoch24, Batch 7000 loss: 0.007686751510495192\n",
      "Epoch24, Batch 9000 loss: 0.007830662119725613\n",
      "Epoch25, Batch 1000 loss: 0.007831826573114655\n",
      "Epoch25, Batch 3000 loss: 0.007721014877395868\n",
      "Epoch25, Batch 5000 loss: 0.007784588983692408\n",
      "Epoch25, Batch 7000 loss: 0.007377492450540343\n",
      "Epoch25, Batch 9000 loss: 0.007671994352040033\n",
      "Epoch26, Batch 1000 loss: 0.007782714139251178\n",
      "Epoch26, Batch 3000 loss: 0.007661100176870889\n",
      "Epoch26, Batch 5000 loss: 0.007491778511099858\n",
      "Epoch26, Batch 7000 loss: 0.00733039331583466\n",
      "Epoch26, Batch 9000 loss: 0.00730591755197952\n",
      "Epoch27, Batch 1000 loss: 0.007804880921694244\n",
      "Epoch27, Batch 3000 loss: 0.007082078178936303\n",
      "Epoch27, Batch 5000 loss: 0.007543937580281138\n",
      "Epoch27, Batch 7000 loss: 0.007245918020418201\n",
      "Epoch27, Batch 9000 loss: 0.007261800991994011\n",
      "Epoch28, Batch 1000 loss: 0.007313181290854769\n",
      "Epoch28, Batch 3000 loss: 0.0070317580332054045\n",
      "Epoch28, Batch 5000 loss: 0.00717727221336922\n",
      "Epoch28, Batch 7000 loss: 0.007225944273532197\n",
      "Epoch28, Batch 9000 loss: 0.007146719024510212\n",
      "Epoch29, Batch 1000 loss: 0.007324444945095895\n",
      "Epoch29, Batch 3000 loss: 0.007237841555706796\n",
      "Epoch29, Batch 5000 loss: 0.006973095588913424\n",
      "Epoch29, Batch 7000 loss: 0.00700045133838406\n",
      "Epoch29, Batch 9000 loss: 0.006876145881055503\n",
      "Epoch30, Batch 1000 loss: 0.007321759712087424\n",
      "Epoch30, Batch 3000 loss: 0.006872571233823123\n",
      "Epoch30, Batch 5000 loss: 0.007125758729621082\n",
      "Epoch30, Batch 7000 loss: 0.006652562739253735\n",
      "Epoch30, Batch 9000 loss: 0.006772677436768362\n",
      "Epoch31, Batch 1000 loss: 0.006918859652476738\n",
      "Epoch31, Batch 3000 loss: 0.006581374638299195\n",
      "Epoch31, Batch 5000 loss: 0.00679839136189175\n",
      "Epoch31, Batch 7000 loss: 0.006575567134107263\n",
      "Epoch31, Batch 9000 loss: 0.006488606620704269\n",
      "Epoch32, Batch 1000 loss: 0.007069550382862763\n",
      "Epoch32, Batch 3000 loss: 0.0067190932059244075\n",
      "Epoch32, Batch 5000 loss: 0.00658557422363873\n",
      "Epoch32, Batch 7000 loss: 0.006483144480910132\n",
      "Epoch32, Batch 9000 loss: 0.006574236891011316\n",
      "Epoch33, Batch 1000 loss: 0.006762225351409725\n",
      "Epoch33, Batch 3000 loss: 0.006434344438518571\n",
      "Epoch33, Batch 5000 loss: 0.006391212535964553\n",
      "Epoch33, Batch 7000 loss: 0.006262621161426009\n",
      "Epoch33, Batch 9000 loss: 0.0062062374488575265\n",
      "Epoch34, Batch 1000 loss: 0.006731567338107582\n",
      "Epoch34, Batch 3000 loss: 0.006316087968338812\n",
      "Epoch34, Batch 5000 loss: 0.0063869196554806745\n",
      "Epoch34, Batch 7000 loss: 0.006248393886726892\n",
      "Epoch34, Batch 9000 loss: 0.006468976854021379\n",
      "Epoch35, Batch 1000 loss: 0.006442265116559585\n",
      "Epoch35, Batch 3000 loss: 0.006306401861396891\n",
      "Epoch35, Batch 5000 loss: 0.006009501201973131\n",
      "Epoch35, Batch 7000 loss: 0.0062969593024383656\n",
      "Epoch35, Batch 9000 loss: 0.0061816288942036154\n",
      "Epoch36, Batch 1000 loss: 0.006288526242943871\n",
      "Epoch36, Batch 3000 loss: 0.006283762337058742\n",
      "Epoch36, Batch 5000 loss: 0.00630190283366159\n",
      "Epoch36, Batch 7000 loss: 0.005919424630334414\n",
      "Epoch36, Batch 9000 loss: 0.005842804459164582\n",
      "Epoch37, Batch 1000 loss: 0.006154396628887473\n",
      "Epoch37, Batch 3000 loss: 0.0059014423280601925\n",
      "Epoch37, Batch 5000 loss: 0.006028175035899048\n",
      "Epoch37, Batch 7000 loss: 0.0058033168275425755\n",
      "Epoch37, Batch 9000 loss: 0.00600963844855764\n",
      "Epoch38, Batch 1000 loss: 0.006259333378456844\n",
      "Epoch38, Batch 3000 loss: 0.0057633217931303046\n",
      "Epoch38, Batch 5000 loss: 0.005886393087087473\n",
      "Epoch38, Batch 7000 loss: 0.00572227395353698\n",
      "Epoch38, Batch 9000 loss: 0.005801060306812798\n",
      "Epoch39, Batch 1000 loss: 0.005879512496425952\n",
      "Epoch39, Batch 3000 loss: 0.0058327033663065075\n",
      "Epoch39, Batch 5000 loss: 0.005559151733090532\n",
      "Epoch39, Batch 7000 loss: 0.005660865979446207\n",
      "Epoch39, Batch 9000 loss: 0.005628640086666065\n",
      "Epoch40, Batch 1000 loss: 0.006089912607024542\n",
      "Epoch40, Batch 3000 loss: 0.0054735889247032165\n",
      "Epoch40, Batch 5000 loss: 0.005601597178862002\n",
      "Epoch40, Batch 7000 loss: 0.00550138607939345\n",
      "Epoch40, Batch 9000 loss: 0.005650540901591249\n",
      "Epoch41, Batch 1000 loss: 0.005671308209552842\n",
      "Epoch41, Batch 3000 loss: 0.005291338875225616\n",
      "Epoch41, Batch 5000 loss: 0.00537817994774044\n",
      "Epoch41, Batch 7000 loss: 0.005529237778926102\n",
      "Epoch41, Batch 9000 loss: 0.005298667620791937\n",
      "Epoch42, Batch 1000 loss: 0.0055806569320238225\n",
      "Epoch42, Batch 3000 loss: 0.005481014184518584\n",
      "Epoch42, Batch 5000 loss: 0.005309977864571944\n",
      "Epoch42, Batch 7000 loss: 0.005450743826158762\n",
      "Epoch42, Batch 9000 loss: 0.005276012862906279\n",
      "Epoch43, Batch 1000 loss: 0.005718573037176628\n",
      "Epoch43, Batch 3000 loss: 0.005218927128150747\n",
      "Epoch43, Batch 5000 loss: 0.005247571825642019\n",
      "Epoch43, Batch 7000 loss: 0.005249020936623238\n",
      "Epoch43, Batch 9000 loss: 0.004950329153354041\n",
      "Epoch44, Batch 1000 loss: 0.005519159468255167\n",
      "Epoch44, Batch 3000 loss: 0.005046100591380506\n",
      "Epoch44, Batch 5000 loss: 0.005303747568259002\n",
      "Epoch44, Batch 7000 loss: 0.00534086910569963\n",
      "Epoch44, Batch 9000 loss: 0.005126425192960959\n",
      "Epoch45, Batch 1000 loss: 0.00520907893200255\n",
      "Epoch45, Batch 3000 loss: 0.005398857586356325\n",
      "Epoch45, Batch 5000 loss: 0.004909841108129852\n",
      "Epoch45, Batch 7000 loss: 0.004881359424645063\n",
      "Epoch45, Batch 9000 loss: 0.005036258932637206\n",
      "Epoch46, Batch 1000 loss: 0.005139788621294277\n",
      "Epoch46, Batch 3000 loss: 0.00485147915745026\n",
      "Epoch46, Batch 5000 loss: 0.004912164301580435\n",
      "Epoch46, Batch 7000 loss: 0.004915941891911629\n",
      "Epoch46, Batch 9000 loss: 0.004863348328825424\n",
      "Epoch47, Batch 1000 loss: 0.004862373375208828\n",
      "Epoch47, Batch 3000 loss: 0.004906990364973641\n",
      "Epoch47, Batch 5000 loss: 0.004787136778815249\n",
      "Epoch47, Batch 7000 loss: 0.004916669080754953\n",
      "Epoch47, Batch 9000 loss: 0.004754528663104879\n",
      "Epoch48, Batch 1000 loss: 0.004973074706153228\n",
      "Epoch48, Batch 3000 loss: 0.004663044640003087\n",
      "Epoch48, Batch 5000 loss: 0.0047507000967089465\n",
      "Epoch48, Batch 7000 loss: 0.004556310388421663\n",
      "Epoch48, Batch 9000 loss: 0.004624085655149465\n",
      "Epoch49, Batch 1000 loss: 0.004844708857079734\n",
      "Epoch49, Batch 3000 loss: 0.004570307776444003\n",
      "Epoch49, Batch 5000 loss: 0.004735811283579254\n",
      "Epoch49, Batch 7000 loss: 0.004632767851558852\n",
      "Epoch49, Batch 9000 loss: 0.004633502501531321\n",
      "Epoch50, Batch 1000 loss: 0.00478412583165036\n",
      "Epoch50, Batch 3000 loss: 0.0045079819302376286\n",
      "Epoch50, Batch 5000 loss: 0.004463867397462008\n",
      "Epoch50, Batch 7000 loss: 0.0045382251702796375\n",
      "Epoch50, Batch 9000 loss: 0.004559998681351501\n",
      "Epoch51, Batch 1000 loss: 0.004498110752428132\n",
      "Epoch51, Batch 3000 loss: 0.004440807887179029\n",
      "Epoch51, Batch 5000 loss: 0.004405058992904351\n",
      "Epoch51, Batch 7000 loss: 0.004316359027205236\n",
      "Epoch51, Batch 9000 loss: 0.00443128042648448\n",
      "Epoch52, Batch 1000 loss: 0.004465665353916489\n",
      "Epoch52, Batch 3000 loss: 0.004385944427988115\n",
      "Epoch52, Batch 5000 loss: 0.0042573212949044235\n",
      "Epoch52, Batch 7000 loss: 0.004247025924723128\n",
      "Epoch52, Batch 9000 loss: 0.0048008538382444035\n",
      "Epoch53, Batch 1000 loss: 0.004375075429273894\n",
      "Epoch53, Batch 3000 loss: 0.004198540001855589\n",
      "Epoch53, Batch 5000 loss: 0.004269454508155746\n",
      "Epoch53, Batch 7000 loss: 0.004276847835105967\n",
      "Epoch53, Batch 9000 loss: 0.004183303887213647\n",
      "Epoch54, Batch 1000 loss: 0.004330119497119435\n",
      "Epoch54, Batch 3000 loss: 0.003986250809269621\n",
      "Epoch54, Batch 5000 loss: 0.00408643270173593\n",
      "Epoch54, Batch 7000 loss: 0.004146969853733222\n",
      "Epoch54, Batch 9000 loss: 0.004213925271821509\n",
      "Epoch55, Batch 1000 loss: 0.00418785230384829\n",
      "Epoch55, Batch 3000 loss: 0.004062309680181216\n",
      "Epoch55, Batch 5000 loss: 0.004072858942613992\n",
      "Epoch55, Batch 7000 loss: 0.004207508218310034\n",
      "Epoch55, Batch 9000 loss: 0.003925770299085559\n",
      "Epoch56, Batch 1000 loss: 0.004116698098987251\n",
      "Epoch56, Batch 3000 loss: 0.0039072513560927755\n",
      "Epoch56, Batch 5000 loss: 0.004041884859060271\n",
      "Epoch56, Batch 7000 loss: 0.003864556574724491\n",
      "Epoch56, Batch 9000 loss: 0.003997462523923992\n",
      "Epoch57, Batch 1000 loss: 0.004160858124835269\n",
      "Epoch57, Batch 3000 loss: 0.0038058396936164036\n",
      "Epoch57, Batch 5000 loss: 0.003910706832117052\n",
      "Epoch57, Batch 7000 loss: 0.0038030254361002525\n",
      "Epoch57, Batch 9000 loss: 0.0038730484063593798\n",
      "Epoch58, Batch 1000 loss: 0.003882725163859699\n",
      "Epoch58, Batch 3000 loss: 0.0037718072482072367\n",
      "Epoch58, Batch 5000 loss: 0.003793238941970863\n",
      "Epoch58, Batch 7000 loss: 0.0037679990330643934\n",
      "Epoch58, Batch 9000 loss: 0.00379769179816105\n",
      "Epoch59, Batch 1000 loss: 0.0038670952204574033\n",
      "Epoch59, Batch 3000 loss: 0.0037143550127431224\n",
      "Epoch59, Batch 5000 loss: 0.003897114545575177\n",
      "Epoch59, Batch 7000 loss: 0.003780714351252478\n",
      "Epoch59, Batch 9000 loss: 0.0036849725081570424\n",
      "Epoch60, Batch 1000 loss: 0.00381426357827778\n",
      "Epoch60, Batch 3000 loss: 0.0036594499456453324\n",
      "Epoch60, Batch 5000 loss: 0.0036095919055162393\n",
      "Epoch60, Batch 7000 loss: 0.003576654075157323\n",
      "Epoch60, Batch 9000 loss: 0.003684763775336916\n",
      "Epoch61, Batch 1000 loss: 0.0036392622953974655\n",
      "Epoch61, Batch 3000 loss: 0.0034769960254182075\n",
      "Epoch61, Batch 5000 loss: 0.0035628741275298627\n",
      "Epoch61, Batch 7000 loss: 0.0035630269437421378\n",
      "Epoch61, Batch 9000 loss: 0.003469329924106771\n",
      "Epoch62, Batch 1000 loss: 0.003565747769668015\n",
      "Epoch62, Batch 3000 loss: 0.003495807211446055\n",
      "Epoch62, Batch 5000 loss: 0.0032788485557578373\n",
      "Epoch62, Batch 7000 loss: 0.0033723975367512837\n",
      "Epoch62, Batch 9000 loss: 0.003523044939832241\n",
      "Epoch63, Batch 1000 loss: 0.003523618908496968\n",
      "Epoch63, Batch 3000 loss: 0.003403336549790525\n",
      "Epoch63, Batch 5000 loss: 0.0033500134575738766\n",
      "Epoch63, Batch 7000 loss: 0.0032998702377707643\n",
      "Epoch63, Batch 9000 loss: 0.0032953535909919893\n",
      "Epoch64, Batch 1000 loss: 0.0034754124026880524\n",
      "Epoch64, Batch 3000 loss: 0.0034953789652382374\n",
      "Epoch64, Batch 5000 loss: 0.003171117950575047\n",
      "Epoch64, Batch 7000 loss: 0.00331116303532908\n",
      "Epoch64, Batch 9000 loss: 0.003232946089898985\n",
      "Epoch65, Batch 1000 loss: 0.0032431312611710876\n",
      "Epoch65, Batch 3000 loss: 0.0032478660360015362\n",
      "Epoch65, Batch 5000 loss: 0.003168123041269251\n",
      "Epoch65, Batch 7000 loss: 0.003132588979743607\n",
      "Epoch65, Batch 9000 loss: 0.0032287495451228\n",
      "Epoch66, Batch 1000 loss: 0.0032611027934438537\n",
      "Epoch66, Batch 3000 loss: 0.003179347208127649\n",
      "Epoch66, Batch 5000 loss: 0.003103871816334483\n",
      "Epoch66, Batch 7000 loss: 0.0030587456839515693\n",
      "Epoch66, Batch 9000 loss: 0.0030700053043086935\n",
      "Epoch67, Batch 1000 loss: 0.003270607570612108\n",
      "Epoch67, Batch 3000 loss: 0.002911858400150932\n",
      "Epoch67, Batch 5000 loss: 0.0031531694426721875\n",
      "Epoch67, Batch 7000 loss: 0.002983641385313179\n",
      "Epoch67, Batch 9000 loss: 0.002976065222092647\n",
      "Epoch68, Batch 1000 loss: 0.0031484434565953586\n",
      "Epoch68, Batch 3000 loss: 0.0029969278805723645\n",
      "Epoch68, Batch 5000 loss: 0.0029921008033181734\n",
      "Epoch68, Batch 7000 loss: 0.0030169441318930053\n",
      "Epoch68, Batch 9000 loss: 0.0029792316653012674\n",
      "Epoch69, Batch 1000 loss: 0.0030409292084512016\n",
      "Epoch69, Batch 3000 loss: 0.00286981416376686\n",
      "Epoch69, Batch 5000 loss: 0.002862957908627909\n",
      "Epoch69, Batch 7000 loss: 0.0029329026022511086\n",
      "Epoch69, Batch 9000 loss: 0.0028473773282789625\n",
      "Epoch70, Batch 1000 loss: 0.0029603678841956245\n",
      "Epoch70, Batch 3000 loss: 0.002854430738499894\n",
      "Epoch70, Batch 5000 loss: 0.0027952762731597606\n",
      "Epoch70, Batch 7000 loss: 0.0028293001611580603\n",
      "Epoch70, Batch 9000 loss: 0.0027001696314522586\n",
      "Epoch71, Batch 1000 loss: 0.0028193387972510844\n",
      "Epoch71, Batch 3000 loss: 0.0027349899986365113\n",
      "Epoch71, Batch 5000 loss: 0.0026909168386083275\n",
      "Epoch71, Batch 7000 loss: 0.0027501135464457656\n",
      "Epoch71, Batch 9000 loss: 0.0027197408109307358\n",
      "Epoch72, Batch 1000 loss: 0.002834903394743272\n",
      "Epoch72, Batch 3000 loss: 0.002606081162513323\n",
      "Epoch72, Batch 5000 loss: 0.002698121835797277\n",
      "Epoch72, Batch 7000 loss: 0.0026202773618572775\n",
      "Epoch72, Batch 9000 loss: 0.0027236584141367057\n",
      "Epoch73, Batch 1000 loss: 0.0027130614122661463\n",
      "Epoch73, Batch 3000 loss: 0.00260506903504752\n",
      "Epoch73, Batch 5000 loss: 0.0025969046979460596\n",
      "Epoch73, Batch 7000 loss: 0.0025226027803297675\n",
      "Epoch73, Batch 9000 loss: 0.0026054585042295545\n",
      "Epoch74, Batch 1000 loss: 0.0026047253701579195\n",
      "Epoch74, Batch 3000 loss: 0.0025445177575877855\n",
      "Epoch74, Batch 5000 loss: 0.002471636512906502\n",
      "Epoch74, Batch 7000 loss: 0.0025125641504984185\n",
      "Epoch74, Batch 9000 loss: 0.0025502718636283323\n",
      "Epoch75, Batch 1000 loss: 0.0025876055002316996\n",
      "Epoch75, Batch 3000 loss: 0.002418621765521573\n",
      "Epoch75, Batch 5000 loss: 0.002400124924270624\n",
      "Epoch75, Batch 7000 loss: 0.0024455120758716882\n",
      "Epoch75, Batch 9000 loss: 0.002399434768647727\n",
      "Epoch76, Batch 1000 loss: 0.002553997100570259\n",
      "Epoch76, Batch 3000 loss: 0.0023167618509237374\n",
      "Epoch76, Batch 5000 loss: 0.0024362652892976856\n",
      "Epoch76, Batch 7000 loss: 0.0023491275774821917\n",
      "Epoch76, Batch 9000 loss: 0.0023610046104235085\n",
      "Epoch77, Batch 1000 loss: 0.0024058135160880834\n",
      "Epoch77, Batch 3000 loss: 0.00231625951351489\n",
      "Epoch77, Batch 5000 loss: 0.0022494633453704668\n",
      "Epoch77, Batch 7000 loss: 0.0023347010175276415\n",
      "Epoch77, Batch 9000 loss: 0.0022356086774071135\n",
      "Epoch78, Batch 1000 loss: 0.0022929016436538255\n",
      "Epoch78, Batch 3000 loss: 0.00223187946067416\n",
      "Epoch78, Batch 5000 loss: 0.002203934994723248\n",
      "Epoch78, Batch 7000 loss: 0.0022334689627298496\n",
      "Epoch78, Batch 9000 loss: 0.002190566803350648\n",
      "Epoch79, Batch 1000 loss: 0.0022660639826626\n",
      "Epoch79, Batch 3000 loss: 0.0021799716939438357\n",
      "Epoch79, Batch 5000 loss: 0.002106110977955135\n",
      "Epoch79, Batch 7000 loss: 0.0020900975854443457\n",
      "Epoch79, Batch 9000 loss: 0.0021165924718523545\n",
      "Epoch80, Batch 1000 loss: 0.0022252003409345324\n",
      "Epoch80, Batch 3000 loss: 0.002095909819080428\n",
      "Epoch80, Batch 5000 loss: 0.0020411817309644847\n",
      "Epoch80, Batch 7000 loss: 0.0020832689056302194\n",
      "Epoch80, Batch 9000 loss: 0.0020614606867496226\n",
      "Epoch81, Batch 1000 loss: 0.0020892941342158826\n",
      "Epoch81, Batch 3000 loss: 0.0020518309905867213\n",
      "Epoch81, Batch 5000 loss: 0.00199137898772574\n",
      "Epoch81, Batch 7000 loss: 0.0019825799579713875\n",
      "Epoch81, Batch 9000 loss: 0.002051087574755933\n",
      "Epoch82, Batch 1000 loss: 0.0020250749143538387\n",
      "Epoch82, Batch 3000 loss: 0.001913266352947223\n",
      "Epoch82, Batch 5000 loss: 0.0019594690789253716\n",
      "Epoch82, Batch 7000 loss: 0.0019206710558982768\n",
      "Epoch82, Batch 9000 loss: 0.001976570295371468\n",
      "Epoch83, Batch 1000 loss: 0.001945070927636884\n",
      "Epoch83, Batch 3000 loss: 0.001914380224651216\n",
      "Epoch83, Batch 5000 loss: 0.0018938940966316505\n",
      "Epoch83, Batch 7000 loss: 0.0018948255809266166\n",
      "Epoch83, Batch 9000 loss: 0.0018613390645907232\n",
      "Epoch84, Batch 1000 loss: 0.0019642182720976554\n",
      "Epoch84, Batch 3000 loss: 0.0018044373690770169\n",
      "Epoch84, Batch 5000 loss: 0.0018527887829648162\n",
      "Epoch84, Batch 7000 loss: 0.0018231183642555202\n",
      "Epoch84, Batch 9000 loss: 0.001783407584902998\n",
      "Epoch85, Batch 1000 loss: 0.0018986534593645053\n",
      "Epoch85, Batch 3000 loss: 0.0017390839978380244\n",
      "Epoch85, Batch 5000 loss: 0.0017913917441492989\n",
      "Epoch85, Batch 7000 loss: 0.0017903552323945081\n",
      "Epoch85, Batch 9000 loss: 0.0017985801862733576\n",
      "Epoch86, Batch 1000 loss: 0.0017782663421280465\n",
      "Epoch86, Batch 3000 loss: 0.0016865879780006367\n",
      "Epoch86, Batch 5000 loss: 0.001683142056735355\n",
      "Epoch86, Batch 7000 loss: 0.001728018379636258\n",
      "Epoch86, Batch 9000 loss: 0.0017064486254407085\n",
      "Epoch87, Batch 1000 loss: 0.001733057505050427\n",
      "Epoch87, Batch 3000 loss: 0.0016414730375282401\n",
      "Epoch87, Batch 5000 loss: 0.0016432461276482395\n",
      "Epoch87, Batch 7000 loss: 0.001654348866232183\n",
      "Epoch87, Batch 9000 loss: 0.001636220863070508\n",
      "Epoch88, Batch 1000 loss: 0.001674450028711905\n",
      "Epoch88, Batch 3000 loss: 0.001581360660650498\n",
      "Epoch88, Batch 5000 loss: 0.0015763144308423792\n",
      "Epoch88, Batch 7000 loss: 0.0016145578673877584\n",
      "Epoch88, Batch 9000 loss: 0.0016149031636185735\n",
      "Epoch89, Batch 1000 loss: 0.0016391823154473003\n",
      "Epoch89, Batch 3000 loss: 0.0015287330938401251\n",
      "Epoch89, Batch 5000 loss: 0.0015407066539597058\n",
      "Epoch89, Batch 7000 loss: 0.001542470629609125\n",
      "Epoch89, Batch 9000 loss: 0.0015485655050848234\n",
      "Epoch90, Batch 1000 loss: 0.0015951404537777416\n",
      "Epoch90, Batch 3000 loss: 0.0014933952352509155\n",
      "Epoch90, Batch 5000 loss: 0.0015163349098120359\n",
      "Epoch90, Batch 7000 loss: 0.0014840516403735078\n",
      "Epoch90, Batch 9000 loss: 0.0014862363824526793\n",
      "Epoch91, Batch 1000 loss: 0.001534428571548507\n",
      "Epoch91, Batch 3000 loss: 0.0014536151496088129\n",
      "Epoch91, Batch 5000 loss: 0.0014489147698225576\n",
      "Epoch91, Batch 7000 loss: 0.001457797339678881\n",
      "Epoch91, Batch 9000 loss: 0.0014540737262267419\n",
      "Epoch92, Batch 1000 loss: 0.001481049197579459\n",
      "Epoch92, Batch 3000 loss: 0.0014057079457771983\n",
      "Epoch92, Batch 5000 loss: 0.0014032404560173845\n",
      "Epoch92, Batch 7000 loss: 0.0014118552325073344\n",
      "Epoch92, Batch 9000 loss: 0.001413664271552928\n",
      "Epoch93, Batch 1000 loss: 0.001439701763366422\n",
      "Epoch93, Batch 3000 loss: 0.0013634677456464404\n",
      "Epoch93, Batch 5000 loss: 0.001366475150416863\n",
      "Epoch93, Batch 7000 loss: 0.001375540501629777\n",
      "Epoch93, Batch 9000 loss: 0.0013569432138828554\n",
      "Epoch94, Batch 1000 loss: 0.0013882446803453483\n",
      "Epoch94, Batch 3000 loss: 0.0013330311072155768\n",
      "Epoch94, Batch 5000 loss: 0.001326419648670295\n",
      "Epoch94, Batch 7000 loss: 0.0013266846227729231\n",
      "Epoch94, Batch 9000 loss: 0.0013207898360098147\n",
      "Epoch95, Batch 1000 loss: 0.0013528155179206235\n",
      "Epoch95, Batch 3000 loss: 0.0012870350473713032\n",
      "Epoch95, Batch 5000 loss: 0.001294000030496263\n",
      "Epoch95, Batch 7000 loss: 0.0012868586065077148\n",
      "Epoch95, Batch 9000 loss: 0.0012897151166911412\n",
      "Epoch96, Batch 1000 loss: 0.0013197129954763338\n",
      "Epoch96, Batch 3000 loss: 0.001255236162863552\n",
      "Epoch96, Batch 5000 loss: 0.0012586998813610376\n",
      "Epoch96, Batch 7000 loss: 0.0012568048796734053\n",
      "Epoch96, Batch 9000 loss: 0.0012549281039442642\n",
      "Epoch97, Batch 1000 loss: 0.0012873335066776862\n",
      "Epoch97, Batch 3000 loss: 0.001222149481107853\n",
      "Epoch97, Batch 5000 loss: 0.001222153772250567\n",
      "Epoch97, Batch 7000 loss: 0.0012244225049002965\n",
      "Epoch97, Batch 9000 loss: 0.001220804661889924\n",
      "Epoch98, Batch 1000 loss: 0.0012547899362886175\n",
      "Epoch98, Batch 3000 loss: 0.001197972776872559\n",
      "Epoch98, Batch 5000 loss: 0.0011935738275823074\n",
      "Epoch98, Batch 7000 loss: 0.0011938224221673924\n",
      "Epoch98, Batch 9000 loss: 0.001196772292181529\n",
      "Epoch99, Batch 1000 loss: 0.0012273231901110337\n",
      "Epoch99, Batch 3000 loss: 0.0011671987133805037\n",
      "Epoch99, Batch 5000 loss: 0.0011645780506767823\n",
      "Epoch99, Batch 7000 loss: 0.001168767761868048\n",
      "Epoch99, Batch 9000 loss: 0.001166597920317178\n",
      "Epoch100, Batch 1000 loss: 0.0012148118304492817\n",
      "Epoch100, Batch 3000 loss: 0.0011698680705811704\n",
      "Epoch100, Batch 5000 loss: 0.001170785432329712\n",
      "Epoch100, Batch 7000 loss: 0.0011697787316542019\n",
      "Epoch100, Batch 9000 loss: 0.0011687664543588163\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "autoencoder = MarketAutoencoder()\n",
    "autoencoder.train(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a3e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.00968653 0.02383407 0.0379732  0.05166884 0.06441092 0.0758303\n",
      " 0.08509396 0.09131183 0.09365956 0.09136251 0.08364954 0.07055068\n",
      " 0.05182462 0.02809936 1.         1.         1.         1.\n",
      " 1.         0.97138199 0.94950889 0.9303455  0.91237224 0.89503472\n",
      " 0.87812996 0.86158142 0.84535794 0.82944467 0.81383265 0.79851508\n",
      " 0.78348602 0.76873991 0.75427136 0.74007514]\n",
      "[-4.60517019 -4.60517019 -4.60517019 -4.60517019 -4.60517019 -4.60517019\n",
      " -3.92782064 -3.38628705 -3.03711268 -2.78597644 -2.59815252 -2.45538313\n",
      " -2.35288981 -2.28955206 -2.26664324 -2.28905195 -2.3681958  -2.5188687\n",
      " -2.78345355 -3.26755786  1.          1.          1.          1.\n",
      "  1.          0.94276398  0.89901779  0.86069101  0.82474448  0.79006945\n",
      "  0.75625992  0.72316284  0.69071589  0.65888935  0.62766529  0.59703015\n",
      "  0.56697204  0.53747982  0.50854273  0.48015028]\n",
      "---------------------\n",
      "tensor([-0.0096, -0.5630, -0.4998], dtype=torch.float64,\n",
      "       grad_fn=<TanhBackward0>)\n",
      "---------------------\n",
      "tensor([-4.6054, -4.6050, -4.6041, -4.5994, -4.6049, -4.6408, -3.9092, -3.3826,\n",
      "        -3.0350, -2.7852, -2.5973, -2.4548, -2.3527, -2.2900, -2.2671, -2.2906,\n",
      "        -2.3690, -2.5202, -2.7835, -3.2672,  1.0001,  1.0001,  0.9993,  0.9924,\n",
      "         0.9961,  0.9415,  0.8987,  0.8608,  0.8249,  0.7903,  0.7564,  0.7234,\n",
      "         0.6910,  0.6591,  0.6279,  0.5973,  0.5672,  0.5377,  0.5087,  0.4803],\n",
      "       dtype=torch.float64, grad_fn=<ViewBackward0>)\n",
      "tensor([-1.8096e-06,  1.9504e-06,  1.1144e-05,  5.7841e-05,  2.7717e-06,\n",
      "        -3.4974e-04,  1.0057e-02,  2.3958e-02,  3.8075e-02,  5.1720e-02,\n",
      "         6.4475e-02,  7.5884e-02,  8.5110e-02,  9.1269e-02,  9.3613e-02,\n",
      "         9.1204e-02,  8.3576e-02,  7.0441e-02,  5.1819e-02,  2.8114e-02,\n",
      "         1.0000e+00,  1.0000e+00,  9.9967e-01,  9.9620e-01,  9.9803e-01,\n",
      "         9.7073e-01,  9.4933e-01,  9.3039e-01,  9.1245e-01,  8.9515e-01,\n",
      "         8.7822e-01,  8.6171e-01,  8.4548e-01,  8.2956e-01,  8.1395e-01,\n",
      "         7.9863e-01,  7.8359e-01,  7.6883e-01,  7.5437e-01,  7.4016e-01],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "example_path = paths[150]\n",
    "t = 252*5\n",
    "Swaptions = [0.0] + [example_path.Swaptions[i][t] for i in range(0,19)]\n",
    "Q_s =  [example_path.Q_s[i][t] for i in range(1,21)]\n",
    "sample = np.concat([Swaptions,Q_s])\n",
    "print(sample)\n",
    "autoencoder.preprocess(sample)\n",
    "print(sample)\n",
    "print(\"---------------------\")\n",
    "k = autoencoder.encoder(th.from_numpy(sample))\n",
    "print(k)\n",
    "print(\"---------------------\")\n",
    "recovered = autoencoder.decoder(k)\n",
    "print(recovered)\n",
    "print(autoencoder.deprocess(recovered.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabcacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"autoencoderT\",\"wb\") as fp:\n",
    "    pickle.dump(autoencoder,fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
