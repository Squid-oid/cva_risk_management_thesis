{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "## Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Paths\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear LR schedule, see SB3 Documentation at https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "## Policy Kwargs\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128], vf=[512,512,256,128], optimizers_class = th.optim.Adam)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=60480, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005688921 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.7       |\n",
      "|    explained_variance   | -2.34       |\n",
      "|    learning_rate        | 0.00247     |\n",
      "|    loss                 | -0.00559    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.000894    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.316   |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0854      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077126115 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -56.6        |\n",
      "|    explained_variance   | -0.057       |\n",
      "|    learning_rate        | 0.00242      |\n",
      "|    loss                 | -0.00531     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 2.46e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0807     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007824284 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.5       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00237     |\n",
      "|    loss                 | -0.00635    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 1.51e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.311   |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009703192 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.3       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | -0.00371    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00479    |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 2.03e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.317   |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008593642 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.3       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00227     |\n",
      "|    loss                 | -0.0038     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 3.06e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.316   |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0531     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009024942 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56         |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00222     |\n",
      "|    loss                 | -0.00811    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 1.49e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 529      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0373     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009593927 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.9       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00217     |\n",
      "|    loss                 | -0.0069     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 2.09e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.307   |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 626      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0504      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 483840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073669306 |\n",
      "|    clip_fraction        | 0.0888       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -55.7        |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.00212      |\n",
      "|    loss                 | -0.00729     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 1.23e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.302   |\n",
      "| time/              |          |\n",
      "|    fps             | 672      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 719      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0502     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009878332 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.6       |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.00207     |\n",
      "|    loss                 | -0.00338    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 2.85e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.298   |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 810      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.061      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011777221 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.5       |\n",
      "|    explained_variance   | 0.0668      |\n",
      "|    learning_rate        | 0.00202     |\n",
      "|    loss                 | -0.00724    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00511    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 5.44e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.302   |\n",
      "| time/              |          |\n",
      "|    fps             | 666      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 907      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0503     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009773183 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.3       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.00197     |\n",
      "|    loss                 | -0.00774    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 1.64e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 994      |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0548     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010580172 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.1       |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.00192     |\n",
      "|    loss                 | -0.00835    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 2.22e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 663      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1093     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0459    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 786240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01038809 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -54.8      |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.00187    |\n",
      "|    loss                 | -0.00654   |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0058    |\n",
      "|    std                  | 0.954      |\n",
      "|    value_loss           | 7.12e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 658      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1194     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0496     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008901755 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.7       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | -0.00254    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0062     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 6.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.295   |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 1295     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.042     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 907200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00976386 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -54.4      |\n",
      "|    explained_variance   | 0.129      |\n",
      "|    learning_rate        | 0.00177    |\n",
      "|    loss                 | -0.00623   |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.00569   |\n",
      "|    std                  | 0.945      |\n",
      "|    value_loss           | 3.5e-06    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.297   |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 1403     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0449      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 967680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077719116 |\n",
      "|    clip_fraction        | 0.0935       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -54.2        |\n",
      "|    explained_variance   | 0.323        |\n",
      "|    learning_rate        | 0.00172      |\n",
      "|    loss                 | -0.00915     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    std                  | 0.94         |\n",
      "|    value_loss           | 4.63e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 1511     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0428     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008989461 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54         |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00167     |\n",
      "|    loss                 | -0.00625    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 7.78e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.283   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 1625     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0405     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011531881 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.8       |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | -0.00734    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00658    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 2.09e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.291   |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 1730     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0438     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009744931 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.5       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00157     |\n",
      "|    loss                 | -0.00755    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    std                  | 0.925       |\n",
      "|    value_loss           | 1.41e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.282   |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1838     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0328     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010604975 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.2       |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | -0.00769    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    std                  | 0.919       |\n",
      "|    value_loss           | 1.44e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.279   |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1942     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.026      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008457611 |\n",
      "|    clip_fraction        | 0.0962      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.9       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.00147     |\n",
      "|    loss                 | -0.0077     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.912       |\n",
      "|    value_loss           | 5.32e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.276   |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 2075     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.04 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0373     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009489216 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.6       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00839    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 1.86e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.273   |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 2198     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0289     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009446249 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.3       |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | -0.00968    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.901       |\n",
      "|    value_loss           | 6.79e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.283   |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 2309     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Training section  #\n",
    "#####################\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs'), # <- Set action and obs\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs')  # <- Set action and obs\n",
    "]),filename='logs-xsbig')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs'),    # <- Set action and obs\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/models/xsbig_best', # <- Remember to name best model save path\n",
    "    log_path='./logs/eval_logs/xsbig', # Remeber to name eval data path\n",
    "    eval_freq=252*12*5, \n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0025), policy_kwargs=policy_kwargs, n_steps=252*12*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "model.save('./logs/models/xsbig_final') # <- Save final model in case we need it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
