{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Paths\n",
    "with open(\"../0.7corr5001.pkl\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"../0.7corr5002.pkl\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabellebyman/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=10000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1.26e+03  |\n",
      "|    mean_reward     | -6.83e-06 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 10000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000233    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008831629  |\n",
      "|    clip_fraction        | 0.0575       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.020309329 |\n",
      "|    learning_rate        | 0.00498      |\n",
      "|    loss                 | -0.00527     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 0.00157      |\n",
      "------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -1.1163046 |\n",
      "| time/              |            |\n",
      "|    fps             | 779        |\n",
      "|    iterations      | 2          |\n",
      "|    time_elapsed    | 25         |\n",
      "|    total_timesteps | 20160      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00024     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010408258  |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.0003670454 |\n",
      "|    learning_rate        | 0.00497      |\n",
      "|    loss                 | 0.0718       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    std                  | 0.942        |\n",
      "|    value_loss           | 0.0539       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00153     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032223328 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | -1.7515459   |\n",
      "|    learning_rate        | 0.00495      |\n",
      "|    loss                 | 8.2e-05      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    std                  | 0.946        |\n",
      "|    value_loss           | 1.75e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.58258796 |\n",
      "| time/              |             |\n",
      "|    fps             | 691         |\n",
      "|    iterations      | 4           |\n",
      "|    time_elapsed    | 58          |\n",
      "|    total_timesteps | 40320       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0031     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01229416  |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | -0.01066494 |\n",
      "|    learning_rate        | 0.00493     |\n",
      "|    loss                 | -0.00318    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000946   |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 0.00053     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037456853 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | -0.19817233  |\n",
      "|    learning_rate        | 0.00492      |\n",
      "|    loss                 | -0.000286    |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    std                  | 0.931        |\n",
      "|    value_loss           | 1.84e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.45672858 |\n",
      "| time/              |             |\n",
      "|    fps             | 659         |\n",
      "|    iterations      | 6           |\n",
      "|    time_elapsed    | 91          |\n",
      "|    total_timesteps | 60480       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000401   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015206876 |\n",
      "|    clip_fraction        | 0.0827      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.004224956 |\n",
      "|    learning_rate        | 0.0049      |\n",
      "|    loss                 | 0.000442    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    std                  | 0.943       |\n",
      "|    value_loss           | 8.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00183    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004709755 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | -0.12264812 |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.00164    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    std                  | 0.92        |\n",
      "|    value_loss           | 1.85e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.36479536 |\n",
      "| time/              |             |\n",
      "|    fps             | 640         |\n",
      "|    iterations      | 8           |\n",
      "|    time_elapsed    | 125         |\n",
      "|    total_timesteps | 80640       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000795    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004953723  |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.0004965067 |\n",
      "|    learning_rate        | 0.00487      |\n",
      "|    loss                 | -0.00187     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.000134     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000465     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 100000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.012281107   |\n",
      "|    clip_fraction        | 0.0391        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.72         |\n",
      "|    explained_variance   | -0.0066336393 |\n",
      "|    learning_rate        | 0.00485       |\n",
      "|    loss                 | -0.0038       |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    std                  | 0.934         |\n",
      "|    value_loss           | 6.66e-05      |\n",
      "-------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -0.3271803 |\n",
      "| time/              |            |\n",
      "|    fps             | 627        |\n",
      "|    iterations      | 10         |\n",
      "|    time_elapsed    | 160        |\n",
      "|    total_timesteps | 100800     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00301     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 110000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028083017 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.0010252595 |\n",
      "|    learning_rate        | 0.00483      |\n",
      "|    loss                 | 0.000499     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000195    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 0.000374     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000321   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001683176 |\n",
      "|    clip_fraction        | 0.00786     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | -1.8907819  |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | 0.00139     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000659   |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 2.84e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.27931407 |\n",
      "| time/              |             |\n",
      "|    fps             | 614         |\n",
      "|    iterations      | 12          |\n",
      "|    time_elapsed    | 196         |\n",
      "|    total_timesteps | 120960      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000381    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006231527  |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | -0.010569215 |\n",
      "|    learning_rate        | 0.0048       |\n",
      "|    loss                 | 0.000384     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000756    |\n",
      "|    std                  | 0.906        |\n",
      "|    value_loss           | 1.81e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000506   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012320365 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | -0.31634057 |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    std                  | 0.848       |\n",
      "|    value_loss           | 2.43e-08    |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.263535 |\n",
      "| time/              |           |\n",
      "|    fps             | 600       |\n",
      "|    iterations      | 14        |\n",
      "|    time_elapsed    | 235       |\n",
      "|    total_timesteps | 141120    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -5.75e-05      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 150000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0061175516   |\n",
      "|    clip_fraction        | 0.0199         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.45          |\n",
      "|    explained_variance   | -4.9591064e-05 |\n",
      "|    learning_rate        | 0.00476        |\n",
      "|    loss                 | 0.00203        |\n",
      "|    n_updates            | 140            |\n",
      "|    policy_gradient_loss | -0.000525      |\n",
      "|    std                  | 0.823          |\n",
      "|    value_loss           | 0.000974       |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -0.00383       |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 160000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.014902418    |\n",
      "|    clip_fraction        | 0.0624         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.44          |\n",
      "|    explained_variance   | -0.00063860416 |\n",
      "|    learning_rate        | 0.00475        |\n",
      "|    loss                 | 0.000643       |\n",
      "|    n_updates            | 150            |\n",
      "|    policy_gradient_loss | -0.000709      |\n",
      "|    std                  | 0.837          |\n",
      "|    value_loss           | 0.00594        |\n",
      "--------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.30565795 |\n",
      "| time/              |             |\n",
      "|    fps             | 594         |\n",
      "|    iterations      | 16          |\n",
      "|    time_elapsed    | 271         |\n",
      "|    total_timesteps | 161280      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00479     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 170000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008986337  |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.48        |\n",
      "|    explained_variance   | -0.008439779 |\n",
      "|    learning_rate        | 0.00473      |\n",
      "|    loss                 | -0.00411     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    std                  | 0.865        |\n",
      "|    value_loss           | 0.00125      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00814    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005114359 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | -0.08742249 |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | 0.00121     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 6.88e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -0.4260285 |\n",
      "| time/              |            |\n",
      "|    fps             | 586        |\n",
      "|    iterations      | 18         |\n",
      "|    time_elapsed    | 309        |\n",
      "|    total_timesteps | 181440     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000293     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 190000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0039300723  |\n",
      "|    clip_fraction        | 0.0237        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.47         |\n",
      "|    explained_variance   | 0.00010383129 |\n",
      "|    learning_rate        | 0.0047        |\n",
      "|    loss                 | 0.0482        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000552     |\n",
      "|    std                  | 0.851         |\n",
      "|    value_loss           | 0.0561        |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00114     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031168247 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.49        |\n",
      "|    explained_variance   | -0.18827093  |\n",
      "|    learning_rate        | 0.00468      |\n",
      "|    loss                 | -0.00133     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 0.000352     |\n",
      "------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -0.3193556 |\n",
      "| time/              |            |\n",
      "|    fps             | 574        |\n",
      "|    iterations      | 20         |\n",
      "|    time_elapsed    | 350        |\n",
      "|    total_timesteps | 201600     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000555    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 210000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.013051583  |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | -0.054326057 |\n",
      "|    learning_rate        | 0.00466      |\n",
      "|    loss                 | -0.00465     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 3.03e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000486     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 220000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.015372137   |\n",
      "|    clip_fraction        | 0.0623        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.43         |\n",
      "|    explained_variance   | 0.00038820505 |\n",
      "|    learning_rate        | 0.00465       |\n",
      "|    loss                 | -0.00237      |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.0015       |\n",
      "|    std                  | 0.868         |\n",
      "|    value_loss           | 0.00121       |\n",
      "-------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.309643 |\n",
      "| time/              |           |\n",
      "|    fps             | 563       |\n",
      "|    iterations      | 22        |\n",
      "|    time_elapsed    | 393       |\n",
      "|    total_timesteps | 221760    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00223     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 230000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004187439  |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | -0.011538982 |\n",
      "|    learning_rate        | 0.00463      |\n",
      "|    loss                 | -0.00131     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000347    |\n",
      "|    std                  | 0.896        |\n",
      "|    value_loss           | 0.000306     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -0.000386      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 240000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0059215077   |\n",
      "|    clip_fraction        | 0.0404         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.51          |\n",
      "|    explained_variance   | -1.4424324e-05 |\n",
      "|    learning_rate        | 0.00461        |\n",
      "|    loss                 | -0.00261       |\n",
      "|    n_updates            | 230            |\n",
      "|    policy_gradient_loss | -0.00128       |\n",
      "|    std                  | 0.894          |\n",
      "|    value_loss           | 0.000404       |\n",
      "--------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.26e+03  |\n",
      "|    ep_rew_mean     | -0.323242 |\n",
      "| time/              |           |\n",
      "|    fps             | 564       |\n",
      "|    iterations      | 24        |\n",
      "|    time_elapsed    | 428       |\n",
      "|    total_timesteps | 241920    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.00121      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 250000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014292416   |\n",
      "|    clip_fraction        | 0.102         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.52         |\n",
      "|    explained_variance   | 0.00021404028 |\n",
      "|    learning_rate        | 0.0046        |\n",
      "|    loss                 | 0.00258       |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.00169      |\n",
      "|    std                  | 0.919         |\n",
      "|    value_loss           | 0.000228      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000179     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 260000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008358505   |\n",
      "|    clip_fraction        | 0.0564        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.55         |\n",
      "|    explained_variance   | -0.0001821518 |\n",
      "|    learning_rate        | 0.00458       |\n",
      "|    loss                 | 0.00386       |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.000892     |\n",
      "|    std                  | 0.929         |\n",
      "|    value_loss           | 0.00112       |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.32828316 |\n",
      "| time/              |             |\n",
      "|    fps             | 564         |\n",
      "|    iterations      | 26          |\n",
      "|    time_elapsed    | 464         |\n",
      "|    total_timesteps | 262080      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -5.93e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002494934 |\n",
      "|    clip_fraction        | 0.0224      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | -4.8581185  |\n",
      "|    learning_rate        | 0.00456     |\n",
      "|    loss                 | -0.00264    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    std                  | 0.904       |\n",
      "|    value_loss           | 3.6e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000229    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 280000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023640136 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | -0.035629988 |\n",
      "|    learning_rate        | 0.00455      |\n",
      "|    loss                 | -0.00438     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000891    |\n",
      "|    std                  | 0.905        |\n",
      "|    value_loss           | 1.23e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.31512725 |\n",
      "| time/              |             |\n",
      "|    fps             | 564         |\n",
      "|    iterations      | 28          |\n",
      "|    time_elapsed    | 500         |\n",
      "|    total_timesteps | 282240      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000296    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 290000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003715395  |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | -0.018427849 |\n",
      "|    learning_rate        | 0.00453      |\n",
      "|    loss                 | -0.000971    |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000399    |\n",
      "|    std                  | 0.878        |\n",
      "|    value_loss           | 1.06e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -0.00148       |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 300000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0014191375   |\n",
      "|    clip_fraction        | 0.000575       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -2.4           |\n",
      "|    explained_variance   | -0.00029850006 |\n",
      "|    learning_rate        | 0.00451        |\n",
      "|    loss                 | 0.000485       |\n",
      "|    n_updates            | 290            |\n",
      "|    policy_gradient_loss | -6.57e-05      |\n",
      "|    std                  | 0.876          |\n",
      "|    value_loss           | 9.46e-05       |\n",
      "--------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.32502964 |\n",
      "| time/              |             |\n",
      "|    fps             | 564         |\n",
      "|    iterations      | 30          |\n",
      "|    time_elapsed    | 536         |\n",
      "|    total_timesteps | 302400      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00126     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 310000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066655707 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.32        |\n",
      "|    explained_variance   | 0.011710882  |\n",
      "|    learning_rate        | 0.0045       |\n",
      "|    loss                 | -0.00209     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    std                  | 0.834        |\n",
      "|    value_loss           | 1.88e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00109     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066371984 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.21        |\n",
      "|    explained_variance   | -0.007440448 |\n",
      "|    learning_rate        | 0.00448      |\n",
      "|    loss                 | -0.0031      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    std                  | 0.792        |\n",
      "|    value_loss           | 4.93e-08     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.24153034 |\n",
      "| time/              |             |\n",
      "|    fps             | 564         |\n",
      "|    iterations      | 32          |\n",
      "|    time_elapsed    | 571         |\n",
      "|    total_timesteps | 322560      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000825     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 330000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00950389    |\n",
      "|    clip_fraction        | 0.0394        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.2          |\n",
      "|    explained_variance   | 0.00067412853 |\n",
      "|    learning_rate        | 0.00446       |\n",
      "|    loss                 | -0.00286      |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00135      |\n",
      "|    std                  | 0.806         |\n",
      "|    value_loss           | 4.39e-05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000342   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009593321 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.001990199 |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.00504    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 0.000198    |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.10987163 |\n",
      "| time/              |             |\n",
      "|    fps             | 563         |\n",
      "|    iterations      | 34          |\n",
      "|    time_elapsed    | 607         |\n",
      "|    total_timesteps | 342720      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00121    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004875198 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | -0.6646974  |\n",
      "|    learning_rate        | 0.00443     |\n",
      "|    loss                 | -0.00173    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 5.14e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -5.05e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007918794 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.009329557 |\n",
      "|    learning_rate        | 0.00441     |\n",
      "|    loss                 | -0.00403    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    std                  | 0.765       |\n",
      "|    value_loss           | 2.41e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.11171262 |\n",
      "| time/              |             |\n",
      "|    fps             | 563         |\n",
      "|    iterations      | 36          |\n",
      "|    time_elapsed    | 643         |\n",
      "|    total_timesteps | 362880      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000346     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 370000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0092730615  |\n",
      "|    clip_fraction        | 0.146         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.99         |\n",
      "|    explained_variance   | 0.00048196316 |\n",
      "|    learning_rate        | 0.0044        |\n",
      "|    loss                 | -0.00268      |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.00159      |\n",
      "|    std                  | 0.759         |\n",
      "|    value_loss           | 0.000897      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -4.92e-05     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 380000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007999076   |\n",
      "|    clip_fraction        | 0.0957        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.01         |\n",
      "|    explained_variance   | -0.0010486841 |\n",
      "|    learning_rate        | 0.00438       |\n",
      "|    loss                 | -0.00152      |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.00176      |\n",
      "|    std                  | 0.788         |\n",
      "|    value_loss           | 0.00101       |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.12057191 |\n",
      "| time/              |             |\n",
      "|    fps             | 563         |\n",
      "|    iterations      | 38          |\n",
      "|    time_elapsed    | 679         |\n",
      "|    total_timesteps | 383040      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000209    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 390000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.002594209  |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.03        |\n",
      "|    explained_variance   | -0.038562298 |\n",
      "|    learning_rate        | 0.00436      |\n",
      "|    loss                 | -0.00335     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000993    |\n",
      "|    std                  | 0.793        |\n",
      "|    value_loss           | 1.05e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00138     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097454665 |\n",
      "|    clip_fraction        | 0.0582       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.99        |\n",
      "|    explained_variance   | -0.03840661  |\n",
      "|    learning_rate        | 0.00434      |\n",
      "|    loss                 | -0.000692    |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    std                  | 0.769        |\n",
      "|    value_loss           | 2.15e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -0.1132181 |\n",
      "| time/              |            |\n",
      "|    fps             | 563        |\n",
      "|    iterations      | 40         |\n",
      "|    time_elapsed    | 715        |\n",
      "|    total_timesteps | 403200     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00225     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 410000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024248594 |\n",
      "|    clip_fraction        | 0.00309      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.97        |\n",
      "|    explained_variance   | 0.0009740591 |\n",
      "|    learning_rate        | 0.00433      |\n",
      "|    loss                 | -0.00138     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000179    |\n",
      "|    std                  | 0.78         |\n",
      "|    value_loss           | 7.87e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00234     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025859342 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.97        |\n",
      "|    explained_variance   | -0.2024411   |\n",
      "|    learning_rate        | 0.00431      |\n",
      "|    loss                 | -0.00141     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000499    |\n",
      "|    std                  | 0.775        |\n",
      "|    value_loss           | 3.78e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.08873905 |\n",
      "| time/              |             |\n",
      "|    fps             | 562         |\n",
      "|    iterations      | 42          |\n",
      "|    time_elapsed    | 752         |\n",
      "|    total_timesteps | 423360      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0016      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 430000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025090496 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.005585015  |\n",
      "|    learning_rate        | 0.00429      |\n",
      "|    loss                 | -0.00185     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000294    |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 5.13e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -0.00211       |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 440000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.008207895    |\n",
      "|    clip_fraction        | 0.0497         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.87          |\n",
      "|    explained_variance   | -0.00092577934 |\n",
      "|    learning_rate        | 0.00428        |\n",
      "|    loss                 | -0.00364       |\n",
      "|    n_updates            | 430            |\n",
      "|    policy_gradient_loss | -0.000644      |\n",
      "|    std                  | 0.73           |\n",
      "|    value_loss           | 1.44e-05       |\n",
      "--------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.091108285 |\n",
      "| time/              |              |\n",
      "|    fps             | 561          |\n",
      "|    iterations      | 44           |\n",
      "|    time_elapsed    | 790          |\n",
      "|    total_timesteps | 443520       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-0.02 +/- 0.03\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0213       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 450000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0067076297  |\n",
      "|    clip_fraction        | 0.0713        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.84         |\n",
      "|    explained_variance   | -0.0016229153 |\n",
      "|    learning_rate        | 0.00426       |\n",
      "|    loss                 | -0.00414      |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.0015       |\n",
      "|    std                  | 0.729         |\n",
      "|    value_loss           | 2.44e-05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0139       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 460000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0085820705  |\n",
      "|    clip_fraction        | 0.0409        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.83         |\n",
      "|    explained_variance   | -0.0025639534 |\n",
      "|    learning_rate        | 0.00424       |\n",
      "|    loss                 | 0.000434      |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000398     |\n",
      "|    std                  | 0.733         |\n",
      "|    value_loss           | 2.3e-05       |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.08381299 |\n",
      "| time/              |             |\n",
      "|    fps             | 560         |\n",
      "|    iterations      | 46          |\n",
      "|    time_elapsed    | 827         |\n",
      "|    total_timesteps | 463680      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-0.03 +/- 0.05\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0276       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 470000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009532681   |\n",
      "|    clip_fraction        | 0.0324        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.77         |\n",
      "|    explained_variance   | -0.0065288544 |\n",
      "|    learning_rate        | 0.00423       |\n",
      "|    loss                 | -0.000471     |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.00107      |\n",
      "|    std                  | 0.695         |\n",
      "|    value_loss           | 3.27e-06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-0.02 +/- 0.03\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0206      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033559173 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.37133718  |\n",
      "|    learning_rate        | 0.00421      |\n",
      "|    loss                 | -0.000474    |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.669        |\n",
      "|    value_loss           | 5.5e-08      |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.080806755 |\n",
      "| time/              |              |\n",
      "|    fps             | 558          |\n",
      "|    iterations      | 48           |\n",
      "|    time_elapsed    | 867          |\n",
      "|    total_timesteps | 483840       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00108     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 490000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134788975 |\n",
      "|    clip_fraction        | 0.164        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -0.000284791 |\n",
      "|    learning_rate        | 0.00419      |\n",
      "|    loss                 | -0.00511     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.662        |\n",
      "|    value_loss           | 2.32e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00143     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006400337  |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | -0.012615323 |\n",
      "|    learning_rate        | 0.00418      |\n",
      "|    loss                 | -0.00287     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.634        |\n",
      "|    value_loss           | 7.04e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.11382301 |\n",
      "| time/              |             |\n",
      "|    fps             | 556         |\n",
      "|    iterations      | 50          |\n",
      "|    time_elapsed    | 906         |\n",
      "|    total_timesteps | 504000      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.00023      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 510000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014425505   |\n",
      "|    clip_fraction        | 0.106         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.49         |\n",
      "|    explained_variance   | -8.332729e-05 |\n",
      "|    learning_rate        | 0.00416       |\n",
      "|    loss                 | -0.00357      |\n",
      "|    n_updates            | 500           |\n",
      "|    policy_gradient_loss | -0.0019       |\n",
      "|    std                  | 0.586         |\n",
      "|    value_loss           | 0.0029        |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000384   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004170943 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -1.7479575  |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | -0.00156    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00106    |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 8.12e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.09636354 |\n",
      "| time/              |             |\n",
      "|    fps             | 554         |\n",
      "|    iterations      | 52          |\n",
      "|    time_elapsed    | 945         |\n",
      "|    total_timesteps | 524160      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -5.9e-05     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 530000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032943347 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -3.681634    |\n",
      "|    learning_rate        | 0.00413      |\n",
      "|    loss                 | 0.000649     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.559        |\n",
      "|    value_loss           | 5.33e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00165     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 540000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025911634 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.111623764 |\n",
      "|    learning_rate        | 0.00411      |\n",
      "|    loss                 | -0.00116     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000434    |\n",
      "|    std                  | 0.544        |\n",
      "|    value_loss           | 1.37e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -0.0781382 |\n",
      "| time/              |            |\n",
      "|    fps             | 552        |\n",
      "|    iterations      | 54         |\n",
      "|    time_elapsed    | 985        |\n",
      "|    total_timesteps | 544320     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000389    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 550000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010031601  |\n",
      "|    clip_fraction        | 0.0774       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | -0.020924807 |\n",
      "|    learning_rate        | 0.00409      |\n",
      "|    loss                 | -0.0039      |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00472     |\n",
      "|    std                  | 0.505        |\n",
      "|    value_loss           | 5.81e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000112   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016421085 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.32815182  |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | -0.00239    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00638    |\n",
      "|    std                  | 0.46        |\n",
      "|    value_loss           | 4.24e-09    |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.07064192 |\n",
      "| time/              |             |\n",
      "|    fps             | 549         |\n",
      "|    iterations      | 56          |\n",
      "|    time_elapsed    | 1027        |\n",
      "|    total_timesteps | 564480      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -4.33e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 570000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013754539 |\n",
      "|    clip_fraction        | 0.0948      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.873      |\n",
      "|    explained_variance   | 0.47717774  |\n",
      "|    learning_rate        | 0.00406     |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    std                  | 0.423       |\n",
      "|    value_loss           | 3.47e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000277     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 580000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.012033104   |\n",
      "|    clip_fraction        | 0.185         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.883        |\n",
      "|    explained_variance   | -8.249283e-05 |\n",
      "|    learning_rate        | 0.00404       |\n",
      "|    loss                 | -0.00133      |\n",
      "|    n_updates            | 570           |\n",
      "|    policy_gradient_loss | -0.00239      |\n",
      "|    std                  | 0.44          |\n",
      "|    value_loss           | 0.00104       |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.09200771 |\n",
      "| time/              |             |\n",
      "|    fps             | 549         |\n",
      "|    iterations      | 58          |\n",
      "|    time_elapsed    | 1063        |\n",
      "|    total_timesteps | 584640      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -9.69e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 590000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008736549  |\n",
      "|    clip_fraction        | 0.0805       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.887       |\n",
      "|    explained_variance   | 0.0037139058 |\n",
      "|    learning_rate        | 0.00403      |\n",
      "|    loss                 | -0.00272     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    std                  | 0.432        |\n",
      "|    value_loss           | 0.000111     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -5.32e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 600000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00923322   |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.834       |\n",
      "|    explained_variance   | 0.0005354285 |\n",
      "|    learning_rate        | 0.00401      |\n",
      "|    loss                 | -0.00446     |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00242     |\n",
      "|    std                  | 0.423        |\n",
      "|    value_loss           | 0.00032      |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.096556425 |\n",
      "| time/              |              |\n",
      "|    fps             | 549          |\n",
      "|    iterations      | 60           |\n",
      "|    time_elapsed    | 1100         |\n",
      "|    total_timesteps | 604800       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000251    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 610000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017403837 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.812       |\n",
      "|    explained_variance   | -4.9134192   |\n",
      "|    learning_rate        | 0.00399      |\n",
      "|    loss                 | -0.00112     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000741    |\n",
      "|    std                  | 0.415        |\n",
      "|    value_loss           | 1.16e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -4.46e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 620000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004202844  |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.801       |\n",
      "|    explained_variance   | -0.029239655 |\n",
      "|    learning_rate        | 0.00398      |\n",
      "|    loss                 | -0.000575    |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000637    |\n",
      "|    std                  | 0.414        |\n",
      "|    value_loss           | 1.5e-06      |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.09181081 |\n",
      "| time/              |             |\n",
      "|    fps             | 550         |\n",
      "|    iterations      | 62          |\n",
      "|    time_elapsed    | 1136        |\n",
      "|    total_timesteps | 624960      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00026     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 630000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.01107326   |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.74        |\n",
      "|    explained_variance   | -0.010290146 |\n",
      "|    learning_rate        | 0.00396      |\n",
      "|    loss                 | -0.0018      |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.39         |\n",
      "|    value_loss           | 1.18e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0018      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040678857 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | -0.011660457 |\n",
      "|    learning_rate        | 0.00394      |\n",
      "|    loss                 | -0.00201     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000834    |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 2.73e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 1.26e+03   |\n",
      "|    ep_rew_mean     | -0.0906871 |\n",
      "| time/              |            |\n",
      "|    fps             | 549        |\n",
      "|    iterations      | 64         |\n",
      "|    time_elapsed    | 1173       |\n",
      "|    total_timesteps | 645120     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -5.17e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015942164 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | -0.01259768 |\n",
      "|    learning_rate        | 0.00392     |\n",
      "|    loss                 | -0.00206    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 1.73e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000355     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 660000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008183204   |\n",
      "|    clip_fraction        | 0.0467        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.518        |\n",
      "|    explained_variance   | -0.0014365911 |\n",
      "|    learning_rate        | 0.00391       |\n",
      "|    loss                 | -0.00298      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | -0.000574     |\n",
      "|    std                  | 0.355         |\n",
      "|    value_loss           | 4.75e-07      |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.06560972 |\n",
      "| time/              |             |\n",
      "|    fps             | 549         |\n",
      "|    iterations      | 66          |\n",
      "|    time_elapsed    | 1209        |\n",
      "|    total_timesteps | 665280      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000151    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 670000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155104585 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.10590768   |\n",
      "|    learning_rate        | 0.00389      |\n",
      "|    loss                 | -0.00903     |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00839     |\n",
      "|    std                  | 0.325        |\n",
      "|    value_loss           | 2.85e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -7.52e-05     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 680000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0066786027  |\n",
      "|    clip_fraction        | 0.0426        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.342        |\n",
      "|    explained_variance   | -0.0004312992 |\n",
      "|    learning_rate        | 0.00387       |\n",
      "|    loss                 | -0.00321      |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.000965     |\n",
      "|    std                  | 0.312         |\n",
      "|    value_loss           | 4.51e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.050152317 |\n",
      "| time/              |              |\n",
      "|    fps             | 550          |\n",
      "|    iterations      | 68           |\n",
      "|    time_elapsed    | 1246         |\n",
      "|    total_timesteps | 685440       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000502    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 690000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036357935 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | -0.749822    |\n",
      "|    learning_rate        | 0.00386      |\n",
      "|    loss                 | -0.00136     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    std                  | 0.301        |\n",
      "|    value_loss           | 7.37e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000255    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 700000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045159007 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | -5.920128    |\n",
      "|    learning_rate        | 0.00384      |\n",
      "|    loss                 | -0.00486     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    std                  | 0.287        |\n",
      "|    value_loss           | 4.11e-09     |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.04889551 |\n",
      "| time/              |             |\n",
      "|    fps             | 550         |\n",
      "|    iterations      | 70          |\n",
      "|    time_elapsed    | 1282        |\n",
      "|    total_timesteps | 705600      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -9.23e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 710000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038644327 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | -0.00762105  |\n",
      "|    learning_rate        | 0.00382      |\n",
      "|    loss                 | -0.00299     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.000922    |\n",
      "|    std                  | 0.279        |\n",
      "|    value_loss           | 9.47e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -7.98e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008473539 |\n",
      "|    clip_fraction        | 0.0755      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0195     |\n",
      "|    explained_variance   | -1.1025832  |\n",
      "|    learning_rate        | 0.00381     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    std                  | 0.262       |\n",
      "|    value_loss           | 1.07e-09    |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.047892615 |\n",
      "| time/              |              |\n",
      "|    fps             | 549          |\n",
      "|    iterations      | 72           |\n",
      "|    time_elapsed    | 1319         |\n",
      "|    total_timesteps | 725760       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -5.25e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 730000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.016613647  |\n",
      "|    clip_fraction        | 0.0861       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.103        |\n",
      "|    explained_variance   | -0.075969934 |\n",
      "|    learning_rate        | 0.00379      |\n",
      "|    loss                 | -0.00873     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00707     |\n",
      "|    std                  | 0.24         |\n",
      "|    value_loss           | 9.5e-10      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000314     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 740000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009022383   |\n",
      "|    clip_fraction        | 0.0905        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.155         |\n",
      "|    explained_variance   | -0.0016233921 |\n",
      "|    learning_rate        | 0.00377       |\n",
      "|    loss                 | 0.00167       |\n",
      "|    n_updates            | 730           |\n",
      "|    policy_gradient_loss | -0.00151      |\n",
      "|    std                  | 0.232         |\n",
      "|    value_loss           | 6.23e-08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.024609946 |\n",
      "| time/              |              |\n",
      "|    fps             | 550          |\n",
      "|    iterations      | 74           |\n",
      "|    time_elapsed    | 1355         |\n",
      "|    total_timesteps | 745920       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000682    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 750000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011188257  |\n",
      "|    clip_fraction        | 0.0882       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.222        |\n",
      "|    explained_variance   | 0.0023199916 |\n",
      "|    learning_rate        | 0.00376      |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000982    |\n",
      "|    std                  | 0.226        |\n",
      "|    value_loss           | 2.63e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -0.000497      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 760000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.012890613    |\n",
      "|    clip_fraction        | 0.106          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | 0.281          |\n",
      "|    explained_variance   | -5.6505203e-05 |\n",
      "|    learning_rate        | 0.00374        |\n",
      "|    loss                 | 0.00181        |\n",
      "|    n_updates            | 750            |\n",
      "|    policy_gradient_loss | -0.00161       |\n",
      "|    std                  | 0.222          |\n",
      "|    value_loss           | 2.19e-05       |\n",
      "--------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.017287726 |\n",
      "| time/              |              |\n",
      "|    fps             | 550          |\n",
      "|    iterations      | 76           |\n",
      "|    time_elapsed    | 1390         |\n",
      "|    total_timesteps | 766080       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.000174    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 770000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009352142  |\n",
      "|    clip_fraction        | 0.0905       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.299        |\n",
      "|    explained_variance   | -0.005847931 |\n",
      "|    learning_rate        | 0.00372      |\n",
      "|    loss                 | -0.00227     |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.22         |\n",
      "|    value_loss           | 3.11e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00126     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 780000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00544221   |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.332        |\n",
      "|    explained_variance   | 0.0022960901 |\n",
      "|    learning_rate        | 0.00371      |\n",
      "|    loss                 | -0.00339     |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000551    |\n",
      "|    std                  | 0.212        |\n",
      "|    value_loss           | 2.23e-07     |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.015591093 |\n",
      "| time/              |              |\n",
      "|    fps             | 551          |\n",
      "|    iterations      | 78           |\n",
      "|    time_elapsed    | 1425         |\n",
      "|    total_timesteps | 786240       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000345     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 790000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014680371   |\n",
      "|    clip_fraction        | 0.151         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.382         |\n",
      "|    explained_variance   | -0.0031445026 |\n",
      "|    learning_rate        | 0.00369       |\n",
      "|    loss                 | -0.00233      |\n",
      "|    n_updates            | 780           |\n",
      "|    policy_gradient_loss | -0.00187      |\n",
      "|    std                  | 0.21          |\n",
      "|    value_loss           | 4.85e-06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.000181     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 800000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.01520088    |\n",
      "|    clip_fraction        | 0.0933        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.443         |\n",
      "|    explained_variance   | -0.0015734434 |\n",
      "|    learning_rate        | 0.00367       |\n",
      "|    loss                 | -0.00415      |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    std                  | 0.201         |\n",
      "|    value_loss           | 6.15e-07      |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| rollout/           |             |\n",
      "|    ep_len_mean     | 1.26e+03    |\n",
      "|    ep_rew_mean     | -0.01768004 |\n",
      "| time/              |             |\n",
      "|    fps             | 552         |\n",
      "|    iterations      | 80          |\n",
      "|    time_elapsed    | 1460        |\n",
      "|    total_timesteps | 806400      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.00417      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 810000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009737958   |\n",
      "|    clip_fraction        | 0.119         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.507         |\n",
      "|    explained_variance   | 2.8908253e-05 |\n",
      "|    learning_rate        | 0.00366       |\n",
      "|    loss                 | 0.00298       |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | -0.00098      |\n",
      "|    std                  | 0.197         |\n",
      "|    value_loss           | 1.87e-05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.00066     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 820000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051261415 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.562        |\n",
      "|    explained_variance   | -0.002372861 |\n",
      "|    learning_rate        | 0.00364      |\n",
      "|    loss                 | 0.00183      |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00071     |\n",
      "|    std                  | 0.193        |\n",
      "|    value_loss           | 9.93e-07     |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.018200735 |\n",
      "| time/              |              |\n",
      "|    fps             | 552          |\n",
      "|    iterations      | 82           |\n",
      "|    time_elapsed    | 1494         |\n",
      "|    total_timesteps | 826560       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -0.00104       |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 830000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.013163907    |\n",
      "|    clip_fraction        | 0.156          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | 0.637          |\n",
      "|    explained_variance   | -0.00039064884 |\n",
      "|    learning_rate        | 0.00362        |\n",
      "|    loss                 | -0.00288       |\n",
      "|    n_updates            | 820            |\n",
      "|    policy_gradient_loss | -0.00261       |\n",
      "|    std                  | 0.184          |\n",
      "|    value_loss           | 5.05e-06       |\n",
      "--------------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.00135    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004495146 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.729       |\n",
      "|    explained_variance   | -0.42750263 |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | -0.000756   |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 7.67e-09    |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.013860023 |\n",
      "| time/              |              |\n",
      "|    fps             | 553          |\n",
      "|    iterations      | 84           |\n",
      "|    time_elapsed    | 1529         |\n",
      "|    total_timesteps | 846720       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000272   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 850000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009273747 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.851       |\n",
      "|    explained_variance   | -0.8611877  |\n",
      "|    learning_rate        | 0.00359     |\n",
      "|    loss                 | -0.00659    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 6.67e-10    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.000771   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016924161 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.959       |\n",
      "|    explained_variance   | 0.05737084  |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00708    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 4.09e-10    |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.013592404 |\n",
      "| time/              |              |\n",
      "|    fps             | 554          |\n",
      "|    iterations      | 86           |\n",
      "|    time_elapsed    | 1564         |\n",
      "|    total_timesteps | 866880       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -2e-05        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 870000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.01070603    |\n",
      "|    clip_fraction        | 0.136         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 0.996         |\n",
      "|    explained_variance   | -0.0020530224 |\n",
      "|    learning_rate        | 0.00356       |\n",
      "|    loss                 | -0.0039       |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    std                  | 0.158         |\n",
      "|    value_loss           | 1.46e-07      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1.26e+03       |\n",
      "|    mean_reward          | -9.71e-05      |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 880000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.007506668    |\n",
      "|    clip_fraction        | 0.0866         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | 0.951          |\n",
      "|    explained_variance   | -0.00028705597 |\n",
      "|    learning_rate        | 0.00354        |\n",
      "|    loss                 | -0.00373       |\n",
      "|    n_updates            | 870            |\n",
      "|    policy_gradient_loss | -0.0011        |\n",
      "|    std                  | 0.162          |\n",
      "|    value_loss           | 2.01e-07       |\n",
      "--------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.018296652 |\n",
      "| time/              |              |\n",
      "|    fps             | 554          |\n",
      "|    iterations      | 88           |\n",
      "|    time_elapsed    | 1598         |\n",
      "|    total_timesteps | 887040       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -6.37e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 890000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.022150572  |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.941        |\n",
      "|    explained_variance   | 0.0001821518 |\n",
      "|    learning_rate        | 0.00352      |\n",
      "|    loss                 | -0.00401     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    std                  | 0.164        |\n",
      "|    value_loss           | 8.8e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -4.05e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 900000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051987036 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.96         |\n",
      "|    explained_variance   | -1.2333386   |\n",
      "|    learning_rate        | 0.0035       |\n",
      "|    loss                 | -0.00124     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000954    |\n",
      "|    std                  | 0.161        |\n",
      "|    value_loss           | 3.1e-08      |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.017853063 |\n",
      "| time/              |              |\n",
      "|    fps             | 555          |\n",
      "|    iterations      | 90           |\n",
      "|    time_elapsed    | 1633         |\n",
      "|    total_timesteps | 907200       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -6.77e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 910000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031671561 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.987        |\n",
      "|    explained_variance   | -2.510275    |\n",
      "|    learning_rate        | 0.00349      |\n",
      "|    loss                 | -0.00223     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.000696    |\n",
      "|    std                  | 0.157        |\n",
      "|    value_loss           | 3.4e-09      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -4.01e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 920000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034233457 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.05         |\n",
      "|    explained_variance   | -0.648684    |\n",
      "|    learning_rate        | 0.00347      |\n",
      "|    loss                 | -0.000462    |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    std                  | 0.151        |\n",
      "|    value_loss           | 2.09e-09     |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.013987191 |\n",
      "| time/              |              |\n",
      "|    fps             | 555          |\n",
      "|    iterations      | 92           |\n",
      "|    time_elapsed    | 1668         |\n",
      "|    total_timesteps | 927360       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -6.43e-05    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029676475 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.12         |\n",
      "|    explained_variance   | -0.4435575   |\n",
      "|    learning_rate        | 0.00345      |\n",
      "|    loss                 | -0.000378    |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.146        |\n",
      "|    value_loss           | 2.19e-10     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -5.52e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011770389 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.22        |\n",
      "|    explained_variance   | -0.05718541 |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | -0.00895    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 1.09e-09    |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.013384447 |\n",
      "| time/              |              |\n",
      "|    fps             | 547          |\n",
      "|    iterations      | 94           |\n",
      "|    time_elapsed    | 1731         |\n",
      "|    total_timesteps | 947520       |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -2.02e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 950000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009314608 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.37        |\n",
      "|    explained_variance   | 0.044172823 |\n",
      "|    learning_rate        | 0.00342     |\n",
      "|    loss                 | -0.00583    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 5.34e-10    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 1260.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -8.49e-06     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 960000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.012478192   |\n",
      "|    clip_fraction        | 0.0842        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 1.45          |\n",
      "|    explained_variance   | 0.00045841932 |\n",
      "|    learning_rate        | 0.0034        |\n",
      "|    loss                 | -0.000296     |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -0.000622     |\n",
      "|    std                  | 0.121         |\n",
      "|    value_loss           | 9.48e-08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 1.26e+03     |\n",
      "|    ep_rew_mean     | -0.011015117 |\n",
      "| time/              |              |\n",
      "|    fps             | 135          |\n",
      "|    iterations      | 96           |\n",
      "|    time_elapsed    | 7135         |\n",
      "|    total_timesteps | 967680       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m\n",
      "\u001b[32m     44\u001b[39m policy_kwargs = \u001b[38;5;28mdict\u001b[39m(activation_fn=th.nn.LeakyReLU,\n",
      "\u001b[32m     45\u001b[39m                      net_arch=\u001b[38;5;28mdict\u001b[39m(pi=[\u001b[32m512\u001b[39m,\u001b[32m512\u001b[39m,\u001b[32m256\u001b[39m,\u001b[32m128\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m36\u001b[39m,\u001b[32m18\u001b[39m], vf=[\u001b[32m512\u001b[39m,\u001b[32m512\u001b[39m,\u001b[32m256\u001b[39m,\u001b[32m128\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m36\u001b[39m,\u001b[32m18\u001b[39m], optimizers_class = th.optim.Adam)) \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m     46\u001b[39m model = PPO(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, envs, batch_size = \u001b[32m252\u001b[39m*\u001b[32m2\u001b[39m*\u001b[32m5\u001b[39m, learning_rate=linear_schedule(\u001b[32m0.005\u001b[39m), policy_kwargs=policy_kwargs, n_steps=\u001b[32m252\u001b[39m*\u001b[32m4\u001b[39m*\u001b[32m5\u001b[39m, normalize_advantage=\u001b[38;5;28;01mTrue\u001b[39;00m, gamma = \u001b[32m0.9\u001b[39m, verbose = \u001b[32m1\u001b[39m) \n",
      "\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Save the agent\u001b[39;00m\n",
      "\u001b[32m     50\u001b[39m model.save(\u001b[33m\"\u001b[39m\u001b[33m0.7basfall\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n",
      "\u001b[32m    299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n",
      "\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n",
      "\u001b[32m    301\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[32m    307\u001b[39m ) -> SelfPPO:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n",
      "\u001b[32m    278\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.record(\u001b[33m\"\u001b[39m\u001b[33mtime/total_timesteps\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_timesteps, exclude=\u001b[33m\"\u001b[39m\u001b[33mtensorboard\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    279\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.dump(step=\u001b[38;5;28mself\u001b[39m.num_timesteps)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    283\u001b[39m callback.on_training_end()\n",
      "\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/stable_baselines3/ppo/ppo.py:272\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n",
      "\u001b[32m    271\u001b[39m \u001b[38;5;28mself\u001b[39m.policy.optimizer.zero_grad()\n",
      "\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n",
      "\u001b[32m    274\u001b[39m th.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy.parameters(), \u001b[38;5;28mself\u001b[39m.max_grad_norm)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n",
      "\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[32m    573\u001b[39m         Tensor.backward,\n",
      "\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n",
      "\u001b[32m    580\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n",
      "\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n",
      "\u001b[32m    342\u001b[39m     retain_graph = create_graph\n",
      "\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## LR schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "# Def Env\n",
    "def start_and_release(set, action = 'small', obs = 'xs'):\n",
    "    ret = tradingEng(set, action = action, obs = obs, reward='L2')\n",
    "    #del(set)\n",
    "    return ret\n",
    "\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: start_and_release(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: start_and_release(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    envs,\n",
    "    best_model_save_path='./logs/best_model',\n",
    "    log_path='./logs/eval_logs/',\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "# Save the agent\n",
    "model.save(\"0.7basfall\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./logs/eval_logs/evaluations.npz')\n",
    "timesteps = data['timesteps'].flatten()\n",
    "mean_rewards = data['results'].mean(axis=1)  # average reward per eval\n",
    "std_rewards = data['results'].std(axis=1)    # standard deviation\n",
    "\n",
    "\n",
    "# Convert to pandas Series for rolling\n",
    "timesteps_s = pd.Series(timesteps)\n",
    "mean_s = pd.Series(mean_rewards)\n",
    "std_s = pd.Series(std_rewards)\n",
    "\n",
    "# Rolling averages\n",
    "mean_smoothed = mean_s.rolling(window=10, min_periods=1).mean()\n",
    "std_smoothed = std_s.rolling(window=10, min_periods=1).mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(timesteps_s, mean_smoothed, label='Mean Eval Reward (Smoothed)', linewidth=2)\n",
    "plt.fill_between(timesteps_s, mean_smoothed - std_smoothed, mean_smoothed + std_smoothed,\n",
    "                 alpha=0.3, label='±1 Std. Dev.', color='tab:blue')\n",
    "\n",
    "plt.xlabel(\"Timesteps\", fontsize=12)\n",
    "plt.ylabel(\"Evaluation Reward\", fontsize=12)\n",
    "plt.title(\"PPO Convergence Over Time\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlim(1000000,3000000)\n",
    "plt.ylim(-0.005,0.001)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
