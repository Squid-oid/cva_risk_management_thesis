{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "## Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Paths\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear LR schedule, see SB3 Documentation at https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "## Policy Kwargs\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128], vf=[512,512,256,128], optimizers_class = th.optim.Adam)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=60480, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005688921 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.7       |\n",
      "|    explained_variance   | -2.34       |\n",
      "|    learning_rate        | 0.00247     |\n",
      "|    loss                 | -0.00559    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.000894    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.316   |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0854      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077126115 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -56.6        |\n",
      "|    explained_variance   | -0.057       |\n",
      "|    learning_rate        | 0.00242      |\n",
      "|    loss                 | -0.00531     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 2.46e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0807     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007824284 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.5       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00237     |\n",
      "|    loss                 | -0.00635    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 1.51e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.311   |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009703192 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.3       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | -0.00371    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00479    |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 2.03e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.317   |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008593642 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.3       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00227     |\n",
      "|    loss                 | -0.0038     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 3.06e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.316   |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0531     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009024942 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56         |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00222     |\n",
      "|    loss                 | -0.00811    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 1.49e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 529      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0373     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009593927 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.9       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00217     |\n",
      "|    loss                 | -0.0069     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 2.09e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.307   |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 626      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Training section  #\n",
    "#####################\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs'), # <- Set action and obs\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs')  # <- Set action and obs\n",
    "]),filename='logs-xsbig')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs'),    # <- Set action and obs\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/models/xsbig_best', # <- Remember to name best model save path\n",
    "    log_path='./logs/eval_logs/xsbig', # Remeber to name eval data path\n",
    "    eval_freq=252*12*5, \n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0025), policy_kwargs=policy_kwargs, n_steps=252*12*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "model.save('./logs/models/xsbig_final') # <- Save final model in case we need it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
