{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Paths\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## LR schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.26e+03 |\n",
      "|    mean_reward     | -0.0954  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40320, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0894     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002400974 |\n",
      "|    clip_fraction        | 0.00796     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | -0.656      |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | -0.00144    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000943   |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 6.68e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.415   |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004229068 |\n",
      "|    clip_fraction        | 0.0114      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.0616      |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00376    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 8.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0986      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80640        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052167685 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.0772       |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | -0.00286     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 1.27e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.415   |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0907      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060905833 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.57        |\n",
      "|    explained_variance   | 0.0964       |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | -0.00409     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.856        |\n",
      "|    value_loss           | 7.26e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0993      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034098278 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00226     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 8.53e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.393   |\n",
      "| time/              |          |\n",
      "|    fps             | 659      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0735      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 141120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052491054 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.43        |\n",
      "|    explained_variance   | 0.0946       |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00254     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    std                  | 0.797        |\n",
      "|    value_loss           | 4.31e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.064      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007425795 |\n",
      "|    clip_fraction        | 0.0248      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.00476    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00364    |\n",
      "|    std                  | 0.761       |\n",
      "|    value_loss           | 2.88e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.372   |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0931     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005962954 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.22       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.00372    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00249    |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 3.61e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0994     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010020916 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.1        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00147     |\n",
      "|    loss                 | -0.00972    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.681       |\n",
      "|    value_loss           | 1.16e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.337   |\n",
      "| time/              |          |\n",
      "|    fps             | 672      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.13 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.128       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 221760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063686315 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.99        |\n",
      "|    explained_variance   | 0.296        |\n",
      "|    learning_rate        | 0.00147      |\n",
      "|    loss                 | -0.00666     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00378     |\n",
      "|    std                  | 0.646        |\n",
      "|    value_loss           | 1.66e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0806    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 241920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00825065 |\n",
      "|    clip_fraction        | 0.0379     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.87      |\n",
      "|    explained_variance   | 0.215      |\n",
      "|    learning_rate        | 0.00147    |\n",
      "|    loss                 | -0.00946   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00567   |\n",
      "|    std                  | 0.612      |\n",
      "|    value_loss           | 8.02e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.307   |\n",
      "| time/              |          |\n",
      "|    fps             | 673      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0869      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051328745 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.77        |\n",
      "|    explained_variance   | 0.29         |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | -0.00612     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    std                  | 0.583        |\n",
      "|    value_loss           | 3.03e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0843      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 282240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057059247 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.68        |\n",
      "|    explained_variance   | 0.0708       |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | -0.00359     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    std                  | 0.56         |\n",
      "|    value_loss           | 7.96e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.284   |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 417      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0834     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007485584 |\n",
      "|    clip_fraction        | 0.0251      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.007      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    std                  | 0.541       |\n",
      "|    value_loss           | 2.85e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0874     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005435189 |\n",
      "|    clip_fraction        | 0.0173      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.00334    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    std                  | 0.526       |\n",
      "|    value_loss           | 3.81e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.266   |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 476      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0801      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 342720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051389355 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.00611     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    std                  | 0.502        |\n",
      "|    value_loss           | 1.62e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0801     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010360683 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    std                  | 0.467       |\n",
      "|    value_loss           | 4.77e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.241   |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 540      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0781     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008955691 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.00525    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00495    |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 4.9e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008041803 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    std                  | 0.414       |\n",
      "|    value_loss           | 3.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.214   |\n",
      "| time/              |          |\n",
      "|    fps             | 672      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 599      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0979      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100789005 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.926       |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00954     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00737     |\n",
      "|    std                  | 0.387        |\n",
      "|    value_loss           | 1.94e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0792      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057312516 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.824       |\n",
      "|    explained_variance   | 0.199        |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00545     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    std                  | 0.375        |\n",
      "|    value_loss           | 2.19e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.199   |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.092      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005420853 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.0051     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    std                  | 0.357       |\n",
      "|    value_loss           | 6.13e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.082      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007743366 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00634    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    std                  | 0.343       |\n",
      "|    value_loss           | 3.63e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.183   |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 713      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003985664 |\n",
      "|    clip_fraction        | 0.0106      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00236    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    std                  | 0.328       |\n",
      "|    value_loss           | 9e-07       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.087      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005302476 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00377    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00318    |\n",
      "|    std                  | 0.317       |\n",
      "|    value_loss           | 6.65e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.175   |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 770      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.115       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048639355 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.35         |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.00334     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    std                  | 0.305        |\n",
      "|    value_loss           | 7.56e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0993     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007244802 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00657    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 0.288       |\n",
      "|    value_loss           | 2.53e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.163   |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 826      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0888      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 584640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065183076 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.366        |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.00504     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    std                  | 0.275        |\n",
      "|    value_loss           | 3.83e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0717      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068172757 |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00205     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    std                  | 0.268        |\n",
      "|    value_loss           | 1.54e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.156   |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 882      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0894     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008280405 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0715     |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.000165   |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    std                  | 0.259       |\n",
      "|    value_loss           | 4.95e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0817     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009466203 |\n",
      "|    clip_fraction        | 0.0588      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.00162    |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.000479   |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    std                  | 0.252       |\n",
      "|    value_loss           | 8.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.149   |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 938      |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0795      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074006035 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0603       |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.0014       |\n",
      "|    loss                 | -0.00373     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 0.244        |\n",
      "|    value_loss           | 6.67e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0934      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073621543 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.128        |\n",
      "|    explained_variance   | 0.219        |\n",
      "|    learning_rate        | 0.0014       |\n",
      "|    loss                 | -0.0074      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    std                  | 0.236        |\n",
      "|    value_loss           | 3.94e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.144   |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 997      |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.108       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 705600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053730765 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.191        |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0014       |\n",
      "|    loss                 | -0.0042      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.228        |\n",
      "|    value_loss           | 4.29e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 1.26e+03 |\n",
      "|    mean_reward          | -0.0754  |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 725760   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.004261 |\n",
      "|    clip_fraction        | 0.0487   |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0.263    |\n",
      "|    explained_variance   | 0.372    |\n",
      "|    learning_rate        | 0.00139  |\n",
      "|    loss                 | -0.00958 |\n",
      "|    n_updates            | 350      |\n",
      "|    policy_gradient_loss | -0.00398 |\n",
      "|    std                  | 0.221    |\n",
      "|    value_loss           | 1.74e-07 |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.135   |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 1056     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0938     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007902393 |\n",
      "|    clip_fraction        | 0.0774      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.336       |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00248    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.214       |\n",
      "|    value_loss           | 1.32e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0765     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008960258 |\n",
      "|    clip_fraction        | 0.0946      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.406       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00642    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    std                  | 0.206       |\n",
      "|    value_loss           | 2.74e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.129   |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1115     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.119      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012675561 |\n",
      "|    clip_fraction        | 0.0984      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.475       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00315    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    std                  | 0.2         |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0779     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006585329 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.525       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00199    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    std                  | 0.195       |\n",
      "|    value_loss           | 2.21e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.123   |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1174     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0819     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007815546 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.573       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00471    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.000579    |\n",
      "|    std                  | 0.191       |\n",
      "|    value_loss           | 8.29e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0719     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020178666 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.612       |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    std                  | 0.188       |\n",
      "|    value_loss           | 1.04e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1233     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0774     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008171851 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.65        |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | -0.00375    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    std                  | 0.184       |\n",
      "|    value_loss           | 1.01e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0811    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 887040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01738232 |\n",
      "|    clip_fraction        | 0.077      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.699      |\n",
      "|    explained_variance   | -0.322     |\n",
      "|    learning_rate        | 0.00137    |\n",
      "|    loss                 | 0.00319    |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00031   |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 1.28e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.114   |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1292     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0758    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 907200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02179355 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.734      |\n",
      "|    explained_variance   | -0.289     |\n",
      "|    learning_rate        | 0.00137    |\n",
      "|    loss                 | -0.00196   |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | 0.00232    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 2.24e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0764    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 927360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01594349 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.776      |\n",
      "|    explained_variance   | 0.0468     |\n",
      "|    learning_rate        | 0.00136    |\n",
      "|    loss                 | 0.00405    |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.000229  |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 2.35e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1350     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0811     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007262891 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.828       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | -0.00154    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 2.45e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0628     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024683613 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.863       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.000891    |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 1.79e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1410     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0657     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016745705 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.901       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.00324    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.000131    |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 2.48e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0884      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064451806 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.928        |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00318     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.000549    |\n",
      "|    std                  | 0.161        |\n",
      "|    value_loss           | 2.32e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.097   |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1469     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0687     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037500102 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.967       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | 0.00723     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | 0.00472     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032883026 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.01        |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 1.36e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0935  |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1528     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0984   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1068480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0632676 |\n",
      "|    clip_fraction        | 0.253     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 1.03      |\n",
      "|    explained_variance   | 0.19      |\n",
      "|    learning_rate        | 0.00134   |\n",
      "|    loss                 | 0.0363    |\n",
      "|    n_updates            | 520       |\n",
      "|    policy_gradient_loss | 0.00948   |\n",
      "|    std                  | 0.153     |\n",
      "|    value_loss           | 9.51e-08  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0713     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102797315 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.06        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.00134     |\n",
      "|    loss                 | 0.00619     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 3.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1584     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0689     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1108800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013294292 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.08        |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.00134     |\n",
      "|    loss                 | -0.00446    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | 0.00638     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 4.58e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0626     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048866376 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.1         |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00133     |\n",
      "|    loss                 | 0.00712     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | 0.0201      |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 2.02e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0952  |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1640     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0732    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1149120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06781423 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.1        |\n",
      "|    explained_variance   | 0.14       |\n",
      "|    learning_rate        | 0.00133    |\n",
      "|    loss                 | 0.0104     |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | 0.0103     |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 7.38e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0818    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1169280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01321567 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.1        |\n",
      "|    explained_variance   | 0.254      |\n",
      "|    learning_rate        | 0.00133    |\n",
      "|    loss                 | 0.00275    |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | 0.00416    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 3.53e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1696     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0732     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1189440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019537255 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.13        |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00309    |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.000226   |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 1.61e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0632     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013282579 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.16        |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0003     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -4.6e-05    |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 1.93e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1753     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0563     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024437333 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.19        |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00867     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | 0.00363     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 9.67e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1249920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0637     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010945763 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.22        |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.000116    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.000115   |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 2.29e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0987  |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1809     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0803     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048229396 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.25        |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 3.2e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0802     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044303246 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.26        |\n",
      "|    explained_variance   | 0.0672      |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 2.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0925  |\n",
      "| time/              |          |\n",
      "|    fps             | 691      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1865     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0684     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035592142 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.27        |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | 0.0255      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | 0.0295      |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 3.25e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0817    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1330560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07173825 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.28       |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 0.0013     |\n",
      "|    loss                 | 0.0221     |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | 0.0148     |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 2.45e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0882  |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1921     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0813     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035091713 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.29        |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 1.64e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0795     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010985056 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.31        |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | -0.00364    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.00271     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 4.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0839  |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1977     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.084      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013170365 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.36        |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | -0.00237    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 8.23e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0702    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1411200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06256936 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.39       |\n",
      "|    explained_variance   | 0.229      |\n",
      "|    learning_rate        | 0.00129    |\n",
      "|    loss                 | 0.0126     |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | 0.000839   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 9.45e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0864  |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 2033     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.066     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1431360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03588184 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.43       |\n",
      "|    explained_variance   | 0.266      |\n",
      "|    learning_rate        | 0.00129    |\n",
      "|    loss                 | 0.0257     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | 0.00577    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 5.93e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0729     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024714971 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.46        |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.0076      |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 2.17e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0859  |\n",
      "| time/              |          |\n",
      "|    fps             | 694      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 2089     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0674     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013374591 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.47        |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | -0.000637   |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | 0.00229     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 3.36e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0557     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008379513 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.5         |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | -0.00512    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.00308     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 4.15e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.086   |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 2146     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0563     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008202021 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.53        |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | 0.000243    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | 0.00267     |\n",
      "|    std                  | 0.119       |\n",
      "|    value_loss           | 1.55e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0669    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1532160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02432614 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.57       |\n",
      "|    explained_variance   | 0.509      |\n",
      "|    learning_rate        | 0.00127    |\n",
      "|    loss                 | 0.00682    |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | 0.00417    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 4.51e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0812  |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 2202     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0669     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010344118 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.6         |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 0.00812     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.0155      |\n",
      "|    std                  | 0.116       |\n",
      "|    value_loss           | 5.18e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0577     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018467221 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.61        |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 0.00154     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 7.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0798  |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 2257     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034583848 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.63        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.00394     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    std                  | 0.113       |\n",
      "|    value_loss           | 1.52e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.054      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018015029 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.66        |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | -0.00343    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.0132      |\n",
      "|    std                  | 0.112       |\n",
      "|    value_loss           | 7.64e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.077   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 2313     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0506     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012813414 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.68        |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | -0.000492   |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    std                  | 0.111       |\n",
      "|    value_loss           | 3.52e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1653120, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0754    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1653120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03027398 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.7        |\n",
      "|    explained_variance   | 0.194      |\n",
      "|    learning_rate        | 0.00126    |\n",
      "|    loss                 | 0.00865    |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | 0.00393    |\n",
      "|    std                  | 0.11       |\n",
      "|    value_loss           | 5.77e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.079   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 2369     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0636    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1673280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08428319 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.71       |\n",
      "|    explained_variance   | 0.266      |\n",
      "|    learning_rate        | 0.00125    |\n",
      "|    loss                 | 0.0268     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | 0.015      |\n",
      "|    std                  | 0.109      |\n",
      "|    value_loss           | 2.33e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021954242 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.73        |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | 0.00435     |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | 0.00576     |\n",
      "|    std                  | 0.108       |\n",
      "|    value_loss           | 5.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0781  |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 2425     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0545    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1713600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02514657 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.75       |\n",
      "|    explained_variance   | 0.293      |\n",
      "|    learning_rate        | 0.00125    |\n",
      "|    loss                 | -0.000721  |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | 0.0161     |\n",
      "|    std                  | 0.107      |\n",
      "|    value_loss           | 1.74e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0545     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028426787 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.77        |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | 0.00272     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.00398     |\n",
      "|    std                  | 0.106       |\n",
      "|    value_loss           | 8.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0768  |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 2480     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0606    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1753920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07449939 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.79       |\n",
      "|    explained_variance   | 0.302      |\n",
      "|    learning_rate        | 0.00124    |\n",
      "|    loss                 | 0.0256     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | 0.0257     |\n",
      "|    std                  | 0.105      |\n",
      "|    value_loss           | 1.49e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0507     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019699978 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.8         |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | 0.00185     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.00194     |\n",
      "|    std                  | 0.104       |\n",
      "|    value_loss           | 3.92e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0758  |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 2536     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0562    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1794240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01371249 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.82       |\n",
      "|    explained_variance   | 0.395      |\n",
      "|    learning_rate        | 0.00123    |\n",
      "|    loss                 | -0.000705  |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | 0.00495    |\n",
      "|    std                  | 0.104      |\n",
      "|    value_loss           | 9.03e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0702    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1814400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02627582 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.84       |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.00123    |\n",
      "|    loss                 | 0.00367    |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | 0.0107     |\n",
      "|    std                  | 0.102      |\n",
      "|    value_loss           | 2.31e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0742  |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 2592     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0771     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024977673 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.85        |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | 0.00146     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.00693     |\n",
      "|    std                  | 0.102       |\n",
      "|    value_loss           | 2.02e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0608     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010240961 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.86        |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.0017      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00139     |\n",
      "|    std                  | 0.101       |\n",
      "|    value_loss           | 8.93e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0743  |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 2647     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0535    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1874880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03402206 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.88       |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.00122    |\n",
      "|    loss                 | 0.00293    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | 0.0026     |\n",
      "|    std                  | 0.101      |\n",
      "|    value_loss           | 1.47e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0589     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017980846 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.89        |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | 0.00445     |\n",
      "|    std                  | 0.101       |\n",
      "|    value_loss           | 6.76e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0772  |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 2703     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0347    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1915200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10236776 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.89       |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.00122    |\n",
      "|    loss                 | 0.00325    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | 0.0127     |\n",
      "|    std                  | 0.1        |\n",
      "|    value_loss           | 4.2e-08    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1935360, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0452     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076623134 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.9         |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00771     |\n",
      "|    std                  | 0.0998      |\n",
      "|    value_loss           | 3.27e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0789  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 2760     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0468    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1955520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03295401 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.91       |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.00121    |\n",
      "|    loss                 | 0.00263    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | 0.0312     |\n",
      "|    std                  | 0.0992     |\n",
      "|    value_loss           | 5.13e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0662     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019619023 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.92        |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | 0.00332     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.00404     |\n",
      "|    std                  | 0.0991      |\n",
      "|    value_loss           | 1.57e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0787  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 2816     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0622     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041950613 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.93        |\n",
      "|    explained_variance   | 0.0756      |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | -0.00172    |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    std                  | 0.0987      |\n",
      "|    value_loss           | 1.88e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412353 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.94        |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | 0.000137    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00224     |\n",
      "|    std                  | 0.098       |\n",
      "|    value_loss           | 3.88e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0728  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 2872     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0473    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2036160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03221255 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.95       |\n",
      "|    explained_variance   | 0.284      |\n",
      "|    learning_rate        | 0.0012     |\n",
      "|    loss                 | 0.00957    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | 0.0116     |\n",
      "|    std                  | 0.0972     |\n",
      "|    value_loss           | 1.36e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0797     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526551 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.97        |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.000169    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    std                  | 0.0962      |\n",
      "|    value_loss           | 1.25e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0711  |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 2928     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0514      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2076480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053048553 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2            |\n",
      "|    explained_variance   | 0.0215       |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00378     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | 0.000362     |\n",
      "|    std                  | 0.095        |\n",
      "|    value_loss           | 1.2e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0575     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011224805 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.02        |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.002       |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.00849     |\n",
      "|    std                  | 0.0939      |\n",
      "|    value_loss           | 3.7e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0673  |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 2984     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0474     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028594043 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.04        |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00184     |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | 0.00313     |\n",
      "|    std                  | 0.0932      |\n",
      "|    value_loss           | 4.59e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0444     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014690755 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.06        |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.00676     |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.00386     |\n",
      "|    std                  | 0.092       |\n",
      "|    value_loss           | 4.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0657  |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 3040     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0538     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017976444 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.09        |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.00588     |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | 0.00627     |\n",
      "|    std                  | 0.0905      |\n",
      "|    value_loss           | 1.07e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0564    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2177280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05235206 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.11       |\n",
      "|    explained_variance   | 0.279      |\n",
      "|    learning_rate        | 0.00118    |\n",
      "|    loss                 | 0.00658    |\n",
      "|    n_updates            | 1070       |\n",
      "|    policy_gradient_loss | 0.0076     |\n",
      "|    std                  | 0.0898     |\n",
      "|    value_loss           | 1.2e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.067   |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 3096     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0647     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2197440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008052055 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.12        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.00204    |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | 0.00708     |\n",
      "|    std                  | 0.0892      |\n",
      "|    value_loss           | 8.68e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0461     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008444678 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.14        |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.000377   |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.00312     |\n",
      "|    std                  | 0.0888      |\n",
      "|    value_loss           | 4.17e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0714  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 3152     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0672     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021976167 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.15        |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | 0.0075      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    std                  | 0.0882      |\n",
      "|    value_loss           | 7.56e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.066    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2257920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0233672 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 2.17      |\n",
      "|    explained_variance   | 0.116     |\n",
      "|    learning_rate        | 0.00116   |\n",
      "|    loss                 | 0.00453   |\n",
      "|    n_updates            | 1110      |\n",
      "|    policy_gradient_loss | 0.0121    |\n",
      "|    std                  | 0.0874    |\n",
      "|    value_loss           | 7.54e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0713  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 3208     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.07       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047811206 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.18        |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    std                  | 0.0863      |\n",
      "|    value_loss           | 7.49e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0465     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2298240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006330749 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.2         |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | 0.00168     |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.00783     |\n",
      "|    std                  | 0.0861      |\n",
      "|    value_loss           | 3.18e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0738  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 3264     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0727     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029164549 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.22        |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | 0.0167      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | 0.00328     |\n",
      "|    std                  | 0.0848      |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0498     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025584128 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.25        |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.0307      |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | 0.0181      |\n",
      "|    std                  | 0.0839      |\n",
      "|    value_loss           | 5.23e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0714  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 3320     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0651   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2358720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0838003 |\n",
      "|    clip_fraction        | 0.351     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 2.27      |\n",
      "|    explained_variance   | 0.269     |\n",
      "|    learning_rate        | 0.00115   |\n",
      "|    loss                 | 0.015     |\n",
      "|    n_updates            | 1160      |\n",
      "|    policy_gradient_loss | 0.0191    |\n",
      "|    std                  | 0.0833    |\n",
      "|    value_loss           | 7.62e-08  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0569     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017453462 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.28        |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.00223    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.0827      |\n",
      "|    value_loss           | 5.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0691  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 3376     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0653     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022689555 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.3         |\n",
      "|    explained_variance   | 0.082       |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.00225     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | 0.00133     |\n",
      "|    std                  | 0.082       |\n",
      "|    value_loss           | 2.36e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0564     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122651845 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.31        |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.022       |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.0211      |\n",
      "|    std                  | 0.0819      |\n",
      "|    value_loss           | 1.68e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0662  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 3431     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0743    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2439360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06631237 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.31       |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.00114    |\n",
      "|    loss                 | 0.0271     |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    std                  | 0.0817     |\n",
      "|    value_loss           | 1.48e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0507     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014113029 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.33        |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.00486     |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.00491     |\n",
      "|    std                  | 0.0808      |\n",
      "|    value_loss           | 4.7e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0663  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 3487     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0687     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016353784 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.35        |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | -0.000607   |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | 0.00711     |\n",
      "|    std                  | 0.08        |\n",
      "|    value_loss           | 1.67e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.052      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011330785 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.36        |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.00111     |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 0.018       |\n",
      "|    std                  | 0.0802      |\n",
      "|    value_loss           | 2.63e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0693  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 3543     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0643     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035403773 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.36        |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    std                  | 0.0795      |\n",
      "|    value_loss           | 4.47e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0692     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050834194 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.38        |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.0396      |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.02        |\n",
      "|    std                  | 0.0789      |\n",
      "|    value_loss           | 7.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0691  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 3599     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0496     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022005238 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.4         |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | -0.00197    |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | 0.00572     |\n",
      "|    std                  | 0.078       |\n",
      "|    value_loss           | 8.65e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0663     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027222944 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.42        |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.00774     |\n",
      "|    std                  | 0.0772      |\n",
      "|    value_loss           | 5.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0657  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 3654     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0523     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030892655 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.43        |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | 0.0211      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | 0.00885     |\n",
      "|    std                  | 0.0765      |\n",
      "|    value_loss           | 1.67e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0542      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2620800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079834545 |\n",
      "|    clip_fraction        | 0.305        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.45         |\n",
      "|    explained_variance   | 0.241        |\n",
      "|    learning_rate        | 0.00111      |\n",
      "|    loss                 | -0.00256     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | 0.016        |\n",
      "|    std                  | 0.076        |\n",
      "|    value_loss           | 8.39e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0635  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 3710     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.05 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0489     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043983933 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.47        |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    std                  | 0.0752      |\n",
      "|    value_loss           | 5.02e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0457     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064867355 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.48        |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | 0.000994    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | 0.0203      |\n",
      "|    std                  | 0.0748      |\n",
      "|    value_loss           | 5.8e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0603  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 3765     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0512     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2681280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046159826 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.5         |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | 0.0237      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | 0.00783     |\n",
      "|    std                  | 0.0744      |\n",
      "|    value_loss           | 5.44e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0549     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091396436 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.5         |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.0253      |\n",
      "|    std                  | 0.0741      |\n",
      "|    value_loss           | 6.63e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0614  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 3821     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0496      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2721600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102599785 |\n",
      "|    clip_fraction        | 0.279        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.52         |\n",
      "|    explained_variance   | 0.00561      |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | 0.000611     |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | 0.0161       |\n",
      "|    std                  | 0.0734       |\n",
      "|    value_loss           | 5.39e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0579     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034106325 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.54        |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.00569     |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | 0.00967     |\n",
      "|    std                  | 0.0727      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0626  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 3877     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0704     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2761920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017360395 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.55        |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | -0.00252    |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    std                  | 0.0723      |\n",
      "|    value_loss           | 1.04e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0541    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2782080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08634714 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.57       |\n",
      "|    explained_variance   | 0.383      |\n",
      "|    learning_rate        | 0.00109    |\n",
      "|    loss                 | 0.0341     |\n",
      "|    n_updates            | 1370       |\n",
      "|    policy_gradient_loss | 0.011      |\n",
      "|    std                  | 0.0714     |\n",
      "|    value_loss           | 5.62e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0668  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 3933     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0533    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2802240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23557815 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.58       |\n",
      "|    explained_variance   | 0.176      |\n",
      "|    learning_rate        | 0.00108    |\n",
      "|    loss                 | 0.0455     |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | 0.0289     |\n",
      "|    std                  | 0.0714     |\n",
      "|    value_loss           | 3.94e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0528    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2822400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08833586 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.58       |\n",
      "|    explained_variance   | 0.605      |\n",
      "|    learning_rate        | 0.00108    |\n",
      "|    loss                 | 0.0364     |\n",
      "|    n_updates            | 1390       |\n",
      "|    policy_gradient_loss | 0.0388     |\n",
      "|    std                  | 0.0717     |\n",
      "|    value_loss           | 1.47e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0727  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 3989     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0479    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2842560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15472263 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.58       |\n",
      "|    explained_variance   | 0.396      |\n",
      "|    learning_rate        | 0.00108    |\n",
      "|    loss                 | 0.0671     |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | 0.0194     |\n",
      "|    std                  | 0.0714     |\n",
      "|    value_loss           | 1.3e-07    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0612    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2862720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09206496 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.59       |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | 0.0346     |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | 0.0148     |\n",
      "|    std                  | 0.0709     |\n",
      "|    value_loss           | 2.36e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0718  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 4045     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0592    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2882880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06341091 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.6        |\n",
      "|    explained_variance   | 0.266      |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | 0.039      |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | 0.0273     |\n",
      "|    std                  | 0.0708     |\n",
      "|    value_loss           | 5.8e-08    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0637    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2903040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06576941 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.6        |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | 0.0613     |\n",
      "|    n_updates            | 1430       |\n",
      "|    policy_gradient_loss | 0.0214     |\n",
      "|    std                  | 0.0706     |\n",
      "|    value_loss           | 6.01e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0686  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 4101     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0481     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061880156 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.61        |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | 0.00538     |\n",
      "|    std                  | 0.0706      |\n",
      "|    value_loss           | 2.36e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0555     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2943360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036978267 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.62        |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    std                  | 0.0703      |\n",
      "|    value_loss           | 7.37e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.069   |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 4157     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0631    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2963520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03274118 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.64       |\n",
      "|    explained_variance   | 0.348      |\n",
      "|    learning_rate        | 0.00106    |\n",
      "|    loss                 | 0.0143     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | 0.00751    |\n",
      "|    std                  | 0.0692     |\n",
      "|    value_loss           | 3.44e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0702    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2983680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01870821 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.66       |\n",
      "|    explained_variance   | 0.148      |\n",
      "|    learning_rate        | 0.00106    |\n",
      "|    loss                 | 0.000943   |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | 0.00897    |\n",
      "|    std                  | 0.0687     |\n",
      "|    value_loss           | 8.65e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0694  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 4213     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0923     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3003840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031122737 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.67        |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.00587     |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.0683      |\n",
      "|    value_loss           | 1.22e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0583    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3024000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01061835 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.67       |\n",
      "|    explained_variance   | 0.0893     |\n",
      "|    learning_rate        | 0.00105    |\n",
      "|    loss                 | -0.000783  |\n",
      "|    n_updates            | 1490       |\n",
      "|    policy_gradient_loss | 0.0135     |\n",
      "|    std                  | 0.0683     |\n",
      "|    value_loss           | 6.6e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0689  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 4269     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3044160, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0651    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3044160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02974872 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.68       |\n",
      "|    explained_variance   | 0.263      |\n",
      "|    learning_rate        | 0.00105    |\n",
      "|    loss                 | 0.000101   |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | 0.0121     |\n",
      "|    std                  | 0.0679     |\n",
      "|    value_loss           | 6.41e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3064320, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0463     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3064320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027405746 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.69        |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | 0.00616     |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.00729     |\n",
      "|    std                  | 0.0674      |\n",
      "|    value_loss           | 8.21e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0647  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 4325     |\n",
      "|    total_timesteps | 3064320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3084480, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0562    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3084480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08586162 |\n",
      "|    clip_fraction        | 0.46       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.7        |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.00104    |\n",
      "|    loss                 | 0.0276     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | 0.027      |\n",
      "|    std                  | 0.0669     |\n",
      "|    value_loss           | 2.28e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3104640, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0534    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3104640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09464757 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.71       |\n",
      "|    explained_variance   | 0.0443     |\n",
      "|    learning_rate        | 0.00104    |\n",
      "|    loss                 | 0.0518     |\n",
      "|    n_updates            | 1530       |\n",
      "|    policy_gradient_loss | 0.0334     |\n",
      "|    std                  | 0.0667     |\n",
      "|    value_loss           | 5.47e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0601  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 4380     |\n",
      "|    total_timesteps | 3104640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3124800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0492    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3124800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06683155 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.72       |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.00103    |\n",
      "|    loss                 | 0.0418     |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | 0.0336     |\n",
      "|    std                  | 0.0664     |\n",
      "|    value_loss           | 4.95e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3144960, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0537    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3144960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07514489 |\n",
      "|    clip_fraction        | 0.455      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.72       |\n",
      "|    explained_variance   | 0.3        |\n",
      "|    learning_rate        | 0.00103    |\n",
      "|    loss                 | 0.0447     |\n",
      "|    n_updates            | 1550       |\n",
      "|    policy_gradient_loss | 0.0292     |\n",
      "|    std                  | 0.0664     |\n",
      "|    value_loss           | 6.94e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0606  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 4436     |\n",
      "|    total_timesteps | 3144960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3165120, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0416     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3165120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030781066 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.73        |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.00815     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | 0.00986     |\n",
      "|    std                  | 0.066       |\n",
      "|    value_loss           | 1.02e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3185280, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.052    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3185280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0705933 |\n",
      "|    clip_fraction        | 0.242     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 2.75      |\n",
      "|    explained_variance   | 0.444     |\n",
      "|    learning_rate        | 0.00103   |\n",
      "|    loss                 | 0.0387    |\n",
      "|    n_updates            | 1570      |\n",
      "|    policy_gradient_loss | 0.00803   |\n",
      "|    std                  | 0.0653    |\n",
      "|    value_loss           | 3.31e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0628  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 4492     |\n",
      "|    total_timesteps | 3185280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3205440, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0625     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3205440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008725186 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.77        |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | 0.00506     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | 0.0439      |\n",
      "|    std                  | 0.0645      |\n",
      "|    value_loss           | 9.72e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3225600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0548   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3225600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0232514 |\n",
      "|    clip_fraction        | 0.273     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 2.79      |\n",
      "|    explained_variance   | 0.272     |\n",
      "|    learning_rate        | 0.00102   |\n",
      "|    loss                 | 0.00718   |\n",
      "|    n_updates            | 1590      |\n",
      "|    policy_gradient_loss | 0.0109    |\n",
      "|    std                  | 0.0637    |\n",
      "|    value_loss           | 1.62e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0669  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 4548     |\n",
      "|    total_timesteps | 3225600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3245760, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0756     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3245760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035944473 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.81        |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | 0.0124      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | 0.00363     |\n",
      "|    std                  | 0.0638      |\n",
      "|    value_loss           | 2.41e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3265920, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0423     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3265920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066634566 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.8         |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    std                  | 0.0637      |\n",
      "|    value_loss           | 1.34e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0683  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 4603     |\n",
      "|    total_timesteps | 3265920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3286080, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0387     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3286080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014321498 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.81        |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.00248     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    std                  | 0.0634      |\n",
      "|    value_loss           | 4.99e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3306240, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0555     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3306240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010716207 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.83        |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | -0.000565   |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.0629      |\n",
      "|    value_loss           | 5.27e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0664  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 4659     |\n",
      "|    total_timesteps | 3306240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3326400, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0623     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3326400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008364283 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.84        |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00259     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | 0.00921     |\n",
      "|    std                  | 0.0625      |\n",
      "|    value_loss           | 1.29e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3346560, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0479     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3346560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018964577 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.87        |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00771     |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    std                  | 0.0617      |\n",
      "|    value_loss           | 2.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0666  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 4715     |\n",
      "|    total_timesteps | 3346560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3366720, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0447    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3366720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16321117 |\n",
      "|    clip_fraction        | 0.455      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.89       |\n",
      "|    explained_variance   | 0.0576     |\n",
      "|    learning_rate        | 0.000998   |\n",
      "|    loss                 | 0.0366     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | 0.0285     |\n",
      "|    std                  | 0.0613     |\n",
      "|    value_loss           | 9.38e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3386880, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0561     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3386880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038394187 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.9         |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.000995    |\n",
      "|    loss                 | 0.00785     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.00829     |\n",
      "|    std                  | 0.0608      |\n",
      "|    value_loss           | 1.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0628  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 4771     |\n",
      "|    total_timesteps | 3386880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3407040, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0611    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3407040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01088013 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.92       |\n",
      "|    explained_variance   | 0.0315     |\n",
      "|    learning_rate        | 0.000992   |\n",
      "|    loss                 | 0.0028     |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | 0.00387    |\n",
      "|    std                  | 0.06       |\n",
      "|    value_loss           | 5.41e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3427200, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0404     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3427200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082071714 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.94        |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.000989    |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.017       |\n",
      "|    std                  | 0.0599      |\n",
      "|    value_loss           | 3.72e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0638  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 4828     |\n",
      "|    total_timesteps | 3427200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3447360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0494     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3447360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017540138 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.94        |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.000986    |\n",
      "|    loss                 | 0.00482     |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.0595      |\n",
      "|    value_loss           | 4.73e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3467520, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0552     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3467520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106804736 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.96        |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.000983    |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.0204      |\n",
      "|    std                  | 0.0591      |\n",
      "|    value_loss           | 6.81e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0614  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 4884     |\n",
      "|    total_timesteps | 3467520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3487680, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.077     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3487680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06958457 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.96       |\n",
      "|    explained_variance   | 0.344      |\n",
      "|    learning_rate        | 0.00098    |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | 0.0209     |\n",
      "|    std                  | 0.0593     |\n",
      "|    value_loss           | 1.82e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3507840, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0583    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3507840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08620474 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.96       |\n",
      "|    explained_variance   | 0.385      |\n",
      "|    learning_rate        | 0.000977   |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 1730       |\n",
      "|    policy_gradient_loss | 0.018      |\n",
      "|    std                  | 0.0591     |\n",
      "|    value_loss           | 1.71e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0622  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 4939     |\n",
      "|    total_timesteps | 3507840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3528000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0536     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3528000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050912667 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.97        |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.000974    |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | 0.0268      |\n",
      "|    std                  | 0.059       |\n",
      "|    value_loss           | 6.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3548160, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0731     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3548160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022608552 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.97        |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.000971    |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.0588      |\n",
      "|    value_loss           | 1.57e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0602  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 4995     |\n",
      "|    total_timesteps | 3548160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3568320, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0417     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3568320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036259934 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.98        |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.000968    |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | 0.0235      |\n",
      "|    std                  | 0.0587      |\n",
      "|    value_loss           | 4.59e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3588480, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3588480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051217064 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.98        |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.000965    |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | 0.02        |\n",
      "|    std                  | 0.0587      |\n",
      "|    value_loss           | 1.01e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0591  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 5051     |\n",
      "|    total_timesteps | 3588480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3608640, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3608640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050361445 |\n",
      "|    clip_fraction        | 0.245        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.98         |\n",
      "|    explained_variance   | 0.0688       |\n",
      "|    learning_rate        | 0.000962     |\n",
      "|    loss                 | -0.00171     |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | 0.00993      |\n",
      "|    std                  | 0.0589       |\n",
      "|    value_loss           | 5.52e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3628800, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0435     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3628800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023407048 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.99        |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.000959    |\n",
      "|    loss                 | 0.00672     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.0587      |\n",
      "|    value_loss           | 2.88e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0597  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 5107     |\n",
      "|    total_timesteps | 3628800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3648960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0538     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3648960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012883648 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.99        |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.000956    |\n",
      "|    loss                 | 0.00109     |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    std                  | 0.0586      |\n",
      "|    value_loss           | 1.3e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3669120, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0598    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3669120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17281757 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3          |\n",
      "|    explained_variance   | 0.262      |\n",
      "|    learning_rate        | 0.000953   |\n",
      "|    loss                 | 0.0477     |\n",
      "|    n_updates            | 1810       |\n",
      "|    policy_gradient_loss | 0.0177     |\n",
      "|    std                  | 0.0587     |\n",
      "|    value_loss           | 1.03e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0628  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 5163     |\n",
      "|    total_timesteps | 3669120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3689280, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0536    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3689280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11468479 |\n",
      "|    clip_fraction        | 0.538      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.99       |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 0.00095    |\n",
      "|    loss                 | 0.0315     |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | 0.0484     |\n",
      "|    std                  | 0.059      |\n",
      "|    value_loss           | 1.21e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3709440, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0563     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3709440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025734153 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.99        |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.000947    |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | 0.0248      |\n",
      "|    std                  | 0.0586      |\n",
      "|    value_loss           | 8.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0618  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 5219     |\n",
      "|    total_timesteps | 3709440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3729600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0489     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3729600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051102057 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.01        |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.000944    |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | 0.00772     |\n",
      "|    std                  | 0.0577      |\n",
      "|    value_loss           | 4.87e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3749760, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.074     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3749760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09156789 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.04       |\n",
      "|    explained_variance   | 0.255      |\n",
      "|    learning_rate        | 0.000941   |\n",
      "|    loss                 | 0.0215     |\n",
      "|    n_updates            | 1850       |\n",
      "|    policy_gradient_loss | 0.01       |\n",
      "|    std                  | 0.0573     |\n",
      "|    value_loss           | 7.36e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0606  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 5275     |\n",
      "|    total_timesteps | 3749760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3769920, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0828     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3769920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037448213 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.04        |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.000938    |\n",
      "|    loss                 | 0.00736     |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | 0.0222      |\n",
      "|    std                  | 0.057       |\n",
      "|    value_loss           | 4.49e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3790080, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0789    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3790080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09053366 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.05       |\n",
      "|    explained_variance   | 0.171      |\n",
      "|    learning_rate        | 0.000935   |\n",
      "|    loss                 | 0.034      |\n",
      "|    n_updates            | 1870       |\n",
      "|    policy_gradient_loss | 0.0185     |\n",
      "|    std                  | 0.0567     |\n",
      "|    value_loss           | 4.51e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0583  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 5331     |\n",
      "|    total_timesteps | 3790080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0606    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3810240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02946025 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.06       |\n",
      "|    explained_variance   | 0.0866     |\n",
      "|    learning_rate        | 0.000931   |\n",
      "|    loss                 | 0.0372     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | 0.0133     |\n",
      "|    std                  | 0.0565     |\n",
      "|    value_loss           | 3.11e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3830400, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0725     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3830400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015103303 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.08        |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.000928    |\n",
      "|    loss                 | -0.00108    |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.0188      |\n",
      "|    std                  | 0.0558      |\n",
      "|    value_loss           | 3.93e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0576  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 5387     |\n",
      "|    total_timesteps | 3830400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850560, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0481     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3850560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021401417 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.09        |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000925    |\n",
      "|    loss                 | 0.00417     |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | 0.0172      |\n",
      "|    std                  | 0.0557      |\n",
      "|    value_loss           | 2.08e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3870720, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0592     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3870720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015396559 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.11        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000922    |\n",
      "|    loss                 | 0.000609    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | 0.00212     |\n",
      "|    std                  | 0.0546      |\n",
      "|    value_loss           | 3.26e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 5443     |\n",
      "|    total_timesteps | 3870720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890880, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0626    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3890880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01333213 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.14       |\n",
      "|    explained_variance   | 0.246      |\n",
      "|    learning_rate        | 0.000919   |\n",
      "|    loss                 | -0.00405   |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    std                  | 0.054      |\n",
      "|    value_loss           | 3.51e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3911040, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0671     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3911040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021228556 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.16        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.000916    |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | 0.0173      |\n",
      "|    std                  | 0.0535      |\n",
      "|    value_loss           | 5.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0599  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 5498     |\n",
      "|    total_timesteps | 3911040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3931200, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0702     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3931200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029009912 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.17        |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.000913    |\n",
      "|    loss                 | 0.00267     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | 0.00807     |\n",
      "|    std                  | 0.0532      |\n",
      "|    value_loss           | 1.27e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3951360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0574     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3951360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103264436 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.18        |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00091     |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.0258      |\n",
      "|    std                  | 0.053       |\n",
      "|    value_loss           | 6.87e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0613  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 5554     |\n",
      "|    total_timesteps | 3951360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3971520, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0683     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3971520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030049631 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.19        |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.000907    |\n",
      "|    loss                 | 0.00211     |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | 0.026       |\n",
      "|    std                  | 0.0531      |\n",
      "|    value_loss           | 7.22e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3991680, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0553    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3991680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02997398 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.19       |\n",
      "|    explained_variance   | 0.238      |\n",
      "|    learning_rate        | 0.000904   |\n",
      "|    loss                 | 0.0165     |\n",
      "|    n_updates            | 1970       |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    std                  | 0.0528     |\n",
      "|    value_loss           | 3.75e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0585  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 5610     |\n",
      "|    total_timesteps | 3991680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4011840, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0648    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4011840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14173087 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.2        |\n",
      "|    explained_variance   | 0.39       |\n",
      "|    learning_rate        | 0.000901   |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | 0.0206     |\n",
      "|    std                  | 0.0527     |\n",
      "|    value_loss           | 4.95e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4032000, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0555     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4032000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011593233 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.21        |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.000898    |\n",
      "|    loss                 | -0.00152    |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | 0.0239      |\n",
      "|    std                  | 0.0526      |\n",
      "|    value_loss           | 4.15e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0593  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 5667     |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4052160, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.067     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4052160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03909493 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.22       |\n",
      "|    explained_variance   | 0.57       |\n",
      "|    learning_rate        | 0.000895   |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | 0.0143     |\n",
      "|    std                  | 0.052      |\n",
      "|    value_loss           | 7.88e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4072320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0477     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4072320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011545471 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.24        |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.000892    |\n",
      "|    loss                 | 0.000436    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | 0.0175      |\n",
      "|    std                  | 0.0516      |\n",
      "|    value_loss           | 3.67e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0565  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 5723     |\n",
      "|    total_timesteps | 4072320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4092480, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0564     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4092480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027111128 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.26        |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.000889    |\n",
      "|    loss                 | 0.00857     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | 0.0187      |\n",
      "|    std                  | 0.0511      |\n",
      "|    value_loss           | 6.02e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4112640, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0436     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4112640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026126165 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.27        |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.000886    |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.00469     |\n",
      "|    std                  | 0.0507      |\n",
      "|    value_loss           | 1.02e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0569  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 5779     |\n",
      "|    total_timesteps | 4112640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4132800, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0628     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4132800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012174416 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.28        |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.000883    |\n",
      "|    loss                 | -0.00257    |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | 0.00604     |\n",
      "|    std                  | 0.0503      |\n",
      "|    value_loss           | 7.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4152960, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0637     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4152960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015407704 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.3         |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.00088     |\n",
      "|    loss                 | 0.000375    |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.05        |\n",
      "|    value_loss           | 5.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0551  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 5835     |\n",
      "|    total_timesteps | 4152960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4173120, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0462     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4173120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015797742 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.31        |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.000877    |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.0496      |\n",
      "|    value_loss           | 4.04e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4193280, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0493   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4193280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0935186 |\n",
      "|    clip_fraction        | 0.421     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.32      |\n",
      "|    explained_variance   | 0.359     |\n",
      "|    learning_rate        | 0.000874  |\n",
      "|    loss                 | 0.056     |\n",
      "|    n_updates            | 2070      |\n",
      "|    policy_gradient_loss | 0.0278    |\n",
      "|    std                  | 0.0495    |\n",
      "|    value_loss           | 2.03e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0596  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 5891     |\n",
      "|    total_timesteps | 4193280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4213440, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.06       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4213440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010029022 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.32        |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.000871    |\n",
      "|    loss                 | 0.00251     |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | 0.00249     |\n",
      "|    std                  | 0.0495      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4233600, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0577     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4233600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010171376 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.33        |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.000868    |\n",
      "|    loss                 | 0.00114     |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.00209     |\n",
      "|    std                  | 0.0493      |\n",
      "|    value_loss           | 8.58e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0613  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 5947     |\n",
      "|    total_timesteps | 4233600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4253760, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0507     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4253760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013663695 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.34        |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.000865    |\n",
      "|    loss                 | 0.00108     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | 0.00291     |\n",
      "|    std                  | 0.0491      |\n",
      "|    value_loss           | 1.34e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4273920, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0428    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4273920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05792998 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.35       |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.000862   |\n",
      "|    loss                 | 0.036      |\n",
      "|    n_updates            | 2110       |\n",
      "|    policy_gradient_loss | 0.00603    |\n",
      "|    std                  | 0.0484     |\n",
      "|    value_loss           | 7.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.062   |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 6003     |\n",
      "|    total_timesteps | 4273920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4294080, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0712     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4294080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038724333 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.37        |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.000859    |\n",
      "|    loss                 | 0.00908     |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    std                  | 0.0482      |\n",
      "|    value_loss           | 4.09e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4314240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0632    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4314240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11300019 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.37       |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.000856   |\n",
      "|    loss                 | 0.0606     |\n",
      "|    n_updates            | 2130       |\n",
      "|    policy_gradient_loss | 0.0233     |\n",
      "|    std                  | 0.048      |\n",
      "|    value_loss           | 7.36e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0618  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 6058     |\n",
      "|    total_timesteps | 4314240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4334400, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0681    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4334400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07002455 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.38       |\n",
      "|    explained_variance   | 0.205      |\n",
      "|    learning_rate        | 0.000853   |\n",
      "|    loss                 | 0.0298     |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | 0.0323     |\n",
      "|    std                  | 0.048      |\n",
      "|    value_loss           | 7.61e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4354560, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0614    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4354560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06301925 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.38       |\n",
      "|    explained_variance   | 0.363      |\n",
      "|    learning_rate        | 0.00085    |\n",
      "|    loss                 | 0.0359     |\n",
      "|    n_updates            | 2150       |\n",
      "|    policy_gradient_loss | 0.0296     |\n",
      "|    std                  | 0.0481     |\n",
      "|    value_loss           | 7.07e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0599  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 6114     |\n",
      "|    total_timesteps | 4354560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4374720, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0635     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4374720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056594066 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.38        |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.000847    |\n",
      "|    loss                 | 0.0409      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | 0.0414      |\n",
      "|    std                  | 0.0481      |\n",
      "|    value_loss           | 2.2e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4394880, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0637    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4394880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06348378 |\n",
      "|    clip_fraction        | 0.46       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.38       |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.000844   |\n",
      "|    loss                 | 0.0527     |\n",
      "|    n_updates            | 2170       |\n",
      "|    policy_gradient_loss | 0.0388     |\n",
      "|    std                  | 0.048      |\n",
      "|    value_loss           | 2.69e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0635  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 6170     |\n",
      "|    total_timesteps | 4394880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4415040, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0626    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4415040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06753629 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.38       |\n",
      "|    explained_variance   | 0.532      |\n",
      "|    learning_rate        | 0.000841   |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | 0.0172     |\n",
      "|    std                  | 0.0481     |\n",
      "|    value_loss           | 2.59e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4435200, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0743    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4435200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09178386 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.38       |\n",
      "|    explained_variance   | 0.541      |\n",
      "|    learning_rate        | 0.000838   |\n",
      "|    loss                 | 0.0364     |\n",
      "|    n_updates            | 2190       |\n",
      "|    policy_gradient_loss | 0.0256     |\n",
      "|    std                  | 0.0481     |\n",
      "|    value_loss           | 1.09e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0667  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 6225     |\n",
      "|    total_timesteps | 4435200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4455360, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0577    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4455360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07918673 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.38       |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.000835   |\n",
      "|    loss                 | 0.039      |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | 0.0249     |\n",
      "|    std                  | 0.048      |\n",
      "|    value_loss           | 1.11e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4475520, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0592     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4475520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023140006 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.39        |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.000832    |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | 0.0233      |\n",
      "|    std                  | 0.0481      |\n",
      "|    value_loss           | 5.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0693  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 6281     |\n",
      "|    total_timesteps | 4475520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4495680, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0662    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4495680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06737245 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.39       |\n",
      "|    explained_variance   | 0.527      |\n",
      "|    learning_rate        | 0.000829   |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | 0.0238     |\n",
      "|    std                  | 0.0481     |\n",
      "|    value_loss           | 8.66e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4515840, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.062     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4515840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06361926 |\n",
      "|    clip_fraction        | 0.393      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.39       |\n",
      "|    explained_variance   | 0.29       |\n",
      "|    learning_rate        | 0.000826   |\n",
      "|    loss                 | 0.039      |\n",
      "|    n_updates            | 2230       |\n",
      "|    policy_gradient_loss | 0.0206     |\n",
      "|    std                  | 0.048      |\n",
      "|    value_loss           | 8.7e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0702  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 6337     |\n",
      "|    total_timesteps | 4515840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0634     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4536000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018682452 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.39        |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.000823    |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.0481      |\n",
      "|    value_loss           | 6.58e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4556160, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0517     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4556160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047544613 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.39        |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00082     |\n",
      "|    loss                 | 0.00739     |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    std                  | 0.0479      |\n",
      "|    value_loss           | 6.01e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0659  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 6393     |\n",
      "|    total_timesteps | 4556160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4576320, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0746     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4576320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045971993 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.4         |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000817    |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.0478      |\n",
      "|    value_loss           | 3.76e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4596480, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0528     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4596480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015631348 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.4         |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.000814    |\n",
      "|    loss                 | 0.000337    |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.00611     |\n",
      "|    std                  | 0.0479      |\n",
      "|    value_loss           | 3.82e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0651  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 6448     |\n",
      "|    total_timesteps | 4596480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4616640, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0691    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4616640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06875225 |\n",
      "|    clip_fraction        | 0.528      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.4        |\n",
      "|    explained_variance   | 0.579      |\n",
      "|    learning_rate        | 0.000811   |\n",
      "|    loss                 | 0.0112     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | 0.0398     |\n",
      "|    std                  | 0.0478     |\n",
      "|    value_loss           | 3.94e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4636800, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.041    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4636800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1306583 |\n",
      "|    clip_fraction        | 0.443     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.4       |\n",
      "|    explained_variance   | 0.336     |\n",
      "|    learning_rate        | 0.000808  |\n",
      "|    loss                 | 0.0217    |\n",
      "|    n_updates            | 2290      |\n",
      "|    policy_gradient_loss | 0.0372    |\n",
      "|    std                  | 0.0476    |\n",
      "|    value_loss           | 5.97e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0627  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 6504     |\n",
      "|    total_timesteps | 4636800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4656960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0546    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4656960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01730248 |\n",
      "|    clip_fraction        | 0.429      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.41       |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.000804   |\n",
      "|    loss                 | 0.0104     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | 0.0306     |\n",
      "|    std                  | 0.0475     |\n",
      "|    value_loss           | 6.76e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4677120, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.062     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4677120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06779996 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.41       |\n",
      "|    explained_variance   | 0.173      |\n",
      "|    learning_rate        | 0.000801   |\n",
      "|    loss                 | 0.0325     |\n",
      "|    n_updates            | 2310       |\n",
      "|    policy_gradient_loss | 0.0183     |\n",
      "|    std                  | 0.0473     |\n",
      "|    value_loss           | 1.82e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0629  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 6560     |\n",
      "|    total_timesteps | 4677120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4697280, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0545    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4697280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03941164 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.42       |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.000798   |\n",
      "|    loss                 | 0.0152     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | 0.0154     |\n",
      "|    std                  | 0.0472     |\n",
      "|    value_loss           | 6.66e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4717440, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0465     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4717440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022681758 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.43        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.000795    |\n",
      "|    loss                 | 0.00489     |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.00869     |\n",
      "|    std                  | 0.047       |\n",
      "|    value_loss           | 9.94e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0615  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 6615     |\n",
      "|    total_timesteps | 4717440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4737600, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0477     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4737600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037969436 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.43        |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.000792    |\n",
      "|    loss                 | 0.00975     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    std                  | 0.0468      |\n",
      "|    value_loss           | 1.52e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4757760, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0558    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4757760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03148253 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.44       |\n",
      "|    explained_variance   | 0.325      |\n",
      "|    learning_rate        | 0.000789   |\n",
      "|    loss                 | 0.00832    |\n",
      "|    n_updates            | 2350       |\n",
      "|    policy_gradient_loss | 0.0113     |\n",
      "|    std                  | 0.0468     |\n",
      "|    value_loss           | 1.21e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0609  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 6671     |\n",
      "|    total_timesteps | 4757760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4777920, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0513     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4777920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061604347 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.44        |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.000786    |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    std                  | 0.0466      |\n",
      "|    value_loss           | 9.24e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4798080, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0825     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4798080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020883571 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.45        |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.000783    |\n",
      "|    loss                 | -0.00162    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    std                  | 0.0462      |\n",
      "|    value_loss           | 4.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0573  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 6727     |\n",
      "|    total_timesteps | 4798080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4818240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0581    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4818240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19399068 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.46       |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.00078    |\n",
      "|    loss                 | 0.0859     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | 0.0319     |\n",
      "|    std                  | 0.0461     |\n",
      "|    value_loss           | 5.29e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4838400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0514    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4838400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08592293 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.48       |\n",
      "|    explained_variance   | 0.252      |\n",
      "|    learning_rate        | 0.000777   |\n",
      "|    loss                 | 0.0541     |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | 0.0256     |\n",
      "|    std                  | 0.0456     |\n",
      "|    value_loss           | 9.85e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0578  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 6783     |\n",
      "|    total_timesteps | 4838400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4858560, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0453    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4858560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08283202 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.49       |\n",
      "|    explained_variance   | 0.109      |\n",
      "|    learning_rate        | 0.000774   |\n",
      "|    loss                 | 0.0706     |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | 0.0367     |\n",
      "|    std                  | 0.0455     |\n",
      "|    value_loss           | 4.78e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4878720, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0415    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4878720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11341326 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.51       |\n",
      "|    explained_variance   | 0.344      |\n",
      "|    learning_rate        | 0.000771   |\n",
      "|    loss                 | 0.00815    |\n",
      "|    n_updates            | 2410       |\n",
      "|    policy_gradient_loss | 0.0119     |\n",
      "|    std                  | 0.0449     |\n",
      "|    value_loss           | 7.5e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0554  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 6839     |\n",
      "|    total_timesteps | 4878720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4898880, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0618     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4898880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025427876 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.53        |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.000768    |\n",
      "|    loss                 | 0.00526     |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | 0.0255      |\n",
      "|    std                  | 0.0448      |\n",
      "|    value_loss           | 3.03e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4919040, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0767    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4919040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03006954 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.54       |\n",
      "|    explained_variance   | 0.469      |\n",
      "|    learning_rate        | 0.000765   |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | 0.0112     |\n",
      "|    std                  | 0.0444     |\n",
      "|    value_loss           | 9.53e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0569  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 6894     |\n",
      "|    total_timesteps | 4919040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4939200, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0802     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4939200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038095444 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.56        |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.000762    |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | 0.0176      |\n",
      "|    std                  | 0.0442      |\n",
      "|    value_loss           | 5.78e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4959360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0689     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4959360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025237452 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.56        |\n",
      "|    explained_variance   | 0.0413      |\n",
      "|    learning_rate        | 0.000759    |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | 0.0148      |\n",
      "|    std                  | 0.044       |\n",
      "|    value_loss           | 6.7e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0551  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 6950     |\n",
      "|    total_timesteps | 4959360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4979520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0485     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4979520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039094675 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.57        |\n",
      "|    explained_variance   | 0.0827      |\n",
      "|    learning_rate        | 0.000756    |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    std                  | 0.0437      |\n",
      "|    value_loss           | 3.08e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4999680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0475    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4999680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06509751 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.58       |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.000753   |\n",
      "|    loss                 | 0.0226     |\n",
      "|    n_updates            | 2470       |\n",
      "|    policy_gradient_loss | 0.0246     |\n",
      "|    std                  | 0.0438     |\n",
      "|    value_loss           | 8.45e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0561  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 7006     |\n",
      "|    total_timesteps | 4999680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5019840, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.069      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5019840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007905055 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.58        |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.00075     |\n",
      "|    loss                 | 0.00109     |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | 0.00146     |\n",
      "|    std                  | 0.0438      |\n",
      "|    value_loss           | 1.38e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5040000, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0554    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5040000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12521371 |\n",
      "|    clip_fraction        | 0.427      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.59       |\n",
      "|    explained_variance   | 0.33       |\n",
      "|    learning_rate        | 0.000747   |\n",
      "|    loss                 | 0.0671     |\n",
      "|    n_updates            | 2490       |\n",
      "|    policy_gradient_loss | 0.0358     |\n",
      "|    std                  | 0.0436     |\n",
      "|    value_loss           | 9.15e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0602  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 7061     |\n",
      "|    total_timesteps | 5040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5060160, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0627     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5060160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070467144 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.58        |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.000744    |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | 0.0173      |\n",
      "|    std                  | 0.0441      |\n",
      "|    value_loss           | 3.1e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5080320, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0668     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5080320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020724893 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.59        |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.000741    |\n",
      "|    loss                 | 0.00188     |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | 0.00869     |\n",
      "|    std                  | 0.0437      |\n",
      "|    value_loss           | 1.59e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0622  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 7117     |\n",
      "|    total_timesteps | 5080320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5100480, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0632     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5100480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053602226 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.6         |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.000738    |\n",
      "|    loss                 | 0.000661    |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | 0.0303      |\n",
      "|    std                  | 0.0436      |\n",
      "|    value_loss           | 7.12e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5120640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0458     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5120640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051708095 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.6         |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.000735    |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.00658     |\n",
      "|    std                  | 0.0435      |\n",
      "|    value_loss           | 1.93e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0649  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 7173     |\n",
      "|    total_timesteps | 5120640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5140800, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0797    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5140800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01807932 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.61       |\n",
      "|    explained_variance   | 0.205      |\n",
      "|    learning_rate        | 0.000732   |\n",
      "|    loss                 | 0.0054     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | 0.00643    |\n",
      "|    std                  | 0.0434     |\n",
      "|    value_loss           | 2.19e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5160960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0498     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5160960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029202616 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.61        |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.000729    |\n",
      "|    loss                 | 0.00886     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.00851     |\n",
      "|    std                  | 0.0434      |\n",
      "|    value_loss           | 2.67e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0606  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 7229     |\n",
      "|    total_timesteps | 5160960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5181120, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0759     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5181120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031823102 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.61        |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.000726    |\n",
      "|    loss                 | 0.0019      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.0432      |\n",
      "|    value_loss           | 1.55e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5201280, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0709     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5201280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015816016 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.62        |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.000723    |\n",
      "|    loss                 | 0.00985     |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.043       |\n",
      "|    value_loss           | 6.26e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0619  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 7285     |\n",
      "|    total_timesteps | 5201280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5221440, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0552     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5221440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019422239 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.63        |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00072     |\n",
      "|    loss                 | 0.00382     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.0429      |\n",
      "|    value_loss           | 9.58e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5241600, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0563     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5241600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025547147 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.63        |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.000717    |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.0429      |\n",
      "|    value_loss           | 2.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0641  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 7341     |\n",
      "|    total_timesteps | 5241600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5261760, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0635    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5261760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02182797 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.63       |\n",
      "|    explained_variance   | 0.388      |\n",
      "|    learning_rate        | 0.000714   |\n",
      "|    loss                 | -0.00031   |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | 0.00719    |\n",
      "|    std                  | 0.043      |\n",
      "|    value_loss           | 1.85e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5281920, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0591     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5281920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019303072 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.64        |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.000711    |\n",
      "|    loss                 | 0.00419     |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.00622     |\n",
      "|    std                  | 0.043       |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0622  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 7397     |\n",
      "|    total_timesteps | 5281920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5302080, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0444     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5302080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025147976 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.64        |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.000708    |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | 0.0174      |\n",
      "|    std                  | 0.0426      |\n",
      "|    value_loss           | 3.94e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5322240, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0636    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5322240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03603617 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.66       |\n",
      "|    explained_variance   | 0.375      |\n",
      "|    learning_rate        | 0.000705   |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 2630       |\n",
      "|    policy_gradient_loss | 0.0176     |\n",
      "|    std                  | 0.0422     |\n",
      "|    value_loss           | 4.22e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0595  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 7453     |\n",
      "|    total_timesteps | 5322240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5342400, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0424     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5342400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047111016 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.67        |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.000702    |\n",
      "|    loss                 | 0.00164     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.0421      |\n",
      "|    value_loss           | 4.46e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5362560, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0443    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5362560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04868901 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.68       |\n",
      "|    explained_variance   | 0.129      |\n",
      "|    learning_rate        | 0.000699   |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | 0.0251     |\n",
      "|    std                  | 0.0421     |\n",
      "|    value_loss           | 3.24e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0564  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 7509     |\n",
      "|    total_timesteps | 5362560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5382720, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0803     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5382720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016411347 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.68        |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.000696    |\n",
      "|    loss                 | -0.000675   |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | 0.0307      |\n",
      "|    std                  | 0.042       |\n",
      "|    value_loss           | 4.68e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5402880, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0548    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5402880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03544546 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.69       |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 0.000693   |\n",
      "|    loss                 | 0.00151    |\n",
      "|    n_updates            | 2670       |\n",
      "|    policy_gradient_loss | 0.0105     |\n",
      "|    std                  | 0.0418     |\n",
      "|    value_loss           | 1.28e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0584  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 7565     |\n",
      "|    total_timesteps | 5402880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5423040, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0695     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5423040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078246415 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.7         |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00069     |\n",
      "|    loss                 | 0.0394      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | 0.0178      |\n",
      "|    std                  | 0.0416      |\n",
      "|    value_loss           | 2.27e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5443200, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0475    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5443200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02520411 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.71       |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.000687   |\n",
      "|    loss                 | 0.00314    |\n",
      "|    n_updates            | 2690       |\n",
      "|    policy_gradient_loss | 0.0073     |\n",
      "|    std                  | 0.0414     |\n",
      "|    value_loss           | 1.65e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0634  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 7622     |\n",
      "|    total_timesteps | 5443200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5463360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0456      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5463360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074966988 |\n",
      "|    clip_fraction        | 0.236        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.72         |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.000684     |\n",
      "|    loss                 | 0.00073      |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | 0.00669      |\n",
      "|    std                  | 0.0412       |\n",
      "|    value_loss           | 6.2e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5483520, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0595    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5483520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03649466 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.72       |\n",
      "|    explained_variance   | 0.315      |\n",
      "|    learning_rate        | 0.00068    |\n",
      "|    loss                 | 0.00125    |\n",
      "|    n_updates            | 2710       |\n",
      "|    policy_gradient_loss | 0.0114     |\n",
      "|    std                  | 0.041      |\n",
      "|    value_loss           | 6.84e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0642  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 7678     |\n",
      "|    total_timesteps | 5483520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5503680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0529    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5503680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03410907 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.73       |\n",
      "|    explained_variance   | 0.52       |\n",
      "|    learning_rate        | 0.000677   |\n",
      "|    loss                 | 0.00268    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | 0.00714    |\n",
      "|    std                  | 0.041      |\n",
      "|    value_loss           | 1.39e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5523840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0455    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5523840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08549059 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.74       |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.000674   |\n",
      "|    loss                 | 0.0717     |\n",
      "|    n_updates            | 2730       |\n",
      "|    policy_gradient_loss | 0.0217     |\n",
      "|    std                  | 0.0408     |\n",
      "|    value_loss           | 7.01e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0632  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 7734     |\n",
      "|    total_timesteps | 5523840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5544000, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0505     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5544000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016617896 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.75        |\n",
      "|    explained_variance   | 0.0375      |\n",
      "|    learning_rate        | 0.000671    |\n",
      "|    loss                 | -0.00157    |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    std                  | 0.0405      |\n",
      "|    value_loss           | 2.86e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5564160, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0546     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5564160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039217487 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.76        |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.000668    |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | 0.0264      |\n",
      "|    std                  | 0.0405      |\n",
      "|    value_loss           | 2.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0562  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 7790     |\n",
      "|    total_timesteps | 5564160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5584320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0594     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5584320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024561796 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.76        |\n",
      "|    explained_variance   | 0.0134      |\n",
      "|    learning_rate        | 0.000665    |\n",
      "|    loss                 | 0.0148      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | 0.00767     |\n",
      "|    std                  | 0.0401      |\n",
      "|    value_loss           | 7.62e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5604480, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0627     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5604480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111570165 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.78        |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.000662    |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 2770        |\n",
      "|    policy_gradient_loss | 0.0199      |\n",
      "|    std                  | 0.0401      |\n",
      "|    value_loss           | 2.03e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0558  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 7846     |\n",
      "|    total_timesteps | 5604480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5624640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0454     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5624640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011252128 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.78        |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.000659    |\n",
      "|    loss                 | 0.00751     |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | 0.0048      |\n",
      "|    std                  | 0.04        |\n",
      "|    value_loss           | 1.88e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5644800, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0782    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5644800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07012066 |\n",
      "|    clip_fraction        | 0.437      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.78       |\n",
      "|    explained_variance   | 0.0678     |\n",
      "|    learning_rate        | 0.000656   |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | 0.0362     |\n",
      "|    std                  | 0.0399     |\n",
      "|    value_loss           | 5.18e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0544  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 7902     |\n",
      "|    total_timesteps | 5644800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5664960, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0565      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5664960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077930335 |\n",
      "|    clip_fraction        | 0.233        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.78         |\n",
      "|    explained_variance   | 0.193        |\n",
      "|    learning_rate        | 0.000653     |\n",
      "|    loss                 | 8.36e-05     |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | 0.00863      |\n",
      "|    std                  | 0.0399       |\n",
      "|    value_loss           | 6.15e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5685120, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0545    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5685120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10824816 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.79       |\n",
      "|    explained_variance   | 0.314      |\n",
      "|    learning_rate        | 0.00065    |\n",
      "|    loss                 | 0.046      |\n",
      "|    n_updates            | 2810       |\n",
      "|    policy_gradient_loss | 0.0194     |\n",
      "|    std                  | 0.0398     |\n",
      "|    value_loss           | 1.36e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0566  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 7958     |\n",
      "|    total_timesteps | 5685120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5705280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0595     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5705280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008289451 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.78        |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.000647    |\n",
      "|    loss                 | -0.00417    |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | 9.41e-05    |\n",
      "|    std                  | 0.0399      |\n",
      "|    value_loss           | 8.53e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5725440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0481     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5725440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028329177 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.79        |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.000644    |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | 0.0226      |\n",
      "|    std                  | 0.0398      |\n",
      "|    value_loss           | 3.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0564  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 8014     |\n",
      "|    total_timesteps | 5725440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5745600, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0619     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5745600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042207785 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.79        |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.000641    |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | 0.00989     |\n",
      "|    std                  | 0.0399      |\n",
      "|    value_loss           | 1.55e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5765760, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.058      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5765760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010086752 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.78        |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.000638    |\n",
      "|    loss                 | -0.000113   |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | 0.0163      |\n",
      "|    std                  | 0.0401      |\n",
      "|    value_loss           | 1.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0591  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 8070     |\n",
      "|    total_timesteps | 5765760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5785920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0573    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5785920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08138332 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.78       |\n",
      "|    explained_variance   | 0.589      |\n",
      "|    learning_rate        | 0.000635   |\n",
      "|    loss                 | 0.00538    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | 0.0139     |\n",
      "|    std                  | 0.04       |\n",
      "|    value_loss           | 2.49e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5806080, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0389    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5806080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12109434 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.78       |\n",
      "|    explained_variance   | 0.523      |\n",
      "|    learning_rate        | 0.000632   |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 2870       |\n",
      "|    policy_gradient_loss | 0.0145     |\n",
      "|    std                  | 0.0401     |\n",
      "|    value_loss           | 1.08e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0571  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 8126     |\n",
      "|    total_timesteps | 5806080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5826240, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0507    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5826240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18007359 |\n",
      "|    clip_fraction        | 0.539      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.78       |\n",
      "|    explained_variance   | 0.115      |\n",
      "|    learning_rate        | 0.000629   |\n",
      "|    loss                 | 0.0729     |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | 0.0651     |\n",
      "|    std                  | 0.0401     |\n",
      "|    value_loss           | 1.08e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5846400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0511    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5846400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07672559 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.79       |\n",
      "|    explained_variance   | 0.307      |\n",
      "|    learning_rate        | 0.000626   |\n",
      "|    loss                 | 0.0574     |\n",
      "|    n_updates            | 2890       |\n",
      "|    policy_gradient_loss | 0.0233     |\n",
      "|    std                  | 0.0399     |\n",
      "|    value_loss           | 4.47e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0596  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 8182     |\n",
      "|    total_timesteps | 5846400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5866560, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0704     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5866560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023336638 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.8         |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.000623    |\n",
      "|    loss                 | 0.00961     |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | 0.00714     |\n",
      "|    std                  | 0.0399      |\n",
      "|    value_loss           | 1.08e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5886720, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0439   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5886720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1004726 |\n",
      "|    clip_fraction        | 0.438     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.8       |\n",
      "|    explained_variance   | 0.264     |\n",
      "|    learning_rate        | 0.00062   |\n",
      "|    loss                 | 0.0685    |\n",
      "|    n_updates            | 2910      |\n",
      "|    policy_gradient_loss | 0.0477    |\n",
      "|    std                  | 0.0399    |\n",
      "|    value_loss           | 4.45e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0583  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 8246     |\n",
      "|    total_timesteps | 5886720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5906880, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0664     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5906880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011008138 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.81        |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.000617    |\n",
      "|    loss                 | 0.00169     |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | 0.000652    |\n",
      "|    std                  | 0.0395      |\n",
      "|    value_loss           | 1.33e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5927040, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0483   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5927040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2081466 |\n",
      "|    clip_fraction        | 0.445     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.82      |\n",
      "|    explained_variance   | 0.305     |\n",
      "|    learning_rate        | 0.000614  |\n",
      "|    loss                 | 0.0489    |\n",
      "|    n_updates            | 2930      |\n",
      "|    policy_gradient_loss | 0.0333    |\n",
      "|    std                  | 0.0396    |\n",
      "|    value_loss           | 8.48e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 8309     |\n",
      "|    total_timesteps | 5927040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5947200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0576     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5947200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060257033 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.82        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000611    |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | 0.0287      |\n",
      "|    std                  | 0.0396      |\n",
      "|    value_loss           | 3.37e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5967360, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0557     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5967360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011990387 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.82        |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.000608    |\n",
      "|    loss                 | 0.0083      |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | 0.0092      |\n",
      "|    std                  | 0.0396      |\n",
      "|    value_loss           | 1.49e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0612  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 8370     |\n",
      "|    total_timesteps | 5967360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5987520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0511    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5987520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18434566 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.82       |\n",
      "|    explained_variance   | 0.532      |\n",
      "|    learning_rate        | 0.000605   |\n",
      "|    loss                 | 0.0534     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | 0.0215     |\n",
      "|    std                  | 0.0393     |\n",
      "|    value_loss           | 7.88e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6007680, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0468    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6007680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13022171 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.83       |\n",
      "|    explained_variance   | 0.389      |\n",
      "|    learning_rate        | 0.000602   |\n",
      "|    loss                 | 0.0446     |\n",
      "|    n_updates            | 2970       |\n",
      "|    policy_gradient_loss | 0.0481     |\n",
      "|    std                  | 0.0392     |\n",
      "|    value_loss           | 5.79e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 8426     |\n",
      "|    total_timesteps | 6007680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6027840, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0715     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6027840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007030569 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.85        |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.000599    |\n",
      "|    loss                 | 0.00467     |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    std                  | 0.0386      |\n",
      "|    value_loss           | 3.28e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6048000, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0674     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6048000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058050778 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.87        |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.000596    |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | 0.0309      |\n",
      "|    std                  | 0.0384      |\n",
      "|    value_loss           | 3.21e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0588  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 8482     |\n",
      "|    total_timesteps | 6048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6068160, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0425     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6068160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018537533 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.87        |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.000593    |\n",
      "|    loss                 | 0.00745     |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | 0.00458     |\n",
      "|    std                  | 0.0385      |\n",
      "|    value_loss           | 1.79e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6088320, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0586    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6088320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03617985 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.86       |\n",
      "|    explained_variance   | 0.426      |\n",
      "|    learning_rate        | 0.00059    |\n",
      "|    loss                 | 0.0177     |\n",
      "|    n_updates            | 3010       |\n",
      "|    policy_gradient_loss | 0.00464    |\n",
      "|    std                  | 0.0385     |\n",
      "|    value_loss           | 2.64e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0582  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 8538     |\n",
      "|    total_timesteps | 6088320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6108480, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0549     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6108480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026117342 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.87        |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.000587    |\n",
      "|    loss                 | 0.00444     |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    std                  | 0.0383      |\n",
      "|    value_loss           | 7.43e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6128640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0543     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6128640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024794005 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.88        |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.000584    |\n",
      "|    loss                 | 0.00682     |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | 0.00548     |\n",
      "|    std                  | 0.0381      |\n",
      "|    value_loss           | 7.48e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0581  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 8594     |\n",
      "|    total_timesteps | 6128640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6148800, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.068      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6148800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033473726 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.88        |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.000581    |\n",
      "|    loss                 | 0.00782     |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | 0.00757     |\n",
      "|    std                  | 0.0381      |\n",
      "|    value_loss           | 3.96e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6168960, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0658    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6168960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15521285 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.88       |\n",
      "|    explained_variance   | 0.287      |\n",
      "|    learning_rate        | 0.000578   |\n",
      "|    loss                 | 0.0255     |\n",
      "|    n_updates            | 3050       |\n",
      "|    policy_gradient_loss | 0.05       |\n",
      "|    std                  | 0.038      |\n",
      "|    value_loss           | 4.07e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0575  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 8649     |\n",
      "|    total_timesteps | 6168960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6189120, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0581    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6189120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08000603 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.89       |\n",
      "|    explained_variance   | 0.53       |\n",
      "|    learning_rate        | 0.000575   |\n",
      "|    loss                 | 0.00452    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    std                  | 0.0378     |\n",
      "|    value_loss           | 8.49e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6209280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0581     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6209280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020715296 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.9         |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.000572    |\n",
      "|    loss                 | 0.00187     |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | 0.00455     |\n",
      "|    std                  | 0.0377      |\n",
      "|    value_loss           | 1.99e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0565  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 8706     |\n",
      "|    total_timesteps | 6209280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6229440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0542     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6229440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021037333 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.91        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.000569    |\n",
      "|    loss                 | 0.00535     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | 0.00689     |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 2.32e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6249600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0528     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6249600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020508878 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.91        |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.000566    |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | 0.00728     |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 1.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0586  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 8762     |\n",
      "|    total_timesteps | 6249600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6269760, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0447    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6269760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01848151 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.92       |\n",
      "|    explained_variance   | 0.0507     |\n",
      "|    learning_rate        | 0.000563   |\n",
      "|    loss                 | -0.000107  |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | 0.00759    |\n",
      "|    std                  | 0.0373     |\n",
      "|    value_loss           | 3.1e-08    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6289920, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.055      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6289920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016000409 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.93        |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.00056     |\n",
      "|    loss                 | 0.00459     |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | 0.00231     |\n",
      "|    std                  | 0.0371      |\n",
      "|    value_loss           | 8.91e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0596  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 8818     |\n",
      "|    total_timesteps | 6289920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6310080, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0686    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6310080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07098026 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.93       |\n",
      "|    explained_variance   | 0.32       |\n",
      "|    learning_rate        | 0.000557   |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | 0.0171     |\n",
      "|    std                  | 0.0371     |\n",
      "|    value_loss           | 1.19e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6330240, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0698     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6330240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011577763 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.93        |\n",
      "|    explained_variance   | 0.085       |\n",
      "|    learning_rate        | 0.000553    |\n",
      "|    loss                 | -5.89e-06   |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | 0.0295      |\n",
      "|    std                  | 0.0369      |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0605  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 8874     |\n",
      "|    total_timesteps | 6330240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6350400, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0546     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6350400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019017428 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.94        |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.00055     |\n",
      "|    loss                 | -7.1e-05    |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | 0.0185      |\n",
      "|    std                  | 0.0369      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6370560, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0411     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6370560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008112231 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.94        |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.000547    |\n",
      "|    loss                 | 0.00153     |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | 0.00564     |\n",
      "|    std                  | 0.0371      |\n",
      "|    value_loss           | 7.33e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0611  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 8930     |\n",
      "|    total_timesteps | 6370560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6390720, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0713     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6390720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030990696 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.93        |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.000544    |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | 0.00967     |\n",
      "|    std                  | 0.037       |\n",
      "|    value_loss           | 1.47e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6410880, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.059      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6410880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024096444 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.95        |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.000541    |\n",
      "|    loss                 | 0.00565     |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    std                  | 0.0368      |\n",
      "|    value_loss           | 4.43e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0606  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 8986     |\n",
      "|    total_timesteps | 6410880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6431040, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.075    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 6431040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0154642 |\n",
      "|    clip_fraction        | 0.229     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.96      |\n",
      "|    explained_variance   | 0.281     |\n",
      "|    learning_rate        | 0.000538  |\n",
      "|    loss                 | 0.00863   |\n",
      "|    n_updates            | 3180      |\n",
      "|    policy_gradient_loss | 0.00662   |\n",
      "|    std                  | 0.0366    |\n",
      "|    value_loss           | 6.54e-08  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=6451200, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0492    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6451200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00485944 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.96       |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.000535   |\n",
      "|    loss                 | -0.00101   |\n",
      "|    n_updates            | 3190       |\n",
      "|    policy_gradient_loss | 0.00717    |\n",
      "|    std                  | 0.0367     |\n",
      "|    value_loss           | 2.78e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0621  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 9042     |\n",
      "|    total_timesteps | 6451200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6471360, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0607     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6471360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016270027 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.95        |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.000532    |\n",
      "|    loss                 | 0.00216     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | 0.00266     |\n",
      "|    std                  | 0.0366      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6491520, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0527      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6491520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066908095 |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.97         |\n",
      "|    explained_variance   | 0.318        |\n",
      "|    learning_rate        | 0.000529     |\n",
      "|    loss                 | 0.00102      |\n",
      "|    n_updates            | 3210         |\n",
      "|    policy_gradient_loss | 0.00473      |\n",
      "|    std                  | 0.0363       |\n",
      "|    value_loss           | 1.06e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0636  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 9098     |\n",
      "|    total_timesteps | 6491520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6511680, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0585     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6511680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009962009 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.98        |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.000526    |\n",
      "|    loss                 | 0.000195    |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | 0.00417     |\n",
      "|    std                  | 0.0365      |\n",
      "|    value_loss           | 3.37e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6531840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0526     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6531840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011804337 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.000523    |\n",
      "|    loss                 | 0.00337     |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | 0.00101     |\n",
      "|    std                  | 0.0365      |\n",
      "|    value_loss           | 2.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0631  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 9154     |\n",
      "|    total_timesteps | 6531840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6552000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0551     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6552000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008235641 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00052     |\n",
      "|    loss                 | 0.003       |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | 0.00128     |\n",
      "|    std                  | 0.0363      |\n",
      "|    value_loss           | 6.28e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6572160, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0614    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6572160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11820796 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.98       |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.000517   |\n",
      "|    loss                 | 0.0688     |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | 0.0223     |\n",
      "|    std                  | 0.0362     |\n",
      "|    value_loss           | 8.17e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0618  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 9210     |\n",
      "|    total_timesteps | 6572160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6592320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0597     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6592320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023448264 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.98        |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.000514    |\n",
      "|    loss                 | 0.00822     |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | 0.00548     |\n",
      "|    std                  | 0.0362      |\n",
      "|    value_loss           | 4.06e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6612480, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0767     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6612480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037132263 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.000511    |\n",
      "|    loss                 | -0.00109    |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | 0.00237     |\n",
      "|    std                  | 0.0364      |\n",
      "|    value_loss           | 2.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0599  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 9266     |\n",
      "|    total_timesteps | 6612480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6632640, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0643     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6632640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029502228 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.000508    |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.0366      |\n",
      "|    value_loss           | 7.15e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6652800, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0626     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6652800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052509934 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.96        |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.000505    |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.0366      |\n",
      "|    value_loss           | 1.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0585  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 9322     |\n",
      "|    total_timesteps | 6652800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6672960, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0598     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6672960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050201237 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.96        |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.000502    |\n",
      "|    loss                 | 0.0206      |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | 0.00823     |\n",
      "|    std                  | 0.0365      |\n",
      "|    value_loss           | 6.64e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6693120, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0449     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6693120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098425075 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.000499    |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.0364      |\n",
      "|    value_loss           | 4.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.057   |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 9378     |\n",
      "|    total_timesteps | 6693120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6713280, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0574      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6713280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058349534 |\n",
      "|    clip_fraction        | 0.28         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.97         |\n",
      "|    explained_variance   | 0.306        |\n",
      "|    learning_rate        | 0.000496     |\n",
      "|    loss                 | -0.000698    |\n",
      "|    n_updates            | 3320         |\n",
      "|    policy_gradient_loss | 0.0163       |\n",
      "|    std                  | 0.0363       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6733440, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0686     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6733440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066367134 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.99        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.000493    |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    std                  | 0.036       |\n",
      "|    value_loss           | 7.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0558  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 9434     |\n",
      "|    total_timesteps | 6733440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6753600, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0424     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6753600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027045539 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.00049     |\n",
      "|    loss                 | -2.74e-05   |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | 0.00978     |\n",
      "|    std                  | 0.0359      |\n",
      "|    value_loss           | 7.73e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6773760, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0653     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6773760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026560457 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.000487    |\n",
      "|    loss                 | -0.00144    |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    std                  | 0.0358      |\n",
      "|    value_loss           | 1.85e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0598  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 9490     |\n",
      "|    total_timesteps | 6773760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6793920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0558     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6793920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015052993 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.000484    |\n",
      "|    loss                 | 0.00134     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    std                  | 0.036       |\n",
      "|    value_loss           | 9.61e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6814080, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0655     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6814080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046999425 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | 0.0307      |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | 0.0175      |\n",
      "|    std                  | 0.0358      |\n",
      "|    value_loss           | 4.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.059   |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 9546     |\n",
      "|    total_timesteps | 6814080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6834240, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6834240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038237303 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.01        |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.000478    |\n",
      "|    loss                 | 0.00287     |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    std                  | 0.0357      |\n",
      "|    value_loss           | 8.64e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6854400, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0409    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6854400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11197619 |\n",
      "|    clip_fraction        | 0.523      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.01       |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.000475   |\n",
      "|    loss                 | 0.00043    |\n",
      "|    n_updates            | 3390       |\n",
      "|    policy_gradient_loss | 0.031      |\n",
      "|    std                  | 0.0356     |\n",
      "|    value_loss           | 1.32e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0574  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 9602     |\n",
      "|    total_timesteps | 6854400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6874560, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0493     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6874560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037290975 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.02        |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.000472    |\n",
      "|    loss                 | 0.000902    |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | 0.0379      |\n",
      "|    std                  | 0.0356      |\n",
      "|    value_loss           | 3.63e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6894720, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0611     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6894720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035534028 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.02        |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.000469    |\n",
      "|    loss                 | 0.00224     |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | 0.0186      |\n",
      "|    std                  | 0.0354      |\n",
      "|    value_loss           | 8.28e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0549  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 9658     |\n",
      "|    total_timesteps | 6894720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6914880, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0721    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6914880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06937699 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.04       |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.000466   |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | 0.00945    |\n",
      "|    std                  | 0.0352     |\n",
      "|    value_loss           | 1.14e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6935040, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0734     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6935040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013577815 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.05        |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.000463    |\n",
      "|    loss                 | 0.00398     |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | 0.00456     |\n",
      "|    std                  | 0.0352      |\n",
      "|    value_loss           | 3.88e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0537  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 9714     |\n",
      "|    total_timesteps | 6935040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6955200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0641     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6955200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034451388 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.05        |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.00046     |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.0351      |\n",
      "|    value_loss           | 6.26e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6975360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0594     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6975360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009557127 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.06        |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.000457    |\n",
      "|    loss                 | 0.00474     |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | 0.00351     |\n",
      "|    std                  | 0.0349      |\n",
      "|    value_loss           | 2.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0541  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 9771     |\n",
      "|    total_timesteps | 6975360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6995520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.052      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6995520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008087835 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.07        |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.000454    |\n",
      "|    loss                 | -0.00262    |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | 0.00204     |\n",
      "|    std                  | 0.0347      |\n",
      "|    value_loss           | 6.48e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7015680, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.049     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7015680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03241296 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.08       |\n",
      "|    explained_variance   | 0.37       |\n",
      "|    learning_rate        | 0.000451   |\n",
      "|    loss                 | 0.00556    |\n",
      "|    n_updates            | 3470       |\n",
      "|    policy_gradient_loss | 0.00656    |\n",
      "|    std                  | 0.0347     |\n",
      "|    value_loss           | 1.73e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0556  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 9827     |\n",
      "|    total_timesteps | 7015680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7035840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0539     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7035840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016413888 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.09        |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.000448    |\n",
      "|    loss                 | 0.00139     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | 0.00278     |\n",
      "|    std                  | 0.0346      |\n",
      "|    value_loss           | 1.49e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7056000, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0679     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7056000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012126047 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.09        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.000445    |\n",
      "|    loss                 | -0.00142    |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | 0.00231     |\n",
      "|    std                  | 0.0344      |\n",
      "|    value_loss           | 2.39e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 9883     |\n",
      "|    total_timesteps | 7056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7076160, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0436     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7076160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011155861 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.11        |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.000442    |\n",
      "|    loss                 | -0.000197   |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.0341      |\n",
      "|    value_loss           | 7.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7096320, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0532     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7096320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011063886 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.12        |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.000439    |\n",
      "|    loss                 | 0.00662     |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | 0.0151      |\n",
      "|    std                  | 0.0341      |\n",
      "|    value_loss           | 5.5e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0616  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 9939     |\n",
      "|    total_timesteps | 7096320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7116480, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0474     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7116480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012049879 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.12        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.000436    |\n",
      "|    loss                 | -0.00315    |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | 0.00173     |\n",
      "|    std                  | 0.0342      |\n",
      "|    value_loss           | 1.33e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7136640, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0624     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7136640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011208883 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.11        |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.000433    |\n",
      "|    loss                 | -0.00104    |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | 0.0045      |\n",
      "|    std                  | 0.0341      |\n",
      "|    value_loss           | 1.06e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0595  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 9995     |\n",
      "|    total_timesteps | 7136640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7156800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0452   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7156800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1138139 |\n",
      "|    clip_fraction        | 0.419     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 4.12      |\n",
      "|    explained_variance   | 0.22      |\n",
      "|    learning_rate        | 0.00043   |\n",
      "|    loss                 | 0.00933   |\n",
      "|    n_updates            | 3540      |\n",
      "|    policy_gradient_loss | 0.0331    |\n",
      "|    std                  | 0.034     |\n",
      "|    value_loss           | 3.02e-08  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=7176960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0526     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7176960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016953804 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.12        |\n",
      "|    explained_variance   | 0.064       |\n",
      "|    learning_rate        | 0.000426    |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | 0.0148      |\n",
      "|    std                  | 0.0341      |\n",
      "|    value_loss           | 3.23e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0528  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 10051    |\n",
      "|    total_timesteps | 7176960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7197120, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0496     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7197120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018281661 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.12        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.000423    |\n",
      "|    loss                 | 0.00785     |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | 0.00542     |\n",
      "|    std                  | 0.0339      |\n",
      "|    value_loss           | 5.02e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7217280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0575     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7217280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042210765 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.13        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00042     |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | 0.00871     |\n",
      "|    std                  | 0.0338      |\n",
      "|    value_loss           | 6.68e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0534  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 10107    |\n",
      "|    total_timesteps | 7217280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7237440, episode_reward=-0.05 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0494     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7237440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011998212 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.14        |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.000417    |\n",
      "|    loss                 | -0.00398    |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | 0.00191     |\n",
      "|    std                  | 0.0335      |\n",
      "|    value_loss           | 1.89e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7257600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0537     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7257600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009002661 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.16        |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.000414    |\n",
      "|    loss                 | 0.00105     |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | 0.00161     |\n",
      "|    std                  | 0.0334      |\n",
      "|    value_loss           | 7.33e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0532  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 10163    |\n",
      "|    total_timesteps | 7257600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7277760, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0492      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7277760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090950625 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.17         |\n",
      "|    explained_variance   | 0.294        |\n",
      "|    learning_rate        | 0.000411     |\n",
      "|    loss                 | -0.000548    |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | 0.00102      |\n",
      "|    std                  | 0.0332       |\n",
      "|    value_loss           | 5.45e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7297920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0609    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7297920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01577163 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.18       |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 0.000408   |\n",
      "|    loss                 | -0.000771  |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | 0.0114     |\n",
      "|    std                  | 0.0332     |\n",
      "|    value_loss           | 9.3e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0563  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 10219    |\n",
      "|    total_timesteps | 7297920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7318080, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.053      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7318080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009769569 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.000405    |\n",
      "|    loss                 | 0.00694     |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | 0.00914     |\n",
      "|    std                  | 0.0332      |\n",
      "|    value_loss           | 1.15e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7338240, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0689     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7338240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023686497 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.000402    |\n",
      "|    loss                 | 0.00799     |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | 0.00586     |\n",
      "|    std                  | 0.0331      |\n",
      "|    value_loss           | 1.36e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0595  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 10275    |\n",
      "|    total_timesteps | 7338240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7358400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0493     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7358400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013952465 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.000399    |\n",
      "|    loss                 | 0.00188     |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | 0.00406     |\n",
      "|    std                  | 0.0331      |\n",
      "|    value_loss           | 2.06e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7378560, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0373     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7378560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018181164 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000396    |\n",
      "|    loss                 | 0.0068      |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | 0.000419    |\n",
      "|    std                  | 0.0333      |\n",
      "|    value_loss           | 9.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.062   |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 10331    |\n",
      "|    total_timesteps | 7378560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7398720, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0622    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7398720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01274039 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.16       |\n",
      "|    explained_variance   | 0.486      |\n",
      "|    learning_rate        | 0.000393   |\n",
      "|    loss                 | 0.00148    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | 0.00347    |\n",
      "|    std                  | 0.0333     |\n",
      "|    value_loss           | 1.11e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7418880, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0784     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7418880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023264669 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00039     |\n",
      "|    loss                 | -0.00265    |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | 0.00148     |\n",
      "|    std                  | 0.0333      |\n",
      "|    value_loss           | 9.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0615  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 10387    |\n",
      "|    total_timesteps | 7418880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7439040, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0503     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7439040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031566393 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.000387    |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | 0.00336     |\n",
      "|    std                  | 0.0333      |\n",
      "|    value_loss           | 8.35e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7459200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0551     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7459200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030085256 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.16        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.000384    |\n",
      "|    loss                 | -0.00105    |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    std                  | 0.0334      |\n",
      "|    value_loss           | 3.91e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0556  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 10444    |\n",
      "|    total_timesteps | 7459200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7479360, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0804     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7479360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038908727 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.16        |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.000381    |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | 0.0195      |\n",
      "|    std                  | 0.0332      |\n",
      "|    value_loss           | 2.95e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7499520, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0541     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7499520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016354706 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.000378    |\n",
      "|    loss                 | -0.00121    |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | 0.00879     |\n",
      "|    std                  | 0.0332      |\n",
      "|    value_loss           | 1.36e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 10499    |\n",
      "|    total_timesteps | 7499520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7519680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0549      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7519680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051607727 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.17         |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 0.000375     |\n",
      "|    loss                 | 0.00123      |\n",
      "|    n_updates            | 3720         |\n",
      "|    policy_gradient_loss | 0.00692      |\n",
      "|    std                  | 0.0331       |\n",
      "|    value_loss           | 2.24e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7539840, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0574     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7539840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022749994 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.000372    |\n",
      "|    loss                 | -0.000728   |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | 0.00242     |\n",
      "|    std                  | 0.0331      |\n",
      "|    value_loss           | 1.95e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0535  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 10555    |\n",
      "|    total_timesteps | 7539840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7560000, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0561     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056584775 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.000369    |\n",
      "|    loss                 | 0.00812     |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | 0.00695     |\n",
      "|    std                  | 0.0329      |\n",
      "|    value_loss           | 5.54e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7580160, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0398     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7580160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035335794 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.19        |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.000366    |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | 0.0202      |\n",
      "|    std                  | 0.0328      |\n",
      "|    value_loss           | 2.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0543  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 10611    |\n",
      "|    total_timesteps | 7580160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7600320, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0493     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7600320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008481458 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.2         |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.000363    |\n",
      "|    loss                 | 0.0068      |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | 0.00603     |\n",
      "|    std                  | 0.0328      |\n",
      "|    value_loss           | 1.18e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7620480, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0584   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7620480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0187911 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 4.2       |\n",
      "|    explained_variance   | 0.212     |\n",
      "|    learning_rate        | 0.00036   |\n",
      "|    loss                 | 0.00245   |\n",
      "|    n_updates            | 3770      |\n",
      "|    policy_gradient_loss | 0.00725   |\n",
      "|    std                  | 0.0327    |\n",
      "|    value_loss           | 3.04e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0536  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 10667    |\n",
      "|    total_timesteps | 7620480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7640640, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0577     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7640640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005841719 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.2         |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.000357    |\n",
      "|    loss                 | -0.000277   |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | 0.00167     |\n",
      "|    std                  | 0.0325      |\n",
      "|    value_loss           | 7.36e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7660800, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0364    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7660800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01145525 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.21       |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.000354   |\n",
      "|    loss                 | 0.00421    |\n",
      "|    n_updates            | 3790       |\n",
      "|    policy_gradient_loss | 0.00501    |\n",
      "|    std                  | 0.0324     |\n",
      "|    value_loss           | 5.44e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 10723    |\n",
      "|    total_timesteps | 7660800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7680960, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0654     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7680960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010009147 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.22        |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.000351    |\n",
      "|    loss                 | 0.00417     |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | 0.00315     |\n",
      "|    std                  | 0.0322      |\n",
      "|    value_loss           | 1.88e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7701120, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.062      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7701120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009735609 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.23        |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.000348    |\n",
      "|    loss                 | 3.72e-06    |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | 0.000758    |\n",
      "|    std                  | 0.0323      |\n",
      "|    value_loss           | 1.93e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0586  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 10780    |\n",
      "|    total_timesteps | 7701120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7721280, episode_reward=-0.05 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0537      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7721280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049143257 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.23         |\n",
      "|    explained_variance   | 0.468        |\n",
      "|    learning_rate        | 0.000345     |\n",
      "|    loss                 | 0.00124      |\n",
      "|    n_updates            | 3820         |\n",
      "|    policy_gradient_loss | 0.00179      |\n",
      "|    std                  | 0.0323       |\n",
      "|    value_loss           | 9.1e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7741440, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0594     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7741440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016781513 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.23        |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.000342    |\n",
      "|    loss                 | 0.00569     |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | 0.00899     |\n",
      "|    std                  | 0.0323      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0597  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 10836    |\n",
      "|    total_timesteps | 7741440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7761600, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0442     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7761600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012513068 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.23        |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.000339    |\n",
      "|    loss                 | 0.00472     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | 0.00421     |\n",
      "|    std                  | 0.0322      |\n",
      "|    value_loss           | 8.36e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7781760, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0607    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7781760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02883887 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.23       |\n",
      "|    explained_variance   | 0.16       |\n",
      "|    learning_rate        | 0.000336   |\n",
      "|    loss                 | 0.00417    |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | 0.0124     |\n",
      "|    std                  | 0.0322     |\n",
      "|    value_loss           | 3.19e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0601  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 10892    |\n",
      "|    total_timesteps | 7781760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7801920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0577     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7801920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025857018 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.24        |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.000333    |\n",
      "|    loss                 | -0.000107   |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | 0.00385     |\n",
      "|    std                  | 0.0322      |\n",
      "|    value_loss           | 2.76e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7822080, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7822080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008244239 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.24        |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.00033     |\n",
      "|    loss                 | -0.00209    |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.00424     |\n",
      "|    std                  | 0.0322      |\n",
      "|    value_loss           | 4.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0565  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 10948    |\n",
      "|    total_timesteps | 7822080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7842240, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0717     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7842240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047322452 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.24        |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.000327    |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    std                  | 0.0322      |\n",
      "|    value_loss           | 3.33e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7862400, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.043      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7862400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026512392 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.25        |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.000324    |\n",
      "|    loss                 | 0.00151     |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | 0.00169     |\n",
      "|    std                  | 0.032       |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0576  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 11004    |\n",
      "|    total_timesteps | 7862400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7882560, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0657     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7882560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020439196 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.26        |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.000321    |\n",
      "|    loss                 | 0.00872     |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | 0.00641     |\n",
      "|    std                  | 0.0318      |\n",
      "|    value_loss           | 5.45e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7902720, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0596     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7902720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012333358 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.26        |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.000318    |\n",
      "|    loss                 | 0.00235     |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | 0.00449     |\n",
      "|    std                  | 0.0318      |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 11060    |\n",
      "|    total_timesteps | 7902720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7922880, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0424     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7922880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057555325 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.000315    |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | 0.0113      |\n",
      "|    std                  | 0.0318      |\n",
      "|    value_loss           | 1.18e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7943040, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0458     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7943040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023165366 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.000312    |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | 0.00664     |\n",
      "|    std                  | 0.0318      |\n",
      "|    value_loss           | 8.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0594  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 11116    |\n",
      "|    total_timesteps | 7943040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7963200, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0523     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7963200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023383312 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.000309    |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | 0.00912     |\n",
      "|    std                  | 0.0317      |\n",
      "|    value_loss           | 5.06e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7983360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0468     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7983360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015557732 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.000306    |\n",
      "|    loss                 | 0.00998     |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    std                  | 0.0316      |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0605  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 11172    |\n",
      "|    total_timesteps | 7983360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8003520, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0421     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8003520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009766415 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.28        |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.000302    |\n",
      "|    loss                 | 0.00056     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | 0.00498     |\n",
      "|    std                  | 0.0314      |\n",
      "|    value_loss           | 2e-07       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8023680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0493      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8023680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053865057 |\n",
      "|    clip_fraction        | 0.0939       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.29         |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.000299     |\n",
      "|    loss                 | -0.00146     |\n",
      "|    n_updates            | 3970         |\n",
      "|    policy_gradient_loss | 0.00114      |\n",
      "|    std                  | 0.0313       |\n",
      "|    value_loss           | 3.87e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0582  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 11228    |\n",
      "|    total_timesteps | 8023680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8043840, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8043840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022906043 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.3         |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.000296    |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 3980        |\n",
      "|    policy_gradient_loss | 0.00166     |\n",
      "|    std                  | 0.0312      |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8064000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0542     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8064000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022318717 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.3         |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.000293    |\n",
      "|    loss                 | 0.00329     |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | 0.00851     |\n",
      "|    std                  | 0.0312      |\n",
      "|    value_loss           | 1.11e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0592  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 11283    |\n",
      "|    total_timesteps | 8064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8084160, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.054       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8084160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047461144 |\n",
      "|    clip_fraction        | 0.0865       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.31         |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.00029      |\n",
      "|    loss                 | 0.00223      |\n",
      "|    n_updates            | 4000         |\n",
      "|    policy_gradient_loss | 0.001        |\n",
      "|    std                  | 0.031        |\n",
      "|    value_loss           | 3.36e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8104320, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0573     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8104320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032549225 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.32        |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.000287    |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.031       |\n",
      "|    value_loss           | 4.01e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0551  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 11339    |\n",
      "|    total_timesteps | 8104320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8124480, episode_reward=-0.04 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0403     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8124480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027355341 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.32        |\n",
      "|    explained_variance   | 0.0271      |\n",
      "|    learning_rate        | 0.000284    |\n",
      "|    loss                 | 0.022       |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.0309      |\n",
      "|    value_loss           | 3.54e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8144640, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0444     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8144640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008412246 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.33        |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.000281    |\n",
      "|    loss                 | 0.00295     |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | 0.00764     |\n",
      "|    std                  | 0.0308      |\n",
      "|    value_loss           | 3.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0557  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 11395    |\n",
      "|    total_timesteps | 8144640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8164800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0485     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8164800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008725135 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.33        |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.000278    |\n",
      "|    loss                 | -0.000238   |\n",
      "|    n_updates            | 4040        |\n",
      "|    policy_gradient_loss | 0.00284     |\n",
      "|    std                  | 0.0307      |\n",
      "|    value_loss           | 6.18e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8184960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0472      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8184960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068263477 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.34         |\n",
      "|    explained_variance   | 0.612        |\n",
      "|    learning_rate        | 0.000275     |\n",
      "|    loss                 | 0.00261      |\n",
      "|    n_updates            | 4050         |\n",
      "|    policy_gradient_loss | 0.000683     |\n",
      "|    std                  | 0.0306       |\n",
      "|    value_loss           | 5.34e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0511  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 11450    |\n",
      "|    total_timesteps | 8184960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8205120, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8205120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003068843 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.34        |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.000272    |\n",
      "|    loss                 | -0.00141    |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | 0.00846     |\n",
      "|    std                  | 0.0305      |\n",
      "|    value_loss           | 2.15e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8225280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0642     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8225280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007827081 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.35        |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.000269    |\n",
      "|    loss                 | -0.00197    |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | 0.00326     |\n",
      "|    std                  | 0.0305      |\n",
      "|    value_loss           | 6.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0516  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 11506    |\n",
      "|    total_timesteps | 8225280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8245440, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0478     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8245440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009615004 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.35        |\n",
      "|    explained_variance   | -0.103      |\n",
      "|    learning_rate        | 0.000266    |\n",
      "|    loss                 | -0.000366   |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | 0.00289     |\n",
      "|    std                  | 0.0304      |\n",
      "|    value_loss           | 4.32e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8265600, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0425    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8265600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00520893 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.36       |\n",
      "|    explained_variance   | 0.392      |\n",
      "|    learning_rate        | 0.000263   |\n",
      "|    loss                 | -0.00156   |\n",
      "|    n_updates            | 4090       |\n",
      "|    policy_gradient_loss | 0.00328    |\n",
      "|    std                  | 0.0304     |\n",
      "|    value_loss           | 5.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0507  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 11562    |\n",
      "|    total_timesteps | 8265600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8285760, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0507     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8285760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005781618 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.36        |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.00026     |\n",
      "|    loss                 | 6.8e-05     |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | 0.000372    |\n",
      "|    std                  | 0.0303      |\n",
      "|    value_loss           | 3.53e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8305920, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0741     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8305920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020599134 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.36        |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | 0.00813     |\n",
      "|    std                  | 0.0303      |\n",
      "|    value_loss           | 5.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0518  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 11618    |\n",
      "|    total_timesteps | 8305920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8326080, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0457     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8326080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020958932 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.36        |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.000254    |\n",
      "|    loss                 | 0.00723     |\n",
      "|    n_updates            | 4120        |\n",
      "|    policy_gradient_loss | 0.00842     |\n",
      "|    std                  | 0.0304      |\n",
      "|    value_loss           | 3.95e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8346240, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0624     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8346240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019930245 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.36        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | -0.00297    |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    std                  | 0.0303      |\n",
      "|    value_loss           | 5.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0547  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 11673    |\n",
      "|    total_timesteps | 8346240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8366400, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0422      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8366400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030387514 |\n",
      "|    clip_fraction        | 0.0891       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.37         |\n",
      "|    explained_variance   | 0.39         |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | 0.00786      |\n",
      "|    n_updates            | 4140         |\n",
      "|    policy_gradient_loss | 0.00127      |\n",
      "|    std                  | 0.0302       |\n",
      "|    value_loss           | 5.72e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8386560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.059       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8386560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077500204 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.38         |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | 5.09e-05     |\n",
      "|    n_updates            | 4150         |\n",
      "|    policy_gradient_loss | 0.00195      |\n",
      "|    std                  | 0.0301       |\n",
      "|    value_loss           | 9.84e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0563  |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 11729    |\n",
      "|    total_timesteps | 8386560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8406720, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0494     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8406720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005624556 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.38        |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | 0.00105     |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | 4.4e-05     |\n",
      "|    std                  | 0.03        |\n",
      "|    value_loss           | 7.11e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8426880, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0484     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8426880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022771172 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.39        |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | 0.00496     |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | 0.00302     |\n",
      "|    std                  | 0.03        |\n",
      "|    value_loss           | 3.82e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.054   |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 11785    |\n",
      "|    total_timesteps | 8426880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8447040, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0636     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8447040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047654107 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.39        |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.000236    |\n",
      "|    loss                 | 0.000792    |\n",
      "|    n_updates            | 4180        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.0299      |\n",
      "|    value_loss           | 8.46e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8467200, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0493      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8467200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050938376 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.39         |\n",
      "|    explained_variance   | 0.0915       |\n",
      "|    learning_rate        | 0.000233     |\n",
      "|    loss                 | -0.00132     |\n",
      "|    n_updates            | 4190         |\n",
      "|    policy_gradient_loss | 0.00231      |\n",
      "|    std                  | 0.0299       |\n",
      "|    value_loss           | 6.39e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0533  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 11840    |\n",
      "|    total_timesteps | 8467200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8487360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0471    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8487360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04405876 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.4        |\n",
      "|    explained_variance   | 0.188      |\n",
      "|    learning_rate        | 0.00023    |\n",
      "|    loss                 | 0.0398     |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | 0.00751    |\n",
      "|    std                  | 0.0299     |\n",
      "|    value_loss           | 6.22e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8507520, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0751      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8507520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037545555 |\n",
      "|    clip_fraction        | 0.191        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.4          |\n",
      "|    explained_variance   | 0.182        |\n",
      "|    learning_rate        | 0.000227     |\n",
      "|    loss                 | -0.00061     |\n",
      "|    n_updates            | 4210         |\n",
      "|    policy_gradient_loss | 0.00842      |\n",
      "|    std                  | 0.0298       |\n",
      "|    value_loss           | 2.53e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0512  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 11896    |\n",
      "|    total_timesteps | 8507520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8527680, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0363     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8527680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035374105 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.000224    |\n",
      "|    loss                 | 0.00969     |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | 0.0151      |\n",
      "|    std                  | 0.0298      |\n",
      "|    value_loss           | 5.01e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8547840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0511    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8547840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03812033 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.4        |\n",
      "|    explained_variance   | 0.256      |\n",
      "|    learning_rate        | 0.000221   |\n",
      "|    loss                 | 0.00774    |\n",
      "|    n_updates            | 4230       |\n",
      "|    policy_gradient_loss | 0.00802    |\n",
      "|    std                  | 0.0298     |\n",
      "|    value_loss           | 1.08e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0522  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 11952    |\n",
      "|    total_timesteps | 8547840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8568000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0501     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8568000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010105449 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | -0.0831     |\n",
      "|    learning_rate        | 0.000218    |\n",
      "|    loss                 | 0.000884    |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | 0.00705     |\n",
      "|    std                  | 0.0299      |\n",
      "|    value_loss           | 4.69e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8588160, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0506      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8588160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052038133 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.4          |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.000215     |\n",
      "|    loss                 | -0.00126     |\n",
      "|    n_updates            | 4250         |\n",
      "|    policy_gradient_loss | 0.00258      |\n",
      "|    std                  | 0.0298       |\n",
      "|    value_loss           | 4.5e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0553  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 12008    |\n",
      "|    total_timesteps | 8588160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8608320, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0533     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8608320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006015968 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.41        |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.000212    |\n",
      "|    loss                 | 0.00335     |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | 0.00268     |\n",
      "|    std                  | 0.0297      |\n",
      "|    value_loss           | 9.23e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8628480, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0786     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8628480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004396331 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.41        |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.000209    |\n",
      "|    loss                 | -0.00311    |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | 0.00258     |\n",
      "|    std                  | 0.0298      |\n",
      "|    value_loss           | 7.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0571  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 12063    |\n",
      "|    total_timesteps | 8628480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8648640, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0422     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8648640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013561312 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.000206    |\n",
      "|    loss                 | 0.000318    |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | 0.00689     |\n",
      "|    std                  | 0.0298      |\n",
      "|    value_loss           | 6.84e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8668800, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0633     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8668800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009367321 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.000203    |\n",
      "|    loss                 | 0.00415     |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | 0.00103     |\n",
      "|    std                  | 0.0298      |\n",
      "|    value_loss           | 1.73e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0561  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 12119    |\n",
      "|    total_timesteps | 8668800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8688960, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0544      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8688960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043760156 |\n",
      "|    clip_fraction        | 0.0699       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.4          |\n",
      "|    explained_variance   | 0.348        |\n",
      "|    learning_rate        | 0.0002       |\n",
      "|    loss                 | -0.00218     |\n",
      "|    n_updates            | 4300         |\n",
      "|    policy_gradient_loss | 2.87e-05     |\n",
      "|    std                  | 0.0298       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8709120, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0744     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8709120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009942253 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.000197    |\n",
      "|    loss                 | -5.98e-05   |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | 0.00127     |\n",
      "|    std                  | 0.0297      |\n",
      "|    value_loss           | 5.75e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0581  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 12176    |\n",
      "|    total_timesteps | 8709120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8729280, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0624    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8729280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01161674 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.41       |\n",
      "|    explained_variance   | 0.207      |\n",
      "|    learning_rate        | 0.000194   |\n",
      "|    loss                 | 0.00645    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | 0.00207    |\n",
      "|    std                  | 0.0297     |\n",
      "|    value_loss           | 3.18e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8749440, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0649     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8749440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012834053 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.41        |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.000191    |\n",
      "|    loss                 | -0.00165    |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | 0.000779    |\n",
      "|    std                  | 0.0296      |\n",
      "|    value_loss           | 3.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0568  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 12232    |\n",
      "|    total_timesteps | 8749440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8769600, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0513     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8769600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008807193 |\n",
      "|    clip_fraction        | 0.0722      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.41        |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.000188    |\n",
      "|    loss                 | 0.0057      |\n",
      "|    n_updates            | 4340        |\n",
      "|    policy_gradient_loss | 0.000436    |\n",
      "|    std                  | 0.0297      |\n",
      "|    value_loss           | 1.35e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8789760, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0703     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8789760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004589831 |\n",
      "|    clip_fraction        | 0.0785      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.000185    |\n",
      "|    loss                 | -0.00148    |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | -0.000886   |\n",
      "|    std                  | 0.0295      |\n",
      "|    value_loss           | 5.71e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 12288    |\n",
      "|    total_timesteps | 8789760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8809920, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0697    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8809920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00634317 |\n",
      "|    clip_fraction        | 0.0643     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.42       |\n",
      "|    explained_variance   | 0.366      |\n",
      "|    learning_rate        | 0.000182   |\n",
      "|    loss                 | -0.000417  |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.000336  |\n",
      "|    std                  | 0.0294     |\n",
      "|    value_loss           | 5.14e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8830080, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8830080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021391284 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.000179    |\n",
      "|    loss                 | -0.00122    |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    std                  | 0.0294      |\n",
      "|    value_loss           | 1.01e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0544  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 12344    |\n",
      "|    total_timesteps | 8830080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8850240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0486     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8850240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030071866 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.000175    |\n",
      "|    loss                 | 0.000993    |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | 0.00112     |\n",
      "|    std                  | 0.0294      |\n",
      "|    value_loss           | 4.76e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8870400, episode_reward=-0.05 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0536     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8870400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012287132 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.000172    |\n",
      "|    loss                 | 0.00158     |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | 0.00188     |\n",
      "|    std                  | 0.0295      |\n",
      "|    value_loss           | 1.59e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0576  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 12400    |\n",
      "|    total_timesteps | 8870400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8890560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0619     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8890560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010919854 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.000169    |\n",
      "|    loss                 | -0.000434   |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | 0.00253     |\n",
      "|    std                  | 0.0294      |\n",
      "|    value_loss           | 9.52e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8910720, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0444     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8910720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006558625 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.0822      |\n",
      "|    learning_rate        | 0.000166    |\n",
      "|    loss                 | 0.00174     |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | 0.00311     |\n",
      "|    std                  | 0.0294      |\n",
      "|    value_loss           | 4.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0534  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 12456    |\n",
      "|    total_timesteps | 8910720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8930880, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0654    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8930880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01808497 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.42       |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.000163   |\n",
      "|    loss                 | -0.000213  |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | 0.00316    |\n",
      "|    std                  | 0.0294     |\n",
      "|    value_loss           | 5.13e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8951040, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0434     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8951040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009869608 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00016     |\n",
      "|    loss                 | 0.00592     |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | 0.00195     |\n",
      "|    std                  | 0.0294      |\n",
      "|    value_loss           | 5.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0545  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 12513    |\n",
      "|    total_timesteps | 8951040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8971200, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0601      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8971200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056587956 |\n",
      "|    clip_fraction        | 0.0992       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.43         |\n",
      "|    explained_variance   | 0.0269       |\n",
      "|    learning_rate        | 0.000157     |\n",
      "|    loss                 | -0.00102     |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | 0.000858     |\n",
      "|    std                  | 0.0293       |\n",
      "|    value_loss           | 3.98e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8991360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.06       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8991360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015270822 |\n",
      "|    clip_fraction        | 0.0946      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.43        |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.000154    |\n",
      "|    loss                 | -0.00108    |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | 0.000421    |\n",
      "|    std                  | 0.0293      |\n",
      "|    value_loss           | 1.92e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0558  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 12569    |\n",
      "|    total_timesteps | 8991360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9011520, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0707      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9011520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038476463 |\n",
      "|    clip_fraction        | 0.0793       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.43         |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.000151     |\n",
      "|    loss                 | -0.00162     |\n",
      "|    n_updates            | 4460         |\n",
      "|    policy_gradient_loss | 0.00028      |\n",
      "|    std                  | 0.0293       |\n",
      "|    value_loss           | 9.64e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9031680, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0713     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9031680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009804488 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.43        |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.000148    |\n",
      "|    loss                 | -0.00044    |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | 0.0048      |\n",
      "|    std                  | 0.0292      |\n",
      "|    value_loss           | 9.72e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 12625    |\n",
      "|    total_timesteps | 9031680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9051840, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0533     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9051840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020454628 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.43        |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.000145    |\n",
      "|    loss                 | -0.001      |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    std                  | 0.0292      |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9072000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0401      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9072000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076930737 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.000142     |\n",
      "|    loss                 | 0.0018       |\n",
      "|    n_updates            | 4490         |\n",
      "|    policy_gradient_loss | 0.00054      |\n",
      "|    std                  | 0.0291       |\n",
      "|    value_loss           | 3e-07        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0597  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 12681    |\n",
      "|    total_timesteps | 9072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9092160, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0927     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9092160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005313937 |\n",
      "|    clip_fraction        | 0.045       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.44        |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.000139    |\n",
      "|    loss                 | 0.00237     |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.000562   |\n",
      "|    std                  | 0.0291      |\n",
      "|    value_loss           | 4.76e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9112320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0467     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9112320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004706517 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.44        |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.000136    |\n",
      "|    loss                 | -7.23e-05   |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -6.5e-05    |\n",
      "|    std                  | 0.029       |\n",
      "|    value_loss           | 3.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0575  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 12737    |\n",
      "|    total_timesteps | 9112320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9132480, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0574     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9132480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017200068 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.44        |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.000133    |\n",
      "|    loss                 | 0.007       |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | 0.00114     |\n",
      "|    std                  | 0.0291      |\n",
      "|    value_loss           | 8.81e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9152640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0516     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9152640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005677766 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.44        |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.00013     |\n",
      "|    loss                 | -0.000625   |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.000137   |\n",
      "|    std                  | 0.029       |\n",
      "|    value_loss           | 5.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0584  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 12793    |\n",
      "|    total_timesteps | 9152640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9172800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0515      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9172800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076584555 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.45         |\n",
      "|    explained_variance   | 0.27         |\n",
      "|    learning_rate        | 0.000127     |\n",
      "|    loss                 | 0.00326      |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | 0.00251      |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 1.03e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9192960, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0568      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9192960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066367006 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.176        |\n",
      "|    learning_rate        | 0.000124     |\n",
      "|    loss                 | -0.00141     |\n",
      "|    n_updates            | 4550         |\n",
      "|    policy_gradient_loss | -0.000596    |\n",
      "|    std                  | 0.0291       |\n",
      "|    value_loss           | 4.32e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0549  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 12848    |\n",
      "|    total_timesteps | 9192960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9213120, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0605      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9213120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033124983 |\n",
      "|    clip_fraction        | 0.0954       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.247        |\n",
      "|    learning_rate        | 0.000121     |\n",
      "|    loss                 | -0.00154     |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | 0.000211     |\n",
      "|    std                  | 0.0291       |\n",
      "|    value_loss           | 3.51e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9233280, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0529     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9233280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006481124 |\n",
      "|    clip_fraction        | 0.0968      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.44        |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.000118    |\n",
      "|    loss                 | -0.00128    |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    std                  | 0.0291      |\n",
      "|    value_loss           | 5.37e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0555  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 12904    |\n",
      "|    total_timesteps | 9233280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9253440, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0608      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9253440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032206313 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.0294       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | -0.00156     |\n",
      "|    n_updates            | 4580         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 3.07e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9273600, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0493     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9273600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007029308 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.45        |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.000112    |\n",
      "|    loss                 | 0.00199     |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.000318   |\n",
      "|    std                  | 0.029       |\n",
      "|    value_loss           | 7.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.053   |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 12960    |\n",
      "|    total_timesteps | 9273600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9293760, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.043       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9293760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058826855 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.45         |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 0.000109     |\n",
      "|    loss                 | -0.000196    |\n",
      "|    n_updates            | 4600         |\n",
      "|    policy_gradient_loss | -0.000697    |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 3.87e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9313920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0586      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9313920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027648392 |\n",
      "|    clip_fraction        | 0.091        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.36         |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | -0.0003      |\n",
      "|    n_updates            | 4610         |\n",
      "|    policy_gradient_loss | 0.00144      |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 4.78e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0524  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 13016    |\n",
      "|    total_timesteps | 9313920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9334080, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0636      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9334080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025791088 |\n",
      "|    clip_fraction        | 0.0847       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.000103     |\n",
      "|    loss                 | -0.00134     |\n",
      "|    n_updates            | 4620         |\n",
      "|    policy_gradient_loss | 0.000594     |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 8.83e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9354240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0537      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9354240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067559592 |\n",
      "|    clip_fraction        | 0.0969       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.44         |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 9.99e-05     |\n",
      "|    loss                 | 5.08e-05     |\n",
      "|    n_updates            | 4630         |\n",
      "|    policy_gradient_loss | 0.00077      |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 8.09e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0559  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 13071    |\n",
      "|    total_timesteps | 9354240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9374400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0518      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9374400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026641807 |\n",
      "|    clip_fraction        | 0.0519       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.45         |\n",
      "|    explained_variance   | 0.512        |\n",
      "|    learning_rate        | 9.69e-05     |\n",
      "|    loss                 | -0.00279     |\n",
      "|    n_updates            | 4640         |\n",
      "|    policy_gradient_loss | -0.000371    |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 6.26e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9394560, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0459      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9394560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043627764 |\n",
      "|    clip_fraction        | 0.0587       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.45         |\n",
      "|    explained_variance   | 0.273        |\n",
      "|    learning_rate        | 9.38e-05     |\n",
      "|    loss                 | 0.000221     |\n",
      "|    n_updates            | 4650         |\n",
      "|    policy_gradient_loss | -0.000219    |\n",
      "|    std                  | 0.0289       |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0562  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 13127    |\n",
      "|    total_timesteps | 9394560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9414720, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0584      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9414720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036014668 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.45         |\n",
      "|    explained_variance   | 0.0908       |\n",
      "|    learning_rate        | 9.08e-05     |\n",
      "|    loss                 | -0.00493     |\n",
      "|    n_updates            | 4660         |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 4.47e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9434880, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0657      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9434880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034739892 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 8.78e-05     |\n",
      "|    loss                 | -0.000184    |\n",
      "|    n_updates            | 4670         |\n",
      "|    policy_gradient_loss | -0.000774    |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 8.11e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0565  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 13183    |\n",
      "|    total_timesteps | 9434880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9455040, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0697     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9455040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006844487 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.46        |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 8.48e-05    |\n",
      "|    loss                 | 0.00173     |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | 0.000116    |\n",
      "|    std                  | 0.0289      |\n",
      "|    value_loss           | 5.25e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9475200, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0734     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9475200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003045738 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.45        |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 8.17e-05    |\n",
      "|    loss                 | -0.00349    |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | -4.82e-05   |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 3.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0535  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 13238    |\n",
      "|    total_timesteps | 9475200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9495360, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0357      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9495360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030918494 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.501        |\n",
      "|    learning_rate        | 7.87e-05     |\n",
      "|    loss                 | -0.00168     |\n",
      "|    n_updates            | 4700         |\n",
      "|    policy_gradient_loss | 0.000223     |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 4.25e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9515520, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0448    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9515520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00873103 |\n",
      "|    clip_fraction        | 0.0729     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.46       |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 7.57e-05   |\n",
      "|    loss                 | 0.0029     |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | -0.00099   |\n",
      "|    std                  | 0.0288     |\n",
      "|    value_loss           | 5.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0533  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 13294    |\n",
      "|    total_timesteps | 9515520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9535680, episode_reward=-0.04 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0448     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9535680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004572274 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.46        |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 7.27e-05    |\n",
      "|    loss                 | -0.0017     |\n",
      "|    n_updates            | 4720        |\n",
      "|    policy_gradient_loss | -0.000839   |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 6.42e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9555840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0474      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9555840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044314587 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.0739       |\n",
      "|    learning_rate        | 6.96e-05     |\n",
      "|    loss                 | -0.000906    |\n",
      "|    n_updates            | 4730         |\n",
      "|    policy_gradient_loss | 0.000208     |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0548  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 13350    |\n",
      "|    total_timesteps | 9555840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9576000, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.075      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9576000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004211859 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.46        |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 6.66e-05    |\n",
      "|    loss                 | -0.00111    |\n",
      "|    n_updates            | 4740        |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 8.52e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9596160, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0473      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9596160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021030034 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 6.36e-05     |\n",
      "|    loss                 | -0.000173    |\n",
      "|    n_updates            | 4750         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0546  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 13406    |\n",
      "|    total_timesteps | 9596160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9616320, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0545      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9616320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046882248 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.384        |\n",
      "|    learning_rate        | 6.06e-05     |\n",
      "|    loss                 | -0.00158     |\n",
      "|    n_updates            | 4760         |\n",
      "|    policy_gradient_loss | -0.000635    |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 4.44e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9636480, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0524     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9636480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004784235 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.46        |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 5.76e-05    |\n",
      "|    loss                 | -0.00132    |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.00094    |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 4e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0539  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 13461    |\n",
      "|    total_timesteps | 9636480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9656640, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0652      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9656640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026069665 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 5.45e-05     |\n",
      "|    loss                 | -0.00391     |\n",
      "|    n_updates            | 4780         |\n",
      "|    policy_gradient_loss | -0.000353    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 6.23e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9676800, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0498     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9676800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006327306 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.47        |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 5.15e-05    |\n",
      "|    loss                 | 9.06e-07    |\n",
      "|    n_updates            | 4790        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    std                  | 0.0287      |\n",
      "|    value_loss           | 1.21e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.055   |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 13517    |\n",
      "|    total_timesteps | 9676800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9696960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0524    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9696960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00493889 |\n",
      "|    clip_fraction        | 0.0356     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.46       |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 4.85e-05   |\n",
      "|    loss                 | -0.000766  |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.000944  |\n",
      "|    std                  | 0.0288     |\n",
      "|    value_loss           | 6.44e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9717120, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0533      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9717120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036861687 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.303        |\n",
      "|    learning_rate        | 4.55e-05     |\n",
      "|    loss                 | -0.00245     |\n",
      "|    n_updates            | 4810         |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 3.42e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0533  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 13573    |\n",
      "|    total_timesteps | 9717120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9737280, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0613      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9737280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060873795 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 4.24e-05     |\n",
      "|    loss                 | 0.000838     |\n",
      "|    n_updates            | 4820         |\n",
      "|    policy_gradient_loss | -0.000501    |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 3.01e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9757440, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0725      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9757440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056818593 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 3.94e-05     |\n",
      "|    loss                 | -0.00244     |\n",
      "|    n_updates            | 4830         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 4.57e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0547  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 13628    |\n",
      "|    total_timesteps | 9757440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9777600, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0682      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9777600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025484692 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.398        |\n",
      "|    learning_rate        | 3.64e-05     |\n",
      "|    loss                 | 0.000329     |\n",
      "|    n_updates            | 4840         |\n",
      "|    policy_gradient_loss | -0.000545    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 1.5e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9797760, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0441      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9797760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031650164 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.39         |\n",
      "|    learning_rate        | 3.34e-05     |\n",
      "|    loss                 | -0.00304     |\n",
      "|    n_updates            | 4850         |\n",
      "|    policy_gradient_loss | -0.000493    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 5.64e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0536  |\n",
      "| time/              |          |\n",
      "|    fps             | 715      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 13684    |\n",
      "|    total_timesteps | 9797760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9817920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0561      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9817920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036631234 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.22         |\n",
      "|    learning_rate        | 3.03e-05     |\n",
      "|    loss                 | -0.00337     |\n",
      "|    n_updates            | 4860         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 8.49e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9838080, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0664      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9838080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048516453 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 2.73e-05     |\n",
      "|    loss                 | -1.76e-05    |\n",
      "|    n_updates            | 4870         |\n",
      "|    policy_gradient_loss | -0.000804    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 4.24e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0536  |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 13740    |\n",
      "|    total_timesteps | 9838080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9858240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0561     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9858240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002237006 |\n",
      "|    clip_fraction        | 0.0106      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.47        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 2.43e-05    |\n",
      "|    loss                 | -0.00018    |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.00069    |\n",
      "|    std                  | 0.0287      |\n",
      "|    value_loss           | 5.3e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9878400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0484      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9878400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018532119 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.293        |\n",
      "|    learning_rate        | 2.13e-05     |\n",
      "|    loss                 | -0.00178     |\n",
      "|    n_updates            | 4890         |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 5.49e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0569  |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 490      |\n",
      "|    time_elapsed    | 13795    |\n",
      "|    total_timesteps | 9878400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9898560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0616     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9898560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002700082 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.47        |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 1.82e-05    |\n",
      "|    loss                 | 0.000554    |\n",
      "|    n_updates            | 4900        |\n",
      "|    policy_gradient_loss | -0.000784   |\n",
      "|    std                  | 0.0287      |\n",
      "|    value_loss           | 3.31e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9918720, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0513     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9918720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002982598 |\n",
      "|    clip_fraction        | 0.00995     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.47        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 1.52e-05    |\n",
      "|    loss                 | -0.000628   |\n",
      "|    n_updates            | 4910        |\n",
      "|    policy_gradient_loss | -0.000318   |\n",
      "|    std                  | 0.0287      |\n",
      "|    value_loss           | 5.02e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 492      |\n",
      "|    time_elapsed    | 13851    |\n",
      "|    total_timesteps | 9918720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9938880, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0396     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9938880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002393675 |\n",
      "|    clip_fraction        | 0.00332     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.47        |\n",
      "|    explained_variance   | -0.121      |\n",
      "|    learning_rate        | 1.22e-05    |\n",
      "|    loss                 | 0.00129     |\n",
      "|    n_updates            | 4920        |\n",
      "|    policy_gradient_loss | -0.000215   |\n",
      "|    std                  | 0.0287      |\n",
      "|    value_loss           | 9.05e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9959040, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0443      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9959040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017193526 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 9.17e-06     |\n",
      "|    loss                 | -0.00145     |\n",
      "|    n_updates            | 4930         |\n",
      "|    policy_gradient_loss | -0.000732    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 5.21e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0603  |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 494      |\n",
      "|    time_elapsed    | 13907    |\n",
      "|    total_timesteps | 9959040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9979200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0595      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9979200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012481756 |\n",
      "|    clip_fraction        | 0.000134     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.434        |\n",
      "|    learning_rate        | 6.14e-06     |\n",
      "|    loss                 | -0.0006      |\n",
      "|    n_updates            | 4940         |\n",
      "|    policy_gradient_loss | -0.000532    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 1.13e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9999360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0689      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9999360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023664522 |\n",
      "|    clip_fraction        | 0.00755      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.229        |\n",
      "|    learning_rate        | 3.12e-06     |\n",
      "|    loss                 | -0.000402    |\n",
      "|    n_updates            | 4950         |\n",
      "|    policy_gradient_loss | -0.000473    |\n",
      "|    std                  | 0.0287       |\n",
      "|    value_loss           | 2.02e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0542  |\n",
      "| time/              |          |\n",
      "|    fps             | 716      |\n",
      "|    iterations      | 496      |\n",
      "|    time_elapsed    | 13963    |\n",
      "|    total_timesteps | 9999360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10019520, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0566       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10019520      |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2210104e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 4.47          |\n",
      "|    explained_variance   | 0.485         |\n",
      "|    learning_rate        | 9.6e-08       |\n",
      "|    loss                 | -9.74e-05     |\n",
      "|    n_updates            | 4960          |\n",
      "|    policy_gradient_loss | -3.58e-05     |\n",
      "|    std                  | 0.0287        |\n",
      "|    value_loss           | 8.35e-08      |\n",
      "-------------------------------------------\n",
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | -0.00994 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0115     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004267407 |\n",
      "|    clip_fraction        | 0.0165      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0811      |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | -0.00198    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.000318    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0744  |\n",
      "| time/              |          |\n",
      "|    fps             | 1017     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0175      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039381697 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.72        |\n",
      "|    explained_variance   | 0.0102       |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | 0.000828     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    std                  | 0.927        |\n",
      "|    value_loss           | 1.36e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80640        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058401325 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | -0.0255      |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | -0.00555     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 3.96e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0642  |\n",
      "| time/              |          |\n",
      "|    fps             | 957      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005177913 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | -0.0475     |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00476    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    std                  | 0.841       |\n",
      "|    value_loss           | 1.9e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0133      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033498602 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | 0.0343       |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00143     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.822        |\n",
      "|    value_loss           | 7.9e-06      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0644  |\n",
      "| time/              |          |\n",
      "|    fps             | 941      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004973761 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.0552      |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.00575    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    std                  | 0.782       |\n",
      "|    value_loss           | 3.23e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0163      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 161280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068051247 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.28        |\n",
      "|    explained_variance   | 0.0153       |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00686     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    std                  | 0.738        |\n",
      "|    value_loss           | 4.86e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0644  |\n",
      "| time/              |          |\n",
      "|    fps             | 934      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.014       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058409926 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.19        |\n",
      "|    explained_variance   | 0.0622       |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00436     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    std                  | 0.716        |\n",
      "|    value_loss           | 1.26e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0155    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 201600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00507595 |\n",
      "|    clip_fraction        | 0.0356     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.13      |\n",
      "|    explained_variance   | 0.0243     |\n",
      "|    learning_rate        | 0.00147    |\n",
      "|    loss                 | -0.00242   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00198   |\n",
      "|    std                  | 0.697      |\n",
      "|    value_loss           | 3.79e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0566  |\n",
      "| time/              |          |\n",
      "|    fps             | 929      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004659227 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.0454      |\n",
      "|    learning_rate        | 0.00147     |\n",
      "|    loss                 | -0.00014    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    std                  | 0.669       |\n",
      "|    value_loss           | 4.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 241920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039966744 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.98        |\n",
      "|    explained_variance   | 0.0175       |\n",
      "|    learning_rate        | 0.00147      |\n",
      "|    loss                 | -0.0062      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.643        |\n",
      "|    value_loss           | 5.19e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0558  |\n",
      "| time/              |          |\n",
      "|    fps             | 926      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 261      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031593612 |\n",
      "|    clip_fraction        | 0.00735      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | 0.0302       |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | -0.00238     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.621        |\n",
      "|    value_loss           | 5.04e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004162248 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.081       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.00509    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    std                  | 0.596       |\n",
      "|    value_loss           | 2.24e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0499  |\n",
      "| time/              |          |\n",
      "|    fps             | 924      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 302400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072514997 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.1          |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | -0.00613     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00376     |\n",
      "|    std                  | 0.565        |\n",
      "|    value_loss           | 1.33e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00792     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 322560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094778435 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 0.0547       |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.00568     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.545        |\n",
      "|    value_loss           | 1.09e-05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0465  |\n",
      "| time/              |          |\n",
      "|    fps             | 922      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00922    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012621444 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00773    |\n",
      "|    std                  | 0.506       |\n",
      "|    value_loss           | 4.81e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0131      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 362880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056836586 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0328       |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.00451     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    std                  | 0.482        |\n",
      "|    value_loss           | 2.77e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 919      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 394      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007806539 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.0588      |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.00782    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 1.52e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.017      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006033617 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.0162      |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | -0.00395    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    std                  | 0.432       |\n",
      "|    value_loss           | 2.73e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0364  |\n",
      "| time/              |          |\n",
      "|    fps             | 918      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 438      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102729425 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.0683       |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00787     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00461     |\n",
      "|    std                  | 0.404        |\n",
      "|    value_loss           | 1.35e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0142      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055403626 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.00982      |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00184     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    std                  | 0.389        |\n",
      "|    value_loss           | 1.44e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0349  |\n",
      "| time/              |          |\n",
      "|    fps             | 917      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 483      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00895    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009321483 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.859      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00587    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 1.44e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016417405 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.71       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.014      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.339       |\n",
      "|    value_loss           | 1.56e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0311  |\n",
      "| time/              |          |\n",
      "|    fps             | 917      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008693889 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00946    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    std                  | 0.318       |\n",
      "|    value_loss           | 2.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006829895 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00724    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00303    |\n",
      "|    std                  | 0.303       |\n",
      "|    value_loss           | 7.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0296  |\n",
      "| time/              |          |\n",
      "|    fps             | 916      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 571      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0175      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061939224 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.363       |\n",
      "|    explained_variance   | 0.0554       |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.00343     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    std                  | 0.292        |\n",
      "|    value_loss           | 8.72e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0146      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070590377 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | 0.0845       |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.0053      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    std                  | 0.274        |\n",
      "|    value_loss           | 3.39e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0253  |\n",
      "| time/              |          |\n",
      "|    fps             | 916      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 616      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00951    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007827311 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00911    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.257       |\n",
      "|    value_loss           | 3.26e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00904     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064320825 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0166      |\n",
      "|    explained_variance   | 0.161        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00906     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    std                  | 0.242        |\n",
      "|    value_loss           | 3.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0235  |\n",
      "| time/              |          |\n",
      "|    fps             | 915      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 660      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0116      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072271456 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.105        |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00875     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    std                  | 0.229        |\n",
      "|    value_loss           | 5.09e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0128     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010100942 |\n",
      "|    clip_fraction        | 0.0398      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.237       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.212       |\n",
      "|    value_loss           | 2.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.022   |\n",
      "| time/              |          |\n",
      "|    fps             | 914      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 705      |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00837     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055172355 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.339        |\n",
      "|    explained_variance   | 0.0327       |\n",
      "|    learning_rate        | 0.0014       |\n",
      "|    loss                 | -0.00323     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    std                  | 0.207        |\n",
      "|    value_loss           | 6.65e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0149      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042292206 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.407        |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 0.0014       |\n",
      "|    loss                 | -0.00322     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    std                  | 0.198        |\n",
      "|    value_loss           | 3.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0207  |\n",
      "| time/              |          |\n",
      "|    fps             | 914      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 749      |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00908    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003930483 |\n",
      "|    clip_fraction        | 0.0105      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.497       |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | -0.00286    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 1.17e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004346219 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.583       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00277    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 3.72e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0196  |\n",
      "| time/              |          |\n",
      "|    fps             | 914      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 745920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056918347 |\n",
      "|    clip_fraction        | 0.0206       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.652        |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00272     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.178        |\n",
      "|    value_loss           | 8.72e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022860402 |\n",
      "|    clip_fraction        | 0.00468      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.706        |\n",
      "|    explained_variance   | 0.0845       |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00234     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.173        |\n",
      "|    value_loss           | 8.52e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0191  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 840      |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007293029 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.765       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00148    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 4.77e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003553302 |\n",
      "|    clip_fraction        | 0.0191      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.823       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00605    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 1.53e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0178  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 884      |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 826560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039016982 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.903        |\n",
      "|    explained_variance   | 0.25         |\n",
      "|    learning_rate        | 0.00138      |\n",
      "|    loss                 | -0.00456     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.156        |\n",
      "|    value_loss           | 4.75e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00871     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 846720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050757895 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.978        |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.00138      |\n",
      "|    loss                 | -0.00444     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    std                  | 0.151        |\n",
      "|    value_loss           | 1.7e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0172  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 928      |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005496722 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.05        |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | -0.00508    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 1.42e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 887040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038329232 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.13         |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.00137      |\n",
      "|    loss                 | -0.00487     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.141        |\n",
      "|    value_loss           | 1.97e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0181  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 972      |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00986    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 907200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004071667 |\n",
      "|    clip_fraction        | 0.00817     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.18        |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | -0.00148    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.000874   |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 6.85e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 927360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022461403 |\n",
      "|    clip_fraction        | 0.00892      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.24         |\n",
      "|    explained_variance   | 0.0548       |\n",
      "|    learning_rate        | 0.00136      |\n",
      "|    loss                 | -0.00203     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 0.133        |\n",
      "|    value_loss           | 1.88e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1016     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00941     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 947520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055043274 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.33         |\n",
      "|    explained_variance   | 0.135        |\n",
      "|    learning_rate        | 0.00136      |\n",
      "|    loss                 | -0.00554     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    std                  | 0.127        |\n",
      "|    value_loss           | 3.15e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00941    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003852171 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.42        |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | -0.00383    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 1.52e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0165  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1060     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.017     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 987840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00294937 |\n",
      "|    clip_fraction        | 0.0129     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.49       |\n",
      "|    explained_variance   | 0.328      |\n",
      "|    learning_rate        | 0.00135    |\n",
      "|    loss                 | -0.00205   |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00125   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 9.17e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029649476 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.55         |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00177     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.114        |\n",
      "|    value_loss           | 6.56e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1104     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1028160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050324574 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.63         |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00355     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    std                  | 0.109        |\n",
      "|    value_loss           | 7.98e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0164      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1048320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047809454 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.7          |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.00301     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.105        |\n",
      "|    value_loss           | 3.07e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1148     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0125      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1068480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026518365 |\n",
      "|    clip_fraction        | 0.0093       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.77         |\n",
      "|    explained_variance   | 0.308        |\n",
      "|    learning_rate        | 0.00134      |\n",
      "|    loss                 | -0.00149     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.102        |\n",
      "|    value_loss           | 4.25e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1088640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027047144 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.83         |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.00134      |\n",
      "|    loss                 | -0.000279    |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000963    |\n",
      "|    std                  | 0.0993       |\n",
      "|    value_loss           | 2.03e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1192     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0114     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1108800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002879164 |\n",
      "|    clip_fraction        | 0.0148      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.89        |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.00134     |\n",
      "|    loss                 | -0.0032     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    std                  | 0.0964      |\n",
      "|    value_loss           | 1.69e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1128960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023566647 |\n",
      "|    clip_fraction        | 0.0074       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.95         |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.00133      |\n",
      "|    loss                 | -0.00226     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.0934       |\n",
      "|    value_loss           | 1.72e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0151  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1236     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0176      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1149120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019300026 |\n",
      "|    clip_fraction        | 0.00984      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.01         |\n",
      "|    explained_variance   | 0.2          |\n",
      "|    learning_rate        | 0.00133      |\n",
      "|    loss                 | -0.00269     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.000886    |\n",
      "|    std                  | 0.091        |\n",
      "|    value_loss           | 6.03e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0123      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1169280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026660906 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.06         |\n",
      "|    explained_variance   | 0.0753       |\n",
      "|    learning_rate        | 0.00133      |\n",
      "|    loss                 | -0.00342     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.0894       |\n",
      "|    value_loss           | 1.16e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1280     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1189440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044198167 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.1          |\n",
      "|    explained_variance   | 0.304        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | -0.00114     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000812    |\n",
      "|    std                  | 0.0876       |\n",
      "|    value_loss           | 2.08e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0214      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1209600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035279272 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.14         |\n",
      "|    explained_variance   | 0.295        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | -0.000654    |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000857    |\n",
      "|    std                  | 0.0859       |\n",
      "|    value_loss           | 2.12e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1324     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1229760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020051182 |\n",
      "|    clip_fraction        | 0.00346      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.18         |\n",
      "|    explained_variance   | 0.329        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | -0.00102     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000729    |\n",
      "|    std                  | 0.0832       |\n",
      "|    value_loss           | 1.24e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1249920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021459465 |\n",
      "|    clip_fraction        | 0.00587      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.24         |\n",
      "|    explained_variance   | 0.468        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | -0.00222     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000741    |\n",
      "|    std                  | 0.081        |\n",
      "|    value_loss           | 7.38e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1368     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.014      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004141405 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.29        |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.000134   |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.000785   |\n",
      "|    std                  | 0.0799      |\n",
      "|    value_loss           | 1.4e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1290240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020123846 |\n",
      "|    clip_fraction        | 0.005        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.33         |\n",
      "|    explained_variance   | 0.416        |\n",
      "|    learning_rate        | 0.00131      |\n",
      "|    loss                 | -0.000653    |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    std                  | 0.0773       |\n",
      "|    value_loss           | 4.33e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1413     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003949522 |\n",
      "|    clip_fraction        | 0.0132      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.4         |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    std                  | 0.0746      |\n",
      "|    value_loss           | 2.81e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0111      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1330560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038531786 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.45         |\n",
      "|    explained_variance   | 0.296        |\n",
      "|    learning_rate        | 0.0013       |\n",
      "|    loss                 | -0.00255     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.0732       |\n",
      "|    value_loss           | 1.36e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1457     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1350720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017029541 |\n",
      "|    clip_fraction        | 0.0028       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.49         |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.0013       |\n",
      "|    loss                 | -0.000112    |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000393    |\n",
      "|    std                  | 0.072        |\n",
      "|    value_loss           | 3.78e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1370880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019489969 |\n",
      "|    clip_fraction        | 0.00663      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.54         |\n",
      "|    explained_variance   | 0.373        |\n",
      "|    learning_rate        | 0.0013       |\n",
      "|    loss                 | -0.00128     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000944    |\n",
      "|    std                  | 0.0699       |\n",
      "|    value_loss           | 4.24e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1501     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002318874 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.59        |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | -0.00131    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    std                  | 0.0689      |\n",
      "|    value_loss           | 4.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1411200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014245646 |\n",
      "|    clip_fraction        | 0.00885      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.62         |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.000467    |\n",
      "|    std                  | 0.0677       |\n",
      "|    value_loss           | 1.73e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1545     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002880211 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.65        |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | -0.00147    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.000732   |\n",
      "|    std                  | 0.0665      |\n",
      "|    value_loss           | 2.8e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00969     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1451520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017530413 |\n",
      "|    clip_fraction        | 0.00861      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.69         |\n",
      "|    explained_variance   | 0.369        |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | -0.00217     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000744    |\n",
      "|    std                  | 0.065        |\n",
      "|    value_loss           | 1.19e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1589     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1471680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035975473 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.73         |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.00128      |\n",
      "|    loss                 | 0.000251     |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.000577    |\n",
      "|    std                  | 0.0644       |\n",
      "|    value_loss           | 9.9e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0123      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1491840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023752847 |\n",
      "|    clip_fraction        | 0.00564      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.75         |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 0.00128      |\n",
      "|    loss                 | -0.000878    |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000214    |\n",
      "|    std                  | 0.064        |\n",
      "|    value_loss           | 1.03e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1633     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004976317 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.78        |\n",
      "|    explained_variance   | 0.0919      |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | -0.00328    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    std                  | 0.0628      |\n",
      "|    value_loss           | 1.38e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00927     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1532160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030301593 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.82         |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | -0.00188     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    std                  | 0.0616       |\n",
      "|    value_loss           | 3.97e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1677     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00988     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1552320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028661825 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.86         |\n",
      "|    explained_variance   | 0.283        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | -0.000942    |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.000631    |\n",
      "|    std                  | 0.0606       |\n",
      "|    value_loss           | 1e-07        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004002092 |\n",
      "|    clip_fraction        | 0.0144      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.88        |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | -0.000266   |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.000483   |\n",
      "|    std                  | 0.0602      |\n",
      "|    value_loss           | 5.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1722     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0167     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004159064 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.91        |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | -0.00186    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00071    |\n",
      "|    std                  | 0.0592      |\n",
      "|    value_loss           | 9e-08       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1612800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021973765 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.95         |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 0.00126      |\n",
      "|    loss                 | -0.00159     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.000762    |\n",
      "|    std                  | 0.0576       |\n",
      "|    value_loss           | 1.08e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1766     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00754    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001770593 |\n",
      "|    clip_fraction        | 0.00566     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3           |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | -0.000627   |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00039    |\n",
      "|    std                  | 0.0562      |\n",
      "|    value_loss           | 1.06e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1653120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1653120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045955074 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.04         |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.00126      |\n",
      "|    loss                 | -0.00316     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.000869    |\n",
      "|    std                  | 0.0555       |\n",
      "|    value_loss           | 1.42e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1810     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002333635 |\n",
      "|    clip_fraction        | 0.00625     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.07        |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.000408   |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.000541   |\n",
      "|    std                  | 0.0544      |\n",
      "|    value_loss           | 9.65e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00864    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005178207 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.1         |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.000138   |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    std                  | 0.0539      |\n",
      "|    value_loss           | 1.39e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1854     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0124      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1713600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032818848 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.12         |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 0.00125      |\n",
      "|    loss                 | -0.000976    |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00076     |\n",
      "|    std                  | 0.0534       |\n",
      "|    value_loss           | 0.000147     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0125      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1733760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033134804 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.15         |\n",
      "|    explained_variance   | -0.375       |\n",
      "|    learning_rate        | 0.00124      |\n",
      "|    loss                 | -0.00243     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00082     |\n",
      "|    std                  | 0.0528       |\n",
      "|    value_loss           | 4.39e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1898     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1753920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039492445 |\n",
      "|    clip_fraction        | 0.00717      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.16         |\n",
      "|    explained_variance   | -0.529       |\n",
      "|    learning_rate        | 0.00124      |\n",
      "|    loss                 | 0.00115      |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.000382    |\n",
      "|    std                  | 0.0523       |\n",
      "|    value_loss           | 9.05e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0169      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1774080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039032372 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.18         |\n",
      "|    explained_variance   | -0.179       |\n",
      "|    learning_rate        | 0.00124      |\n",
      "|    loss                 | -0.00139     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    std                  | 0.0519       |\n",
      "|    value_loss           | 1.78e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1943     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00979     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1794240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040802434 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.2          |\n",
      "|    explained_variance   | -0.114       |\n",
      "|    learning_rate        | 0.00123      |\n",
      "|    loss                 | -0.00305     |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.000939    |\n",
      "|    std                  | 0.0513       |\n",
      "|    value_loss           | 1.68e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1814400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036176683 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.22         |\n",
      "|    explained_variance   | -0.159       |\n",
      "|    learning_rate        | 0.00123      |\n",
      "|    loss                 | -0.0014      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000598    |\n",
      "|    std                  | 0.0509       |\n",
      "|    value_loss           | 1.58e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1987     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00993    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003215908 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.24        |\n",
      "|    explained_variance   | -0.173      |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | -0.00103    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00057    |\n",
      "|    std                  | 0.0506      |\n",
      "|    value_loss           | 1.17e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00898     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1854720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025861661 |\n",
      "|    clip_fraction        | 0.00692      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.26         |\n",
      "|    explained_variance   | -0.197       |\n",
      "|    learning_rate        | 0.00122      |\n",
      "|    loss                 | -0.00177     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000419    |\n",
      "|    std                  | 0.0501       |\n",
      "|    value_loss           | 8.98e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 2031     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.014      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002949345 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.28        |\n",
      "|    explained_variance   | -0.189      |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -0.00298    |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.000749   |\n",
      "|    std                  | 0.0499      |\n",
      "|    value_loss           | 7.75e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00943    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002656383 |\n",
      "|    clip_fraction        | 0.00673     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.29        |\n",
      "|    explained_variance   | -0.237      |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -0.000271   |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.000217   |\n",
      "|    std                  | 0.0497      |\n",
      "|    value_loss           | 5.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 2075     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.00817      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 1915200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087465113 |\n",
      "|    clip_fraction        | 0.00184       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 3.3           |\n",
      "|    explained_variance   | -0.0726       |\n",
      "|    learning_rate        | 0.00122       |\n",
      "|    loss                 | -0.000681     |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | -0.000196     |\n",
      "|    std                  | 0.0489        |\n",
      "|    value_loss           | 9.49e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0186      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1935360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005500972 |\n",
      "|    clip_fraction        | 0.00196      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.33         |\n",
      "|    explained_variance   | -0.187       |\n",
      "|    learning_rate        | 0.00121      |\n",
      "|    loss                 | -0.000696    |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -9e-05       |\n",
      "|    std                  | 0.0486       |\n",
      "|    value_loss           | 5.67e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 2119     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003567216 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.33        |\n",
      "|    explained_variance   | -0.0456     |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | 0.00109     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.000504   |\n",
      "|    std                  | 0.0486      |\n",
      "|    value_loss           | 1.47e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00855     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1975680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036553948 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.35         |\n",
      "|    explained_variance   | -0.134       |\n",
      "|    learning_rate        | 0.00121      |\n",
      "|    loss                 | -0.00134     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000882    |\n",
      "|    std                  | 0.048        |\n",
      "|    value_loss           | 4.56e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 2163     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1995840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023326976 |\n",
      "|    clip_fraction        | 0.00266      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.36         |\n",
      "|    explained_variance   | -0.0579      |\n",
      "|    learning_rate        | 0.0012       |\n",
      "|    loss                 | -0.000598    |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.000142    |\n",
      "|    std                  | 0.0478       |\n",
      "|    value_loss           | 7.51e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2016000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016152286 |\n",
      "|    clip_fraction        | 0.00775      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.38         |\n",
      "|    explained_variance   | -0.0537      |\n",
      "|    learning_rate        | 0.0012       |\n",
      "|    loss                 | 0.000856     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.000349    |\n",
      "|    std                  | 0.0474       |\n",
      "|    value_loss           | 7.61e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 2207     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004184072 |\n",
      "|    clip_fraction        | 0.0206      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.4         |\n",
      "|    explained_variance   | -0.0526     |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | -0.000632   |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    std                  | 0.047       |\n",
      "|    value_loss           | 5.68e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00941     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2056320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024173765 |\n",
      "|    clip_fraction        | 0.00305      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.41         |\n",
      "|    explained_variance   | -0.0411      |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 8.41e-05     |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.000186    |\n",
      "|    std                  | 0.0463       |\n",
      "|    value_loss           | 5.6e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 2252     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00892    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003514524 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.43        |\n",
      "|    explained_variance   | -0.0439     |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.00156    |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.000992   |\n",
      "|    std                  | 0.0461      |\n",
      "|    value_loss           | 4.22e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2096640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042880094 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.44         |\n",
      "|    explained_variance   | -0.00885     |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00165     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000496    |\n",
      "|    std                  | 0.0457       |\n",
      "|    value_loss           | 8.45e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 2295     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00993     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2116800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027936392 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.46         |\n",
      "|    explained_variance   | -0.00895     |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.000835    |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.000664    |\n",
      "|    std                  | 0.0453       |\n",
      "|    value_loss           | 5.39e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002403031 |\n",
      "|    clip_fraction        | 0.00548     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.48        |\n",
      "|    explained_variance   | 0.042       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | -0.00111    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.000336   |\n",
      "|    std                  | 0.0449      |\n",
      "|    value_loss           | 1.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 2339     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028961906 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.5          |\n",
      "|    explained_variance   | 0.0122       |\n",
      "|    learning_rate        | 0.00118      |\n",
      "|    loss                 | 0.000331     |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.000537    |\n",
      "|    std                  | 0.0446       |\n",
      "|    value_loss           | 5.76e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002460266 |\n",
      "|    clip_fraction        | 0.00693     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.53        |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | -0.000169   |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.000209   |\n",
      "|    std                  | 0.0438      |\n",
      "|    value_loss           | 1.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 2383     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0198      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2197440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026233979 |\n",
      "|    clip_fraction        | 0.00763      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.56         |\n",
      "|    explained_variance   | 0.0534       |\n",
      "|    learning_rate        | 0.00117      |\n",
      "|    loss                 | -0.000111    |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    std                  | 0.0431       |\n",
      "|    value_loss           | 1.04e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00966     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2217600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020017587 |\n",
      "|    clip_fraction        | 0.00912      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.59         |\n",
      "|    explained_variance   | 0.0352       |\n",
      "|    learning_rate        | 0.00117      |\n",
      "|    loss                 | -0.00124     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.000379    |\n",
      "|    std                  | 0.0427       |\n",
      "|    value_loss           | 4.36e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 2427     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00906    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003081676 |\n",
      "|    clip_fraction        | 0.0037      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.61        |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.00197    |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -9.43e-05   |\n",
      "|    std                  | 0.0424      |\n",
      "|    value_loss           | 6.25e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001535871 |\n",
      "|    clip_fraction        | 0.00375     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.63        |\n",
      "|    explained_variance   | 0.05        |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | -0.00115    |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.000341   |\n",
      "|    std                  | 0.0415      |\n",
      "|    value_loss           | 4.81e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 2471     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2278080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022934151 |\n",
      "|    clip_fraction        | 0.00777      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.65         |\n",
      "|    explained_variance   | 0.0925       |\n",
      "|    learning_rate        | 0.00116      |\n",
      "|    loss                 | -0.0013      |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00021     |\n",
      "|    std                  | 0.0415       |\n",
      "|    value_loss           | 9e-08        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021347587 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.66         |\n",
      "|    explained_variance   | 0.0916       |\n",
      "|    learning_rate        | 0.00116      |\n",
      "|    loss                 | -0.000838    |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | -0.00054     |\n",
      "|    std                  | 0.0412       |\n",
      "|    value_loss           | 3.4e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 2515     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2318400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017360803 |\n",
      "|    clip_fraction        | 0.00125      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.68         |\n",
      "|    explained_variance   | 0.081        |\n",
      "|    learning_rate        | 0.00116      |\n",
      "|    loss                 | 0.000477     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | 5.06e-05     |\n",
      "|    std                  | 0.0408       |\n",
      "|    value_loss           | 8.78e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00882    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003876545 |\n",
      "|    clip_fraction        | 0.0153      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.69        |\n",
      "|    explained_variance   | 0.0808      |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.00195    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.000357   |\n",
      "|    std                  | 0.0406      |\n",
      "|    value_loss           | 8.99e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 2559     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2358720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032249708 |\n",
      "|    clip_fraction        | 0.00972      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.7          |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.00115      |\n",
      "|    loss                 | -0.0014      |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    std                  | 0.0403       |\n",
      "|    value_loss           | 8.7e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0127      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2378880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038901786 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.72         |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.00115      |\n",
      "|    loss                 | 0.000409     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.000498    |\n",
      "|    std                  | 0.04         |\n",
      "|    value_loss           | 4.43e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 2603     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00963     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2399040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031281759 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.73         |\n",
      "|    explained_variance   | 0.069        |\n",
      "|    learning_rate        | 0.00114      |\n",
      "|    loss                 | 0.000229     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -2.68e-05    |\n",
      "|    std                  | 0.0399       |\n",
      "|    value_loss           | 1.29e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0167     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004430685 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.74        |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.00092     |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.000486   |\n",
      "|    std                  | 0.0399      |\n",
      "|    value_loss           | 7.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 2648     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2439360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042044832 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.75         |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 0.00114      |\n",
      "|    loss                 | -0.00192     |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.000957    |\n",
      "|    std                  | 0.0398       |\n",
      "|    value_loss           | 4.66e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0149      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2459520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015016923 |\n",
      "|    clip_fraction        | 0.00411      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.76         |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.00113      |\n",
      "|    loss                 | -0.00133     |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.000202    |\n",
      "|    std                  | 0.0392       |\n",
      "|    value_loss           | 3.24e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 2692     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0159      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2479680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037588193 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.79         |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.00113      |\n",
      "|    loss                 | -0.00276     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.00068     |\n",
      "|    std                  | 0.0385       |\n",
      "|    value_loss           | 8.54e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2499840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015631237 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.81         |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.00113      |\n",
      "|    loss                 | -2.12e-05    |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | -5.32e-05    |\n",
      "|    std                  | 0.0381       |\n",
      "|    value_loss           | 6.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 2736     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010099593 |\n",
      "|    clip_fraction        | 0.00533      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.81         |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.00113      |\n",
      "|    loss                 | -0.000433    |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -1.6e-05     |\n",
      "|    std                  | 0.0383       |\n",
      "|    value_loss           | 1.25e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005086976 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.81        |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | -0.00422    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.00034    |\n",
      "|    std                  | 0.0382      |\n",
      "|    value_loss           | 3.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 2780     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2560320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033164553 |\n",
      "|    clip_fraction        | 0.00858      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.83         |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.00112      |\n",
      "|    loss                 | 0.00121      |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.000219    |\n",
      "|    std                  | 0.038        |\n",
      "|    value_loss           | 8.95e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2580480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019700027 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.85         |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 0.00112      |\n",
      "|    loss                 | -0.000706    |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 3.54e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 2824     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0122      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2600640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027525902 |\n",
      "|    clip_fraction        | 0.00594      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.87         |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.00111      |\n",
      "|    loss                 | 0.000725     |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00013     |\n",
      "|    std                  | 0.0372       |\n",
      "|    value_loss           | 1.24e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2620800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030390173 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.88         |\n",
      "|    explained_variance   | 0.177        |\n",
      "|    learning_rate        | 0.00111      |\n",
      "|    loss                 | 0.00197      |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.000391    |\n",
      "|    std                  | 0.0369       |\n",
      "|    value_loss           | 9.95e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 2868     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2640960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043451283 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.9          |\n",
      "|    explained_variance   | 0.193        |\n",
      "|    learning_rate        | 0.00111      |\n",
      "|    loss                 | -0.00105     |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.000697    |\n",
      "|    std                  | 0.0367       |\n",
      "|    value_loss           | 5.3e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00743     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030289749 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.91         |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.0011       |\n",
      "|    loss                 | -0.00236     |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.000642    |\n",
      "|    std                  | 0.0362       |\n",
      "|    value_loss           | 6.54e-08     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2912     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2681280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004979132 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.94        |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | -0.000733   |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    std                  | 0.0359      |\n",
      "|    value_loss           | 1.15e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00943     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2701440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011573745 |\n",
      "|    clip_fraction        | 0.00506      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.96         |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.0011       |\n",
      "|    loss                 | -0.000948    |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | -0.000313    |\n",
      "|    std                  | 0.0355       |\n",
      "|    value_loss           | 5.78e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2957     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0133      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2721600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029455633 |\n",
      "|    clip_fraction        | 0.00666      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4            |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | -0.000335    |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.000406    |\n",
      "|    std                  | 0.0348       |\n",
      "|    value_loss           | 5.02e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2741760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031716896 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.04         |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | -0.000814    |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.000696    |\n",
      "|    std                  | 0.034        |\n",
      "|    value_loss           | 3.98e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 3001     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2761920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019640673 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.06         |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | 0.000406     |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.000265    |\n",
      "|    std                  | 0.0338       |\n",
      "|    value_loss           | 6.49e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2782080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018420268 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.07         |\n",
      "|    explained_variance   | 0.21         |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | 0.00119      |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -0.000562    |\n",
      "|    std                  | 0.0335       |\n",
      "|    value_loss           | 3.32e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 3046     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2802240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004313992 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.1         |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00108     |\n",
      "|    loss                 | -0.00194    |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.000735   |\n",
      "|    std                  | 0.0331      |\n",
      "|    value_loss           | 3.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003936135 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.11        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00108     |\n",
      "|    loss                 | -0.000406   |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.000684   |\n",
      "|    std                  | 0.0329      |\n",
      "|    value_loss           | 4e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 3090     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2842560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037802162 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.13         |\n",
      "|    explained_variance   | 0.177        |\n",
      "|    learning_rate        | 0.00108      |\n",
      "|    loss                 | -0.000537    |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.000732    |\n",
      "|    std                  | 0.0327       |\n",
      "|    value_loss           | 7.4e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003496984 |\n",
      "|    clip_fraction        | 0.02        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.14        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.00107     |\n",
      "|    loss                 | 7.01e-05    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.000612   |\n",
      "|    std                  | 0.0325      |\n",
      "|    value_loss           | 4.58e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 3134     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00951     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025504653 |\n",
      "|    clip_fraction        | 0.00747      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.15         |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.00107      |\n",
      "|    loss                 | -0.000459    |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.000261    |\n",
      "|    std                  | 0.0326       |\n",
      "|    value_loss           | 9.7e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2903040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002001878 |\n",
      "|    clip_fraction        | 0.00665     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.15        |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00107     |\n",
      "|    loss                 | -0.00109    |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00059    |\n",
      "|    std                  | 0.0325      |\n",
      "|    value_loss           | 2.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 3179     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0126      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2923200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035750326 |\n",
      "|    clip_fraction        | 0.00719      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.17         |\n",
      "|    explained_variance   | 0.234        |\n",
      "|    learning_rate        | 0.00106      |\n",
      "|    loss                 | 0.002        |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.00021     |\n",
      "|    std                  | 0.0323       |\n",
      "|    value_loss           | 1.06e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023126872 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.19         |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.00106      |\n",
      "|    loss                 | 9.48e-05     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -0.000325    |\n",
      "|    std                  | 0.0323       |\n",
      "|    value_loss           | 6.22e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 3223     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0112      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023594801 |\n",
      "|    clip_fraction        | 0.00281      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.18         |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.00106      |\n",
      "|    loss                 | 0.000706     |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -4.82e-05    |\n",
      "|    std                  | 0.0322       |\n",
      "|    value_loss           | 3.71e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.013       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012072814 |\n",
      "|    clip_fraction        | 0.00368      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.2          |\n",
      "|    explained_variance   | 0.253        |\n",
      "|    learning_rate        | 0.00106      |\n",
      "|    loss                 | -0.000784    |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.000124    |\n",
      "|    std                  | 0.032        |\n",
      "|    value_loss           | 4.31e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 3268     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3003840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027994867 |\n",
      "|    clip_fraction        | 0.00776      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.21         |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.00105      |\n",
      "|    loss                 | 7.62e-05     |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.000191    |\n",
      "|    std                  | 0.0317       |\n",
      "|    value_loss           | 1.53e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0147     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001969019 |\n",
      "|    clip_fraction        | 0.00745     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.23        |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | -0.00106    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.000256   |\n",
      "|    std                  | 0.0315      |\n",
      "|    value_loss           | 4.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 3312     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3044160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00964     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3044160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034580494 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.24         |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.00105      |\n",
      "|    loss                 | -0.00244     |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.000425    |\n",
      "|    std                  | 0.0315       |\n",
      "|    value_loss           | 5.85e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3064320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00821     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3064320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024955296 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.25         |\n",
      "|    explained_variance   | 0.261        |\n",
      "|    learning_rate        | 0.00104      |\n",
      "|    loss                 | -0.000168    |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.00047     |\n",
      "|    std                  | 0.0313       |\n",
      "|    value_loss           | 5.79e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 3356     |\n",
      "|    total_timesteps | 3064320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3084480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00921    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3084480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002915744 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.25        |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | -0.00319    |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.000441   |\n",
      "|    std                  | 0.0313      |\n",
      "|    value_loss           | 2.87e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3104640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3104640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029852192 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.27         |\n",
      "|    explained_variance   | 0.182        |\n",
      "|    learning_rate        | 0.00104      |\n",
      "|    loss                 | -0.00176     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -0.000612    |\n",
      "|    std                  | 0.031        |\n",
      "|    value_loss           | 4.58e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 3400     |\n",
      "|    total_timesteps | 3104640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3124800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0126      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3124800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019189824 |\n",
      "|    clip_fraction        | 0.00882      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.28         |\n",
      "|    explained_variance   | 0.167        |\n",
      "|    learning_rate        | 0.00103      |\n",
      "|    loss                 | -0.000804    |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.000726    |\n",
      "|    std                  | 0.0307       |\n",
      "|    value_loss           | 1.09e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3144960, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0183      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3144960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018354079 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.32         |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 0.00103      |\n",
      "|    loss                 | -0.000677    |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.000476    |\n",
      "|    std                  | 0.0301       |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 3444     |\n",
      "|    total_timesteps | 3144960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3165120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3165120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037682909 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.35         |\n",
      "|    explained_variance   | 0.258        |\n",
      "|    learning_rate        | 0.00103      |\n",
      "|    loss                 | 0.000235     |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.000951    |\n",
      "|    std                  | 0.0299       |\n",
      "|    value_loss           | 6.83e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3185280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3185280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010707121 |\n",
      "|    clip_fraction        | 0.00532      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.38         |\n",
      "|    explained_variance   | 0.248        |\n",
      "|    learning_rate        | 0.00103      |\n",
      "|    loss                 | -0.000833    |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -0.000289    |\n",
      "|    std                  | 0.0294       |\n",
      "|    value_loss           | 4.43e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 3488     |\n",
      "|    total_timesteps | 3185280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3205440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00994     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3205440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032638523 |\n",
      "|    clip_fraction        | 0.00622      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.4          |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.00102      |\n",
      "|    loss                 | 0.00111      |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.000209    |\n",
      "|    std                  | 0.0293       |\n",
      "|    value_loss           | 7.24e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3225600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0125      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3225600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022131477 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.41         |\n",
      "|    explained_variance   | 0.173        |\n",
      "|    learning_rate        | 0.00102      |\n",
      "|    loss                 | -0.00148     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.000281    |\n",
      "|    std                  | 0.029        |\n",
      "|    value_loss           | 4.21e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 3532     |\n",
      "|    total_timesteps | 3225600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3245760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3245760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003220592 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.44        |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | -0.00145    |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.000946   |\n",
      "|    std                  | 0.0286      |\n",
      "|    value_loss           | 5.68e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3265920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0112      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3265920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035641375 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.22         |\n",
      "|    learning_rate        | 0.00101      |\n",
      "|    loss                 | -0.000515    |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    std                  | 0.0285       |\n",
      "|    value_loss           | 6.65e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 3576     |\n",
      "|    total_timesteps | 3265920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3286080, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0134      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3286080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016943947 |\n",
      "|    clip_fraction        | 0.00537      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.00101      |\n",
      "|    loss                 | -0.000723    |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    std                  | 0.0283       |\n",
      "|    value_loss           | 7.78e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3306240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00798    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3306240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006172736 |\n",
      "|    clip_fraction        | 0.0119      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.47        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.000875    |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.000122   |\n",
      "|    std                  | 0.0285      |\n",
      "|    value_loss           | 9.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 3620     |\n",
      "|    total_timesteps | 3306240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3326400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00907     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3326400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027260787 |\n",
      "|    clip_fraction        | 0.00853      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.212        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | -0.000564    |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.000486    |\n",
      "|    std                  | 0.0282       |\n",
      "|    value_loss           | 7.07e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3346560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0141      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3346560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028642472 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.5          |\n",
      "|    explained_variance   | 0.304        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | -0.000253    |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.000817    |\n",
      "|    std                  | 0.0279       |\n",
      "|    value_loss           | 5.65e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 3664     |\n",
      "|    total_timesteps | 3346560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3366720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.013       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3366720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021810734 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.51         |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.000998     |\n",
      "|    loss                 | -0.000646    |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.000278    |\n",
      "|    std                  | 0.0277       |\n",
      "|    value_loss           | 4.3e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3386880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3386880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032884276 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.52         |\n",
      "|    explained_variance   | 0.263        |\n",
      "|    learning_rate        | 0.000995     |\n",
      "|    loss                 | -0.00066     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -0.00065     |\n",
      "|    std                  | 0.0275       |\n",
      "|    value_loss           | 7.38e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 3708     |\n",
      "|    total_timesteps | 3386880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3407040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0111      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3407040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026653402 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.53         |\n",
      "|    explained_variance   | 0.274        |\n",
      "|    learning_rate        | 0.000992     |\n",
      "|    loss                 | -0.000578    |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.000912    |\n",
      "|    std                  | 0.0272       |\n",
      "|    value_loss           | 4.87e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3427200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3427200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003609432 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.55        |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.000989    |\n",
      "|    loss                 | -0.00198    |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    std                  | 0.0271      |\n",
      "|    value_loss           | 1.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 3752     |\n",
      "|    total_timesteps | 3427200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3447360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3447360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043873433 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.56         |\n",
      "|    explained_variance   | 0.252        |\n",
      "|    learning_rate        | 0.000986     |\n",
      "|    loss                 | -0.000854    |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.000462    |\n",
      "|    std                  | 0.0269       |\n",
      "|    value_loss           | 9.49e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3467520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0111      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3467520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020328416 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.57         |\n",
      "|    explained_variance   | 0.292        |\n",
      "|    learning_rate        | 0.000983     |\n",
      "|    loss                 | -0.00106     |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.000293    |\n",
      "|    std                  | 0.0268       |\n",
      "|    value_loss           | 4.07e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 3796     |\n",
      "|    total_timesteps | 3467520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3487680, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0169      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3487680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022310393 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.59         |\n",
      "|    explained_variance   | 0.282        |\n",
      "|    learning_rate        | 0.00098      |\n",
      "|    loss                 | -0.000883    |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000698    |\n",
      "|    std                  | 0.0266       |\n",
      "|    value_loss           | 6.6e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3507840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00868     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3507840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011604861 |\n",
      "|    clip_fraction        | 0.00857      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.6          |\n",
      "|    explained_variance   | 0.24         |\n",
      "|    learning_rate        | 0.000977     |\n",
      "|    loss                 | 0.000255     |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.000199    |\n",
      "|    std                  | 0.0263       |\n",
      "|    value_loss           | 8.45e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 3840     |\n",
      "|    total_timesteps | 3507840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3528000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00986     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3528000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013594148 |\n",
      "|    clip_fraction        | 0.00872      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.61         |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.000974     |\n",
      "|    loss                 | -0.000373    |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.000229    |\n",
      "|    std                  | 0.0262       |\n",
      "|    value_loss           | 1.85e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3548160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3548160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013026396 |\n",
      "|    clip_fraction        | 0.00417      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.61         |\n",
      "|    explained_variance   | 0.296        |\n",
      "|    learning_rate        | 0.000971     |\n",
      "|    loss                 | 0.00114      |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | 1.47e-06     |\n",
      "|    std                  | 0.0261       |\n",
      "|    value_loss           | 2.61e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 3884     |\n",
      "|    total_timesteps | 3548160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3568320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3568320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046932455 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.61         |\n",
      "|    explained_variance   | 0.278        |\n",
      "|    learning_rate        | 0.000968     |\n",
      "|    loss                 | -0.00196     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.0259       |\n",
      "|    value_loss           | 2.29e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3588480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0103    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3588480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00215043 |\n",
      "|    clip_fraction        | 0.0194     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.63       |\n",
      "|    explained_variance   | 0.279      |\n",
      "|    learning_rate        | 0.000965   |\n",
      "|    loss                 | 0.00149    |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | -0.00055   |\n",
      "|    std                  | 0.0258     |\n",
      "|    value_loss           | 1.26e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 3928     |\n",
      "|    total_timesteps | 3588480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3608640, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3608640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002754475 |\n",
      "|    clip_fraction        | 0.0164      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.63        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.000962    |\n",
      "|    loss                 | -0.00063    |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.00042    |\n",
      "|    std                  | 0.0258      |\n",
      "|    value_loss           | 8.98e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3628800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0115     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3628800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003665336 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.64        |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.000959    |\n",
      "|    loss                 | -0.000385   |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.00077    |\n",
      "|    std                  | 0.0256      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0107  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 3973     |\n",
      "|    total_timesteps | 3628800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3648960, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0153     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3648960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003097192 |\n",
      "|    clip_fraction        | 0.00758     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.67        |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.000956    |\n",
      "|    loss                 | -0.000196   |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0003     |\n",
      "|    std                  | 0.0253      |\n",
      "|    value_loss           | 6.79e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3669120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0141      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3669120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037466367 |\n",
      "|    clip_fraction        | 0.0066       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.69         |\n",
      "|    explained_variance   | 0.25         |\n",
      "|    learning_rate        | 0.000953     |\n",
      "|    loss                 | 0.00066      |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | -0.000123    |\n",
      "|    std                  | 0.0251       |\n",
      "|    value_loss           | 8.75e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 4017     |\n",
      "|    total_timesteps | 3669120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3689280, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3689280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001488105 |\n",
      "|    clip_fraction        | 0.00289     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.69        |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.00095     |\n",
      "|    loss                 | -0.000266   |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -3.37e-05   |\n",
      "|    std                  | 0.0251      |\n",
      "|    value_loss           | 8.03e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3709440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00991     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3709440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039183083 |\n",
      "|    clip_fraction        | 0.00982      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.71         |\n",
      "|    explained_variance   | 0.326        |\n",
      "|    learning_rate        | 0.000947     |\n",
      "|    loss                 | -0.000134    |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.000377    |\n",
      "|    std                  | 0.0248       |\n",
      "|    value_loss           | 1.55e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 4061     |\n",
      "|    total_timesteps | 3709440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3729600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3729600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011967121 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.74         |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.000944     |\n",
      "|    loss                 | -0.000434    |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.000379    |\n",
      "|    std                  | 0.0244       |\n",
      "|    value_loss           | 2.63e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3749760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3749760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002849162 |\n",
      "|    clip_fraction        | 0.0173      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.76        |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.000941    |\n",
      "|    loss                 | 0.000448    |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.000468   |\n",
      "|    std                  | 0.0242      |\n",
      "|    value_loss           | 1.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 4105     |\n",
      "|    total_timesteps | 3749760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3769920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3769920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029049504 |\n",
      "|    clip_fraction        | 0.00762      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.198        |\n",
      "|    learning_rate        | 0.000938     |\n",
      "|    loss                 | -0.000113    |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -2.23e-06    |\n",
      "|    std                  | 0.0243       |\n",
      "|    value_loss           | 2.08e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3790080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0111      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3790080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040455423 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.291        |\n",
      "|    learning_rate        | 0.000935     |\n",
      "|    loss                 | -0.00199     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    std                  | 0.0242       |\n",
      "|    value_loss           | 5.8e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 4149     |\n",
      "|    total_timesteps | 3790080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3810240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024770165 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.78         |\n",
      "|    explained_variance   | 0.22         |\n",
      "|    learning_rate        | 0.000931     |\n",
      "|    loss                 | -0.000866    |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.000903    |\n",
      "|    std                  | 0.0243       |\n",
      "|    value_loss           | 4.47e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3830400, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0174      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3830400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025560958 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.259        |\n",
      "|    learning_rate        | 0.000928     |\n",
      "|    loss                 | -0.000379    |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.000451    |\n",
      "|    std                  | 0.0242       |\n",
      "|    value_loss           | 4.97e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 4193     |\n",
      "|    total_timesteps | 3830400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00722     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3850560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023063438 |\n",
      "|    clip_fraction        | 0.0057       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.293        |\n",
      "|    learning_rate        | 0.000925     |\n",
      "|    loss                 | -0.00103     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.000212    |\n",
      "|    std                  | 0.024        |\n",
      "|    value_loss           | 3.33e-08     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3870720, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0131      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3870720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011860689 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.8          |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.000922     |\n",
      "|    loss                 | -0.000675    |\n",
      "|    n_updates            | 1910         |\n",
      "|    policy_gradient_loss | -0.000291    |\n",
      "|    std                  | 0.024        |\n",
      "|    value_loss           | 1.03e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 4237     |\n",
      "|    total_timesteps | 3870720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3890880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019750118 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.226        |\n",
      "|    learning_rate        | 0.000919     |\n",
      "|    loss                 | -0.00224     |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.000787    |\n",
      "|    std                  | 0.0238       |\n",
      "|    value_loss           | 8.81e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3911040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3911040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001260528 |\n",
      "|    clip_fraction        | 0.00394     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.83        |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.000916    |\n",
      "|    loss                 | 0.000453    |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -2.21e-05   |\n",
      "|    std                  | 0.0237      |\n",
      "|    value_loss           | 2.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0108  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 4281     |\n",
      "|    total_timesteps | 3911040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3931200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0101     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3931200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003890397 |\n",
      "|    clip_fraction        | 0.00788     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.83        |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.000913    |\n",
      "|    loss                 | 0.000933    |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -9.8e-05    |\n",
      "|    std                  | 0.0235      |\n",
      "|    value_loss           | 5.28e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3951360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00986     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3951360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033616028 |\n",
      "|    clip_fraction        | 0.006        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.84         |\n",
      "|    explained_variance   | 0.268        |\n",
      "|    learning_rate        | 0.00091      |\n",
      "|    loss                 | -0.000759    |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00024     |\n",
      "|    std                  | 0.0235       |\n",
      "|    value_loss           | 4.76e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 4325     |\n",
      "|    total_timesteps | 3951360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3971520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0115     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3971520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001771407 |\n",
      "|    clip_fraction        | 0.00597     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.85        |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.000907    |\n",
      "|    loss                 | -0.00137    |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.000264   |\n",
      "|    std                  | 0.0231      |\n",
      "|    value_loss           | 4.4e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3991680, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0229      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3991680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015608384 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.89         |\n",
      "|    explained_variance   | 0.286        |\n",
      "|    learning_rate        | 0.000904     |\n",
      "|    loss                 | -0.00181     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.00059     |\n",
      "|    std                  | 0.0225       |\n",
      "|    value_loss           | 4.67e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 4369     |\n",
      "|    total_timesteps | 3991680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4011840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4011840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026194241 |\n",
      "|    clip_fraction        | 0.00507      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.91         |\n",
      "|    explained_variance   | 0.27         |\n",
      "|    learning_rate        | 0.000901     |\n",
      "|    loss                 | -0.000601    |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | 4.4e-05      |\n",
      "|    std                  | 0.0225       |\n",
      "|    value_loss           | 5.05e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4032000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4032000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019995826 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.92         |\n",
      "|    explained_variance   | 0.294        |\n",
      "|    learning_rate        | 0.000898     |\n",
      "|    loss                 | -0.000681    |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.000525    |\n",
      "|    std                  | 0.0224       |\n",
      "|    value_loss           | 6.84e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 4413     |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4052160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4052160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010480592 |\n",
      "|    clip_fraction        | 0.00254      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.91         |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.000895     |\n",
      "|    loss                 | -0.000953    |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -8.6e-06     |\n",
      "|    std                  | 0.0226       |\n",
      "|    value_loss           | 8.11e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4072320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4072320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022368405 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.9          |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.000892     |\n",
      "|    loss                 | 4.99e-05     |\n",
      "|    n_updates            | 2010         |\n",
      "|    policy_gradient_loss | -0.00037     |\n",
      "|    std                  | 0.0226       |\n",
      "|    value_loss           | 9.68e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 4457     |\n",
      "|    total_timesteps | 4072320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4092480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4092480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025050214 |\n",
      "|    clip_fraction        | 0.0182       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.91         |\n",
      "|    explained_variance   | 0.294        |\n",
      "|    learning_rate        | 0.000889     |\n",
      "|    loss                 | -0.0016      |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.000678    |\n",
      "|    std                  | 0.0223       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4112640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00957    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4112640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003932027 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.93        |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.000886    |\n",
      "|    loss                 | -0.00126    |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.000615   |\n",
      "|    std                  | 0.0221      |\n",
      "|    value_loss           | 4.63e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 4501     |\n",
      "|    total_timesteps | 4112640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4132800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4132800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028617545 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.94         |\n",
      "|    explained_variance   | 0.287        |\n",
      "|    learning_rate        | 0.000883     |\n",
      "|    loss                 | -0.00186     |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0003      |\n",
      "|    std                  | 0.0221       |\n",
      "|    value_loss           | 8.3e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4152960, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0174      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4152960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026974827 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.96         |\n",
      "|    explained_variance   | 0.278        |\n",
      "|    learning_rate        | 0.00088      |\n",
      "|    loss                 | -0.00166     |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.000597    |\n",
      "|    std                  | 0.0217       |\n",
      "|    value_loss           | 7.91e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 4546     |\n",
      "|    total_timesteps | 4152960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4173120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00973     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4173120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036831999 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.98         |\n",
      "|    explained_variance   | 0.273        |\n",
      "|    learning_rate        | 0.000877     |\n",
      "|    loss                 | -0.00153     |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -5.64e-05    |\n",
      "|    std                  | 0.0216       |\n",
      "|    value_loss           | 4.2e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4193280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00945     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4193280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015952552 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.99         |\n",
      "|    explained_variance   | 0.251        |\n",
      "|    learning_rate        | 0.000874     |\n",
      "|    loss                 | -0.000797    |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.00034     |\n",
      "|    std                  | 0.0214       |\n",
      "|    value_loss           | 1.43e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 4590     |\n",
      "|    total_timesteps | 4193280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4213440, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.016       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4213440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030013288 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.01         |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.000871     |\n",
      "|    loss                 | -0.000799    |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.000986    |\n",
      "|    std                  | 0.0211       |\n",
      "|    value_loss           | 1.69e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4233600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.01         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4233600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038364858 |\n",
      "|    clip_fraction        | 0.00483       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 5.03          |\n",
      "|    explained_variance   | 0.314         |\n",
      "|    learning_rate        | 0.000868      |\n",
      "|    loss                 | -0.000473     |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | -0.000149     |\n",
      "|    std                  | 0.0209        |\n",
      "|    value_loss           | 8.43e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 4634     |\n",
      "|    total_timesteps | 4233600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4253760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4253760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003494564 |\n",
      "|    clip_fraction        | 0.0137      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.05        |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.000865    |\n",
      "|    loss                 | -0.000414   |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.000442   |\n",
      "|    std                  | 0.0208      |\n",
      "|    value_loss           | 4.68e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4273920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00972     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4273920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028328765 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.06         |\n",
      "|    explained_variance   | 0.297        |\n",
      "|    learning_rate        | 0.000862     |\n",
      "|    loss                 | -0.000433    |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | -0.000199    |\n",
      "|    std                  | 0.0208       |\n",
      "|    value_loss           | 4.23e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 4678     |\n",
      "|    total_timesteps | 4273920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4294080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4294080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032314565 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.07         |\n",
      "|    explained_variance   | 0.281        |\n",
      "|    learning_rate        | 0.000859     |\n",
      "|    loss                 | 0.000399     |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.000243    |\n",
      "|    std                  | 0.0207       |\n",
      "|    value_loss           | 1.11e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4314240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4314240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024599757 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.07         |\n",
      "|    explained_variance   | 0.291        |\n",
      "|    learning_rate        | 0.000856     |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -5.79e-05    |\n",
      "|    std                  | 0.0206       |\n",
      "|    value_loss           | 2.93e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 4722     |\n",
      "|    total_timesteps | 4314240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4334400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00908    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4334400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007024552 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.09        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.000853    |\n",
      "|    loss                 | 0.000335    |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -1.64e-05   |\n",
      "|    std                  | 0.0203      |\n",
      "|    value_loss           | 5.62e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4354560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4354560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018399765 |\n",
      "|    clip_fraction        | 0.00502      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.11         |\n",
      "|    explained_variance   | 0.256        |\n",
      "|    learning_rate        | 0.00085      |\n",
      "|    loss                 | 0.000462     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -7.88e-05    |\n",
      "|    std                  | 0.0202       |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 4766     |\n",
      "|    total_timesteps | 4354560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4374720, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0155      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4374720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029276507 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.12         |\n",
      "|    explained_variance   | 0.355        |\n",
      "|    learning_rate        | 0.000847     |\n",
      "|    loss                 | -0.00108     |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.000588    |\n",
      "|    std                  | 0.02         |\n",
      "|    value_loss           | 7.72e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4394880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4394880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050213253 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.13         |\n",
      "|    explained_variance   | 0.219        |\n",
      "|    learning_rate        | 0.000844     |\n",
      "|    loss                 | 0.000316     |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | -0.000377    |\n",
      "|    std                  | 0.02         |\n",
      "|    value_loss           | 7.63e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0107  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 4810     |\n",
      "|    total_timesteps | 4394880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4415040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0122      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4415040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014781184 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.14         |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.000841     |\n",
      "|    loss                 | -0.000403    |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.000242    |\n",
      "|    std                  | 0.0198       |\n",
      "|    value_loss           | 6.79e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4435200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00986     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4435200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020057783 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.15         |\n",
      "|    explained_variance   | -0.558       |\n",
      "|    learning_rate        | 0.000838     |\n",
      "|    loss                 | -0.00082     |\n",
      "|    n_updates            | 2190         |\n",
      "|    policy_gradient_loss | -0.000171    |\n",
      "|    std                  | 0.0198       |\n",
      "|    value_loss           | 6.83e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0108  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 4854     |\n",
      "|    total_timesteps | 4435200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4455360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4455360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018782079 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.14         |\n",
      "|    explained_variance   | -0.489       |\n",
      "|    learning_rate        | 0.000835     |\n",
      "|    loss                 | 0.000411     |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.000465    |\n",
      "|    std                  | 0.0197       |\n",
      "|    value_loss           | 2.41e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4475520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00892     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4475520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016643389 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.15         |\n",
      "|    explained_variance   | -0.385       |\n",
      "|    learning_rate        | 0.000832     |\n",
      "|    loss                 | 0.000115     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    std                  | 0.0197       |\n",
      "|    value_loss           | 9.32e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 4898     |\n",
      "|    total_timesteps | 4475520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4495680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00964     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4495680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029841703 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.14         |\n",
      "|    explained_variance   | -0.323       |\n",
      "|    learning_rate        | 0.000829     |\n",
      "|    loss                 | 0.0004       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.000401    |\n",
      "|    std                  | 0.0197       |\n",
      "|    value_loss           | 9.35e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4515840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0116      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4515840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050205425 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.14         |\n",
      "|    explained_variance   | -0.288       |\n",
      "|    learning_rate        | 0.000826     |\n",
      "|    loss                 | -0.00275     |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -0.000775    |\n",
      "|    std                  | 0.0196       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 4943     |\n",
      "|    total_timesteps | 4515840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4536000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004781383 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.16        |\n",
      "|    explained_variance   | -0.211      |\n",
      "|    learning_rate        | 0.000823    |\n",
      "|    loss                 | -0.00127    |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    std                  | 0.0195      |\n",
      "|    value_loss           | 1.45e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4556160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4556160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012505108 |\n",
      "|    clip_fraction        | 0.01         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.17         |\n",
      "|    explained_variance   | -0.205       |\n",
      "|    learning_rate        | 0.00082      |\n",
      "|    loss                 | -0.000348    |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -0.000208    |\n",
      "|    std                  | 0.0195       |\n",
      "|    value_loss           | 9.77e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 4987     |\n",
      "|    total_timesteps | 4556160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4576320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.012       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4576320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011572391 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.17         |\n",
      "|    explained_variance   | -0.18        |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | 5.34e-05     |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -6.54e-05    |\n",
      "|    std                  | 0.0194       |\n",
      "|    value_loss           | 1.14e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4596480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4596480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028692875 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.18         |\n",
      "|    explained_variance   | -0.304       |\n",
      "|    learning_rate        | 0.000814     |\n",
      "|    loss                 | -0.000191    |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | -0.000357    |\n",
      "|    std                  | 0.0193       |\n",
      "|    value_loss           | 4.88e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 5032     |\n",
      "|    total_timesteps | 4596480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4616640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00956     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4616640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019583763 |\n",
      "|    clip_fraction        | 0.00211      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.2          |\n",
      "|    explained_variance   | -0.205       |\n",
      "|    learning_rate        | 0.000811     |\n",
      "|    loss                 | 0.000128     |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.000259    |\n",
      "|    std                  | 0.0191       |\n",
      "|    value_loss           | 1.1e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4636800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4636800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029300353 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.23         |\n",
      "|    explained_variance   | -0.219       |\n",
      "|    learning_rate        | 0.000808     |\n",
      "|    loss                 | 0.000393     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.000241    |\n",
      "|    std                  | 0.0188       |\n",
      "|    value_loss           | 6.48e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 5077     |\n",
      "|    total_timesteps | 4636800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4656960, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0144      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4656960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058203554 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.25         |\n",
      "|    explained_variance   | -0.281       |\n",
      "|    learning_rate        | 0.000804     |\n",
      "|    loss                 | -0.000959    |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.000806    |\n",
      "|    std                  | 0.0188       |\n",
      "|    value_loss           | 4.62e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4677120, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0177      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4677120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054589557 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.26         |\n",
      "|    explained_variance   | -0.102       |\n",
      "|    learning_rate        | 0.000801     |\n",
      "|    loss                 | -0.000342    |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -0.000713    |\n",
      "|    std                  | 0.0187       |\n",
      "|    value_loss           | 1.53e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 5121     |\n",
      "|    total_timesteps | 4677120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4697280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4697280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035320655 |\n",
      "|    clip_fraction        | 0.00788      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.27         |\n",
      "|    explained_variance   | -0.172       |\n",
      "|    learning_rate        | 0.000798     |\n",
      "|    loss                 | 0.00146      |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -5.58e-05    |\n",
      "|    std                  | 0.0186       |\n",
      "|    value_loss           | 6.69e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4717440, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4717440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002485801 |\n",
      "|    clip_fraction        | 0.00512     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.28        |\n",
      "|    explained_variance   | -0.154      |\n",
      "|    learning_rate        | 0.000795    |\n",
      "|    loss                 | 0.00168     |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -9.48e-05   |\n",
      "|    std                  | 0.0185      |\n",
      "|    value_loss           | 6.83e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 5166     |\n",
      "|    total_timesteps | 4717440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4737600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00979     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4737600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018001356 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.29         |\n",
      "|    explained_variance   | -0.112       |\n",
      "|    learning_rate        | 0.000792     |\n",
      "|    loss                 | -9.27e-05    |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.000419    |\n",
      "|    std                  | 0.0184       |\n",
      "|    value_loss           | 1.22e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4757760, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0151      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4757760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036343746 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.29         |\n",
      "|    explained_variance   | -0.103       |\n",
      "|    learning_rate        | 0.000789     |\n",
      "|    loss                 | -0.00199     |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | -0.000626    |\n",
      "|    std                  | 0.0183       |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 5210     |\n",
      "|    total_timesteps | 4757760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4777920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4777920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005556134 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.3         |\n",
      "|    explained_variance   | -0.102      |\n",
      "|    learning_rate        | 0.000786    |\n",
      "|    loss                 | -0.00111    |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.00054    |\n",
      "|    std                  | 0.0183      |\n",
      "|    value_loss           | 8.26e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4798080, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0152      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4798080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015754859 |\n",
      "|    clip_fraction        | 0.00538      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.3          |\n",
      "|    explained_variance   | -0.0724      |\n",
      "|    learning_rate        | 0.000783     |\n",
      "|    loss                 | -2.14e-05    |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | -0.000106    |\n",
      "|    std                  | 0.0181       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 5255     |\n",
      "|    total_timesteps | 4798080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4818240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0109       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4818240       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042217245 |\n",
      "|    clip_fraction        | 0.00895       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 5.32          |\n",
      "|    explained_variance   | -0.0913       |\n",
      "|    learning_rate        | 0.00078       |\n",
      "|    loss                 | -0.000597     |\n",
      "|    n_updates            | 2380          |\n",
      "|    policy_gradient_loss | -0.000483     |\n",
      "|    std                  | 0.018         |\n",
      "|    value_loss           | 5.16e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=4838400, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0167      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4838400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031310217 |\n",
      "|    clip_fraction        | 0.00867      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.34         |\n",
      "|    explained_variance   | -0.0716      |\n",
      "|    learning_rate        | 0.000777     |\n",
      "|    loss                 | -0.000278    |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.000217    |\n",
      "|    std                  | 0.0179       |\n",
      "|    value_loss           | 8.4e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 913      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 5299     |\n",
      "|    total_timesteps | 4838400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4858560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.012       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4858560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031195905 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.35         |\n",
      "|    explained_variance   | -0.0784      |\n",
      "|    learning_rate        | 0.000774     |\n",
      "|    loss                 | -0.00147     |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.00023     |\n",
      "|    std                  | 0.0179       |\n",
      "|    value_loss           | 1.12e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4878720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4878720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051785754 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.34         |\n",
      "|    explained_variance   | -0.0625      |\n",
      "|    learning_rate        | 0.000771     |\n",
      "|    loss                 | -0.00134     |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | -0.000582    |\n",
      "|    std                  | 0.018        |\n",
      "|    value_loss           | 1.35e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 5344     |\n",
      "|    total_timesteps | 4878720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4898880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00929     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4898880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045873984 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.34         |\n",
      "|    explained_variance   | -0.0576      |\n",
      "|    learning_rate        | 0.000768     |\n",
      "|    loss                 | 0.00187      |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.000242    |\n",
      "|    std                  | 0.018        |\n",
      "|    value_loss           | 1.6e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4919040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4919040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002427754 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.35        |\n",
      "|    explained_variance   | -0.038      |\n",
      "|    learning_rate        | 0.000765    |\n",
      "|    loss                 | -0.000943   |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.000396   |\n",
      "|    std                  | 0.0179      |\n",
      "|    value_loss           | 5.13e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 5388     |\n",
      "|    total_timesteps | 4919040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4939200, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0186     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4939200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001735909 |\n",
      "|    clip_fraction        | 0.00922     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.35        |\n",
      "|    explained_variance   | -0.0638     |\n",
      "|    learning_rate        | 0.000762    |\n",
      "|    loss                 | 0.000429    |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.000374   |\n",
      "|    std                  | 0.0179      |\n",
      "|    value_loss           | 5.53e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4959360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4959360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019179524 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.36         |\n",
      "|    explained_variance   | -0.0191      |\n",
      "|    learning_rate        | 0.000759     |\n",
      "|    loss                 | -0.00204     |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.000473    |\n",
      "|    std                  | 0.0178       |\n",
      "|    value_loss           | 3.96e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 5433     |\n",
      "|    total_timesteps | 4959360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4979520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00917     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4979520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009358119 |\n",
      "|    clip_fraction        | 0.00534      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.37         |\n",
      "|    explained_variance   | -0.0422      |\n",
      "|    learning_rate        | 0.000756     |\n",
      "|    loss                 | -0.000326    |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -1.97e-05    |\n",
      "|    std                  | 0.0176       |\n",
      "|    value_loss           | 8.84e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4999680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00959     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4999680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058081667 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.38         |\n",
      "|    explained_variance   | -0.0249      |\n",
      "|    learning_rate        | 0.000753     |\n",
      "|    loss                 | -0.000198    |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | -0.000853    |\n",
      "|    std                  | 0.0175       |\n",
      "|    value_loss           | 1.36e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 5477     |\n",
      "|    total_timesteps | 4999680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5019840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0136      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5019840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019048962 |\n",
      "|    clip_fraction        | 0.00335      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.39         |\n",
      "|    explained_variance   | -0.0218      |\n",
      "|    learning_rate        | 0.00075      |\n",
      "|    loss                 | 8.71e-05     |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.00018     |\n",
      "|    std                  | 0.0175       |\n",
      "|    value_loss           | 5.37e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5040000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0122      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5040000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020155157 |\n",
      "|    clip_fraction        | 0.0029       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.4          |\n",
      "|    explained_variance   | -0.0392      |\n",
      "|    learning_rate        | 0.000747     |\n",
      "|    loss                 | 5.53e-05     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -3.16e-05    |\n",
      "|    std                  | 0.0174       |\n",
      "|    value_loss           | 1.37e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 5522     |\n",
      "|    total_timesteps | 5040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5060160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0116      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5060160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029602456 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.4          |\n",
      "|    explained_variance   | -0.00333     |\n",
      "|    learning_rate        | 0.000744     |\n",
      "|    loss                 | 0.0002       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.000332    |\n",
      "|    std                  | 0.0176       |\n",
      "|    value_loss           | 8.07e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5080320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5080320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013609447 |\n",
      "|    clip_fraction        | 0.00467      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.41         |\n",
      "|    explained_variance   | -0.0162      |\n",
      "|    learning_rate        | 0.000741     |\n",
      "|    loss                 | -0.00111     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -9.48e-05    |\n",
      "|    std                  | 0.0174       |\n",
      "|    value_loss           | 5.92e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 5566     |\n",
      "|    total_timesteps | 5080320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5100480, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0177      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5100480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034282492 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.43         |\n",
      "|    explained_variance   | -0.00128     |\n",
      "|    learning_rate        | 0.000738     |\n",
      "|    loss                 | -0.00136     |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.000755    |\n",
      "|    std                  | 0.0172       |\n",
      "|    value_loss           | 1.17e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5120640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.00967      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 5120640       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024723203 |\n",
      "|    clip_fraction        | 0.00779       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 5.44          |\n",
      "|    explained_variance   | -0.00897      |\n",
      "|    learning_rate        | 0.000735      |\n",
      "|    loss                 | 0.000258      |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | -8.08e-05     |\n",
      "|    std                  | 0.0171        |\n",
      "|    value_loss           | 1.25e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 5611     |\n",
      "|    total_timesteps | 5120640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5140800, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5140800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024697622 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.45         |\n",
      "|    explained_variance   | 0.062        |\n",
      "|    learning_rate        | 0.000732     |\n",
      "|    loss                 | -0.000602    |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    std                  | 0.017        |\n",
      "|    value_loss           | 3.32e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5160960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5160960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034528598 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.46         |\n",
      "|    explained_variance   | 0.0596       |\n",
      "|    learning_rate        | 0.000729     |\n",
      "|    loss                 | -0.000428    |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.000206    |\n",
      "|    std                  | 0.017        |\n",
      "|    value_loss           | 4.26e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 5655     |\n",
      "|    total_timesteps | 5160960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5181120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5181120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002442691 |\n",
      "|    clip_fraction        | 0.00547     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.47        |\n",
      "|    explained_variance   | -0.00976    |\n",
      "|    learning_rate        | 0.000726    |\n",
      "|    loss                 | -4.29e-05   |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -4.32e-05   |\n",
      "|    std                  | 0.017       |\n",
      "|    value_loss           | 1.57e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5201280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5201280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040725945 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.47         |\n",
      "|    explained_variance   | 0.0226       |\n",
      "|    learning_rate        | 0.000723     |\n",
      "|    loss                 | 0.000556     |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.0169       |\n",
      "|    value_loss           | 1.31e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 5699     |\n",
      "|    total_timesteps | 5201280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5221440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0129      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5221440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039190697 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.47         |\n",
      "|    explained_variance   | 0.0138       |\n",
      "|    learning_rate        | 0.00072      |\n",
      "|    loss                 | 0.0012       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.000162    |\n",
      "|    std                  | 0.0169       |\n",
      "|    value_loss           | 9.05e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5241600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5241600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057664625 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.49         |\n",
      "|    explained_variance   | 0.0601       |\n",
      "|    learning_rate        | 0.000717     |\n",
      "|    loss                 | 8.92e-05     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.000515    |\n",
      "|    std                  | 0.0167       |\n",
      "|    value_loss           | 5.83e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 5744     |\n",
      "|    total_timesteps | 5241600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5261760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5261760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030587418 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.5          |\n",
      "|    explained_variance   | 0.00273      |\n",
      "|    learning_rate        | 0.000714     |\n",
      "|    loss                 | -0.000789    |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.000417    |\n",
      "|    std                  | 0.0166       |\n",
      "|    value_loss           | 1.76e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5281920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5281920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037322976 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.5          |\n",
      "|    explained_variance   | 0.0617       |\n",
      "|    learning_rate        | 0.000711     |\n",
      "|    loss                 | -0.00188     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | -0.000865    |\n",
      "|    std                  | 0.0165       |\n",
      "|    value_loss           | 7.22e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 5788     |\n",
      "|    total_timesteps | 5281920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5302080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5302080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018824022 |\n",
      "|    clip_fraction        | 0.00423      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.52         |\n",
      "|    explained_variance   | 0.0818       |\n",
      "|    learning_rate        | 0.000708     |\n",
      "|    loss                 | 3.92e-05     |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.00026     |\n",
      "|    std                  | 0.0163       |\n",
      "|    value_loss           | 5.18e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5322240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0126      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5322240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022168446 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.54         |\n",
      "|    explained_variance   | 0.0536       |\n",
      "|    learning_rate        | 0.000705     |\n",
      "|    loss                 | -0.00282     |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.000481    |\n",
      "|    std                  | 0.0162       |\n",
      "|    value_loss           | 7.6e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 5832     |\n",
      "|    total_timesteps | 5322240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5342400, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0168      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5342400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026018985 |\n",
      "|    clip_fraction        | 0.00981      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.55         |\n",
      "|    explained_variance   | 0.089        |\n",
      "|    learning_rate        | 0.000702     |\n",
      "|    loss                 | 9.86e-05     |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0002      |\n",
      "|    std                  | 0.0161       |\n",
      "|    value_loss           | 4.28e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5362560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0111    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5362560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00241103 |\n",
      "|    clip_fraction        | 0.00885    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.56       |\n",
      "|    explained_variance   | 0.0198     |\n",
      "|    learning_rate        | 0.000699   |\n",
      "|    loss                 | -0.0013    |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | -0.000231  |\n",
      "|    std                  | 0.016      |\n",
      "|    value_loss           | 1.05e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 5877     |\n",
      "|    total_timesteps | 5362560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5382720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5382720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003776161 |\n",
      "|    clip_fraction        | 0.0111      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.56        |\n",
      "|    explained_variance   | 0.0362      |\n",
      "|    learning_rate        | 0.000696    |\n",
      "|    loss                 | -0.00163    |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.000418   |\n",
      "|    std                  | 0.016       |\n",
      "|    value_loss           | 8.98e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5402880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0131      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5402880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034278429 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.57         |\n",
      "|    explained_variance   | 0.0403       |\n",
      "|    learning_rate        | 0.000693     |\n",
      "|    loss                 | -0.00234     |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -0.000985    |\n",
      "|    std                  | 0.0159       |\n",
      "|    value_loss           | 9.3e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 5921     |\n",
      "|    total_timesteps | 5402880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5423040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00952    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5423040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002030855 |\n",
      "|    clip_fraction        | 0.0025      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.58        |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00069     |\n",
      "|    loss                 | -0.00173    |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.000121   |\n",
      "|    std                  | 0.0159      |\n",
      "|    value_loss           | 4.09e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5443200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00978     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5443200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030601805 |\n",
      "|    clip_fraction        | 0.00901      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.59         |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.000687     |\n",
      "|    loss                 | -0.00113     |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | -0.00039     |\n",
      "|    std                  | 0.0158       |\n",
      "|    value_loss           | 4.34e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 5965     |\n",
      "|    total_timesteps | 5443200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5463360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5463360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003924901 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.6         |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.000684    |\n",
      "|    loss                 | 2.67e-05    |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.000311   |\n",
      "|    std                  | 0.0158      |\n",
      "|    value_loss           | 6.12e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5483520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5483520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026372166 |\n",
      "|    clip_fraction        | 0.00434      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.6          |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.00068      |\n",
      "|    loss                 | 0.000717     |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | -0.000269    |\n",
      "|    std                  | 0.0156       |\n",
      "|    value_loss           | 4.35e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 6010     |\n",
      "|    total_timesteps | 5483520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5503680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00996     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5503680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028120398 |\n",
      "|    clip_fraction        | 0.0209       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.62         |\n",
      "|    explained_variance   | 0.0214       |\n",
      "|    learning_rate        | 0.000677     |\n",
      "|    loss                 | -0.00138     |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.00018     |\n",
      "|    std                  | 0.0156       |\n",
      "|    value_loss           | 1.39e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5523840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00958    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5523840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004381992 |\n",
      "|    clip_fraction        | 0.0193      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.63        |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.000674    |\n",
      "|    loss                 | -0.000993   |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.000579   |\n",
      "|    std                  | 0.0155      |\n",
      "|    value_loss           | 7.43e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 6054     |\n",
      "|    total_timesteps | 5523840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5544000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5544000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037749056 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.64         |\n",
      "|    explained_variance   | 0.0835       |\n",
      "|    learning_rate        | 0.000671     |\n",
      "|    loss                 | 0.000621     |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.000683    |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 7.23e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5564160, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0133      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5564160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015081649 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.64         |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.000668     |\n",
      "|    loss                 | -0.000394    |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | -0.000165    |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 5.7e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 6099     |\n",
      "|    total_timesteps | 5564160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5584320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5584320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028582572 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.64         |\n",
      "|    explained_variance   | 0.086        |\n",
      "|    learning_rate        | 0.000665     |\n",
      "|    loss                 | -0.00127     |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.000531    |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 7.16e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5604480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5604480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031223814 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.64         |\n",
      "|    explained_variance   | 0.0989       |\n",
      "|    learning_rate        | 0.000662     |\n",
      "|    loss                 | -0.000563    |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.000132    |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 6.86e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 6143     |\n",
      "|    total_timesteps | 5604480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5624640, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5624640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001423605 |\n",
      "|    clip_fraction        | 0.0123      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.64        |\n",
      "|    explained_variance   | 0.0474      |\n",
      "|    learning_rate        | 0.000659    |\n",
      "|    loss                 | 0.000165    |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.000217   |\n",
      "|    std                  | 0.0155      |\n",
      "|    value_loss           | 1.04e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5644800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00945    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5644800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002431758 |\n",
      "|    clip_fraction        | 0.0106      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.64        |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.000656    |\n",
      "|    loss                 | -0.000415   |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.000423   |\n",
      "|    std                  | 0.0155      |\n",
      "|    value_loss           | 1.56e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 6188     |\n",
      "|    total_timesteps | 5644800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5664960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00973    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5664960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005581268 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.65        |\n",
      "|    explained_variance   | 0.0331      |\n",
      "|    learning_rate        | 0.000653    |\n",
      "|    loss                 | -0.000337   |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    std                  | 0.0155      |\n",
      "|    value_loss           | 1.89e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5685120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5685120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003281834 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.65        |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.00065     |\n",
      "|    loss                 | 0.00072     |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.000291   |\n",
      "|    std                  | 0.0155      |\n",
      "|    value_loss           | 5.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 6232     |\n",
      "|    total_timesteps | 5685120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5705280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00941     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5705280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020382414 |\n",
      "|    clip_fraction        | 0.00741      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.64         |\n",
      "|    explained_variance   | 0.0411       |\n",
      "|    learning_rate        | 0.000647     |\n",
      "|    loss                 | -0.00073     |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -8.92e-05    |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 1.2e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5725440, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0143      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5725440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022489757 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.65         |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.000644     |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.000483    |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 5.83e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 6276     |\n",
      "|    total_timesteps | 5725440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5745600, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.016      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5745600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004874747 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.65        |\n",
      "|    explained_variance   | 0.0649      |\n",
      "|    learning_rate        | 0.000641    |\n",
      "|    loss                 | -0.00112    |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.00093    |\n",
      "|    std                  | 0.0154      |\n",
      "|    value_loss           | 1.45e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5765760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5765760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030113426 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.65         |\n",
      "|    explained_variance   | 0.0779       |\n",
      "|    learning_rate        | 0.000638     |\n",
      "|    loss                 | -0.00133     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.000346    |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 1.07e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 6321     |\n",
      "|    total_timesteps | 5765760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5785920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5785920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010591422 |\n",
      "|    clip_fraction        | 0.00331      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.65         |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.000635     |\n",
      "|    loss                 | 8.75e-05     |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | 1.91e-05     |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 3.8e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5806080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0122      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5806080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022768765 |\n",
      "|    clip_fraction        | 0.0249       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.65         |\n",
      "|    explained_variance   | 0.0897       |\n",
      "|    learning_rate        | 0.000632     |\n",
      "|    loss                 | -0.00194     |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.000485    |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 6.98e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0108  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 6365     |\n",
      "|    total_timesteps | 5806080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5826240, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0199      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5826240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015967404 |\n",
      "|    clip_fraction        | 0.00987      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.65         |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.000629     |\n",
      "|    loss                 | 0.000153     |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.000357    |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 6e-08        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5846400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5846400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038295346 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.65         |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.000626     |\n",
      "|    loss                 | 3.53e-05     |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.00044     |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 5.78e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0107  |\n",
      "| time/              |          |\n",
      "|    fps             | 912      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 6410     |\n",
      "|    total_timesteps | 5846400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5866560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00958     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5866560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037260535 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.66         |\n",
      "|    explained_variance   | 0.0618       |\n",
      "|    learning_rate        | 0.000623     |\n",
      "|    loss                 | -0.000258    |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.00054     |\n",
      "|    std                  | 0.0155       |\n",
      "|    value_loss           | 7.32e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5886720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5886720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040226253 |\n",
      "|    clip_fraction        | 0.00979      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.66         |\n",
      "|    explained_variance   | 0.0921       |\n",
      "|    learning_rate        | 0.00062      |\n",
      "|    loss                 | 0.000782     |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | 4.09e-05     |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 1.16e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 6454     |\n",
      "|    total_timesteps | 5886720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5906880, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0171      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5906880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030121368 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.67         |\n",
      "|    explained_variance   | 0.093        |\n",
      "|    learning_rate        | 0.000617     |\n",
      "|    loss                 | -0.0022      |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.000163    |\n",
      "|    std                  | 0.0153       |\n",
      "|    value_loss           | 1.13e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5927040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00981    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5927040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003918421 |\n",
      "|    clip_fraction        | 0.00974     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.67        |\n",
      "|    explained_variance   | 0.0553      |\n",
      "|    learning_rate        | 0.000614    |\n",
      "|    loss                 | 0.0016      |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.000109   |\n",
      "|    std                  | 0.0154      |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 6499     |\n",
      "|    total_timesteps | 5927040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5947200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00974     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5947200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046139136 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.66         |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 0.000611     |\n",
      "|    loss                 | -0.000583    |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.000845    |\n",
      "|    std                  | 0.0153       |\n",
      "|    value_loss           | 5.43e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5967360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5967360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032126964 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.68         |\n",
      "|    explained_variance   | 0.148        |\n",
      "|    learning_rate        | 0.000608     |\n",
      "|    loss                 | -0.00212     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000102    |\n",
      "|    std                  | 0.0153       |\n",
      "|    value_loss           | 4.17e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 6543     |\n",
      "|    total_timesteps | 5967360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5987520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5987520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004996876 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.68        |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.000605    |\n",
      "|    loss                 | -0.000375   |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.000455   |\n",
      "|    std                  | 0.0152      |\n",
      "|    value_loss           | 6.12e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6007680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.00839      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 6007680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030712236 |\n",
      "|    clip_fraction        | 0.00857       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 5.68          |\n",
      "|    explained_variance   | 0.168         |\n",
      "|    learning_rate        | 0.000602      |\n",
      "|    loss                 | 0.000168      |\n",
      "|    n_updates            | 2970          |\n",
      "|    policy_gradient_loss | 4.16e-05      |\n",
      "|    std                  | 0.0153        |\n",
      "|    value_loss           | 4.7e-08       |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 6588     |\n",
      "|    total_timesteps | 6007680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6027840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00987    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6027840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005035323 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.68        |\n",
      "|    explained_variance   | 0.0807      |\n",
      "|    learning_rate        | 0.000599    |\n",
      "|    loss                 | -0.000725   |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.00037    |\n",
      "|    std                  | 0.0152      |\n",
      "|    value_loss           | 1.19e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6048000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6048000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048057954 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.69         |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.000596     |\n",
      "|    loss                 | 0.00022      |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | -0.000166    |\n",
      "|    std                  | 0.0152       |\n",
      "|    value_loss           | 4.88e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 6632     |\n",
      "|    total_timesteps | 6048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6068160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6068160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003938832 |\n",
      "|    clip_fraction        | 0.01        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.7         |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.000593    |\n",
      "|    loss                 | 8.44e-05    |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | -0.00032    |\n",
      "|    std                  | 0.0151      |\n",
      "|    value_loss           | 5.35e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6088320, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6088320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001475226 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.72        |\n",
      "|    explained_variance   | 0.0845      |\n",
      "|    learning_rate        | 0.00059     |\n",
      "|    loss                 | 1.41e-05    |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.000458   |\n",
      "|    std                  | 0.015       |\n",
      "|    value_loss           | 8.24e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 6676     |\n",
      "|    total_timesteps | 6088320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6108480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0115       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 6108480       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019016633 |\n",
      "|    clip_fraction        | 0.0116        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 5.73          |\n",
      "|    explained_variance   | 0.139         |\n",
      "|    learning_rate        | 0.000587      |\n",
      "|    loss                 | 8.72e-06      |\n",
      "|    n_updates            | 3020          |\n",
      "|    policy_gradient_loss | -0.000145     |\n",
      "|    std                  | 0.015         |\n",
      "|    value_loss           | 5.38e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=6128640, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.012       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6128640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002430398 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.73         |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 0.000584     |\n",
      "|    loss                 | 0.000147     |\n",
      "|    n_updates            | 3030         |\n",
      "|    policy_gradient_loss | 4.7e-05      |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 5.12e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 6721     |\n",
      "|    total_timesteps | 6128640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6148800, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6148800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002392597 |\n",
      "|    clip_fraction        | 0.0103      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.73        |\n",
      "|    explained_variance   | 0.0874      |\n",
      "|    learning_rate        | 0.000581    |\n",
      "|    loss                 | 0.000384    |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.00031    |\n",
      "|    std                  | 0.015       |\n",
      "|    value_loss           | 7.67e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6168960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0134      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6168960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025686147 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.74         |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 0.000578     |\n",
      "|    loss                 | 0.000136     |\n",
      "|    n_updates            | 3050         |\n",
      "|    policy_gradient_loss | -0.00041     |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 3.39e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 6765     |\n",
      "|    total_timesteps | 6168960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6189120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6189120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016981477 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.73         |\n",
      "|    explained_variance   | 0.0835       |\n",
      "|    learning_rate        | 0.000575     |\n",
      "|    loss                 | -0.000787    |\n",
      "|    n_updates            | 3060         |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.0151       |\n",
      "|    value_loss           | 8.15e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6209280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00913     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6209280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074453773 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.73         |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.000572     |\n",
      "|    loss                 | 0.000173     |\n",
      "|    n_updates            | 3070         |\n",
      "|    policy_gradient_loss | -0.000153    |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 6.5e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0109  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 6809     |\n",
      "|    total_timesteps | 6209280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6229440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6229440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004422588 |\n",
      "|    clip_fraction        | 0.0394      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.73        |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.000569    |\n",
      "|    loss                 | 0.000606    |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.000295   |\n",
      "|    std                  | 0.015       |\n",
      "|    value_loss           | 3.5e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6249600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6249600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002073379 |\n",
      "|    clip_fraction        | 0.000838    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.73        |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.000566    |\n",
      "|    loss                 | 0.00104     |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | 0.000138    |\n",
      "|    std                  | 0.0151      |\n",
      "|    value_loss           | 3.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 6853     |\n",
      "|    total_timesteps | 6249600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6269760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00949     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6269760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022633763 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.72         |\n",
      "|    explained_variance   | 0.174        |\n",
      "|    learning_rate        | 0.000563     |\n",
      "|    loss                 | -0.0019      |\n",
      "|    n_updates            | 3100         |\n",
      "|    policy_gradient_loss | 0.000134     |\n",
      "|    std                  | 0.0151       |\n",
      "|    value_loss           | 4.65e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6289920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00973     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6289920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016667136 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.73         |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.00056      |\n",
      "|    loss                 | -0.000544    |\n",
      "|    n_updates            | 3110         |\n",
      "|    policy_gradient_loss | -0.00016     |\n",
      "|    std                  | 0.0151       |\n",
      "|    value_loss           | 7.53e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 6897     |\n",
      "|    total_timesteps | 6289920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6310080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6310080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040512094 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.74         |\n",
      "|    explained_variance   | 0.0872       |\n",
      "|    learning_rate        | 0.000557     |\n",
      "|    loss                 | 0.00135      |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -0.000761    |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 8.94e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6330240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0106     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6330240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004605753 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.74        |\n",
      "|    explained_variance   | 0.0901      |\n",
      "|    learning_rate        | 0.000553    |\n",
      "|    loss                 | -0.000912   |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | -0.000535   |\n",
      "|    std                  | 0.015       |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 6942     |\n",
      "|    total_timesteps | 6330240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6350400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6350400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029496136 |\n",
      "|    clip_fraction        | 0.00509      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.75         |\n",
      "|    explained_variance   | 0.205        |\n",
      "|    learning_rate        | 0.00055      |\n",
      "|    loss                 | 0.00159      |\n",
      "|    n_updates            | 3140         |\n",
      "|    policy_gradient_loss | -0.000132    |\n",
      "|    std                  | 0.0148       |\n",
      "|    value_loss           | 3.95e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6370560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00979   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6370560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00168879 |\n",
      "|    clip_fraction        | 0.00176    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.77       |\n",
      "|    explained_variance   | 0.181      |\n",
      "|    learning_rate        | 0.000547   |\n",
      "|    loss                 | 0.000464   |\n",
      "|    n_updates            | 3150       |\n",
      "|    policy_gradient_loss | -1.06e-05  |\n",
      "|    std                  | 0.0147     |\n",
      "|    value_loss           | 4.76e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 6986     |\n",
      "|    total_timesteps | 6370560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6390720, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6390720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002266441 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.78        |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.000544    |\n",
      "|    loss                 | -0.000893   |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.000748   |\n",
      "|    std                  | 0.0147      |\n",
      "|    value_loss           | 4.18e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6410880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6410880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015831885 |\n",
      "|    clip_fraction        | 0.00619      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.78         |\n",
      "|    explained_variance   | 0.0807       |\n",
      "|    learning_rate        | 0.000541     |\n",
      "|    loss                 | -0.000934    |\n",
      "|    n_updates            | 3170         |\n",
      "|    policy_gradient_loss | 5.09e-05     |\n",
      "|    std                  | 0.0148       |\n",
      "|    value_loss           | 9.79e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0108  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 7030     |\n",
      "|    total_timesteps | 6410880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6431040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6431040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016635893 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.77         |\n",
      "|    explained_variance   | 0.166        |\n",
      "|    learning_rate        | 0.000538     |\n",
      "|    loss                 | 3.98e-05     |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | -0.000458    |\n",
      "|    std                  | 0.0148       |\n",
      "|    value_loss           | 4.73e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6451200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6451200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019953835 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.77         |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 0.000535     |\n",
      "|    loss                 | -0.000527    |\n",
      "|    n_updates            | 3190         |\n",
      "|    policy_gradient_loss | -0.00051     |\n",
      "|    std                  | 0.0147       |\n",
      "|    value_loss           | 4.03e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 7074     |\n",
      "|    total_timesteps | 6451200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6471360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6471360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002250005 |\n",
      "|    clip_fraction        | 0.00435     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.77        |\n",
      "|    explained_variance   | 0.0724      |\n",
      "|    learning_rate        | 0.000532    |\n",
      "|    loss                 | 0.0015      |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.000124   |\n",
      "|    std                  | 0.0148      |\n",
      "|    value_loss           | 1.32e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6491520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00905     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6491520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040427353 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.77         |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.000529     |\n",
      "|    loss                 | -0.00133     |\n",
      "|    n_updates            | 3210         |\n",
      "|    policy_gradient_loss | -0.000296    |\n",
      "|    std                  | 0.0148       |\n",
      "|    value_loss           | 6.62e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 7118     |\n",
      "|    total_timesteps | 6491520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6511680, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6511680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001697316 |\n",
      "|    clip_fraction        | 0.00846     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.77        |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.000526    |\n",
      "|    loss                 | -0.00108    |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | -0.00025    |\n",
      "|    std                  | 0.0147      |\n",
      "|    value_loss           | 8.11e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6531840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6531840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036097683 |\n",
      "|    clip_fraction        | 0.0072       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.78         |\n",
      "|    explained_variance   | 0.0793       |\n",
      "|    learning_rate        | 0.000523     |\n",
      "|    loss                 | 0.000913     |\n",
      "|    n_updates            | 3230         |\n",
      "|    policy_gradient_loss | -0.000161    |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 9.71e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 7162     |\n",
      "|    total_timesteps | 6531840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6552000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.013       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6552000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001221567 |\n",
      "|    clip_fraction        | 0.000476     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.78         |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 0.00052      |\n",
      "|    loss                 | -0.000282    |\n",
      "|    n_updates            | 3240         |\n",
      "|    policy_gradient_loss | 8.59e-05     |\n",
      "|    std                  | 0.0147       |\n",
      "|    value_loss           | 4.24e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6572160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0101     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6572160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002147951 |\n",
      "|    clip_fraction        | 0.0105      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.78        |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.000517    |\n",
      "|    loss                 | -0.00129    |\n",
      "|    n_updates            | 3250        |\n",
      "|    policy_gradient_loss | -0.000344   |\n",
      "|    std                  | 0.0147      |\n",
      "|    value_loss           | 5.81e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 7206     |\n",
      "|    total_timesteps | 6572160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6592320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6592320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004340238 |\n",
      "|    clip_fraction        | 0.0104      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.79        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.000514    |\n",
      "|    loss                 | 0.0013      |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.000332   |\n",
      "|    std                  | 0.0146      |\n",
      "|    value_loss           | 3.39e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6612480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00929     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6612480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033533894 |\n",
      "|    clip_fraction        | 0.00863      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.79         |\n",
      "|    explained_variance   | 0.178        |\n",
      "|    learning_rate        | 0.000511     |\n",
      "|    loss                 | 0.000473     |\n",
      "|    n_updates            | 3270         |\n",
      "|    policy_gradient_loss | -0.000294    |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 5.25e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 7252     |\n",
      "|    total_timesteps | 6612480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6632640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6632640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025854567 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.79         |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.000508     |\n",
      "|    loss                 | -0.000181    |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | 5.06e-06     |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 7.68e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6652800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6652800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029459023 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.79         |\n",
      "|    explained_variance   | 0.057        |\n",
      "|    learning_rate        | 0.000505     |\n",
      "|    loss                 | 0.000925     |\n",
      "|    n_updates            | 3290         |\n",
      "|    policy_gradient_loss | -0.000494    |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 2.06e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 7299     |\n",
      "|    total_timesteps | 6652800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6672960, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0153      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6672960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044566677 |\n",
      "|    clip_fraction        | 0.009        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.79         |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.000502     |\n",
      "|    loss                 | 0.00236      |\n",
      "|    n_updates            | 3300         |\n",
      "|    policy_gradient_loss | -0.000326    |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 7.55e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6693120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6693120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019692993 |\n",
      "|    clip_fraction        | 0.00537      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.78         |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.000499     |\n",
      "|    loss                 | -0.00109     |\n",
      "|    n_updates            | 3310         |\n",
      "|    policy_gradient_loss | 4.19e-05     |\n",
      "|    std                  | 0.0146       |\n",
      "|    value_loss           | 2.97e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 7344     |\n",
      "|    total_timesteps | 6693120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6713280, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6713280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027507413 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.78         |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.000496     |\n",
      "|    loss                 | -0.00166     |\n",
      "|    n_updates            | 3320         |\n",
      "|    policy_gradient_loss | -0.000932    |\n",
      "|    std                  | 0.0145       |\n",
      "|    value_loss           | 7.17e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6733440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6733440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029066845 |\n",
      "|    clip_fraction        | 0.00494      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.8          |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.000493     |\n",
      "|    loss                 | -0.00147     |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -1.84e-05    |\n",
      "|    std                  | 0.0144       |\n",
      "|    value_loss           | 3.33e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 7388     |\n",
      "|    total_timesteps | 6733440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6753600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6753600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022745798 |\n",
      "|    clip_fraction        | 0.00573      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.81         |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.00049      |\n",
      "|    loss                 | 0.00139      |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.0001      |\n",
      "|    std                  | 0.0144       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6773760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6773760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004465879 |\n",
      "|    clip_fraction        | 0.0139      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.82        |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.000487    |\n",
      "|    loss                 | -0.000198   |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.000589   |\n",
      "|    std                  | 0.0143      |\n",
      "|    value_loss           | 3.67e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 7432     |\n",
      "|    total_timesteps | 6773760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6793920, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0169     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6793920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005337965 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.84        |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.000484    |\n",
      "|    loss                 | -0.00283    |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    std                  | 0.0142      |\n",
      "|    value_loss           | 6.7e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6814080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00948    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6814080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003763419 |\n",
      "|    clip_fraction        | 0.0147      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.85        |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | -0.00102    |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | -0.000509   |\n",
      "|    std                  | 0.0142      |\n",
      "|    value_loss           | 8.8e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 7476     |\n",
      "|    total_timesteps | 6814080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6834240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6834240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003041211 |\n",
      "|    clip_fraction        | 0.00919     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.85        |\n",
      "|    explained_variance   | 0.0492      |\n",
      "|    learning_rate        | 0.000478    |\n",
      "|    loss                 | 0.000234    |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.000545   |\n",
      "|    std                  | 0.0141      |\n",
      "|    value_loss           | 1.71e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6854400, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0146      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6854400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018833434 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.86         |\n",
      "|    explained_variance   | 0.19         |\n",
      "|    learning_rate        | 0.000475     |\n",
      "|    loss                 | -0.00106     |\n",
      "|    n_updates            | 3390         |\n",
      "|    policy_gradient_loss | -0.000116    |\n",
      "|    std                  | 0.014        |\n",
      "|    value_loss           | 6.61e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 7520     |\n",
      "|    total_timesteps | 6854400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6874560, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0134      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6874560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045951772 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.86         |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.000472     |\n",
      "|    loss                 | -0.00134     |\n",
      "|    n_updates            | 3400         |\n",
      "|    policy_gradient_loss | -0.000498    |\n",
      "|    std                  | 0.014        |\n",
      "|    value_loss           | 6.46e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6894720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6894720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017890271 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.87         |\n",
      "|    explained_variance   | 0.0987       |\n",
      "|    learning_rate        | 0.000469     |\n",
      "|    loss                 | -0.000503    |\n",
      "|    n_updates            | 3410         |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    std                  | 0.0139       |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 7565     |\n",
      "|    total_timesteps | 6894720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6914880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.01       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6914880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003088803 |\n",
      "|    clip_fraction        | 0.00976     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.88        |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.000466    |\n",
      "|    loss                 | -0.00146    |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.000374   |\n",
      "|    std                  | 0.0139      |\n",
      "|    value_loss           | 5.44e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6935040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6935040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033419072 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.88         |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 0.000463     |\n",
      "|    loss                 | -0.00173     |\n",
      "|    n_updates            | 3430         |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    std                  | 0.0139       |\n",
      "|    value_loss           | 9.05e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 7609     |\n",
      "|    total_timesteps | 6935040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6955200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00982     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6955200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002664107 |\n",
      "|    clip_fraction        | 0.00368      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.88         |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 0.00046      |\n",
      "|    loss                 | -0.000938    |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.000183    |\n",
      "|    std                  | 0.0139       |\n",
      "|    value_loss           | 6.29e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6975360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6975360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039524855 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.88         |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 0.000457     |\n",
      "|    loss                 | -0.000411    |\n",
      "|    n_updates            | 3450         |\n",
      "|    policy_gradient_loss | -0.000674    |\n",
      "|    std                  | 0.0139       |\n",
      "|    value_loss           | 4.44e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 7653     |\n",
      "|    total_timesteps | 6975360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6995520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.013       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6995520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014339066 |\n",
      "|    clip_fraction        | 0.00745      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.89         |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.000454     |\n",
      "|    loss                 | -0.000397    |\n",
      "|    n_updates            | 3460         |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    std                  | 0.0138       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7015680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7015680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046674507 |\n",
      "|    clip_fraction        | 0.00895      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.91         |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.000451     |\n",
      "|    loss                 | -0.000251    |\n",
      "|    n_updates            | 3470         |\n",
      "|    policy_gradient_loss | -0.000419    |\n",
      "|    std                  | 0.0137       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 7697     |\n",
      "|    total_timesteps | 7015680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7035840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7035840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010659163 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.91         |\n",
      "|    explained_variance   | 0.21         |\n",
      "|    learning_rate        | 0.000448     |\n",
      "|    loss                 | -3.49e-05    |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -5.05e-05    |\n",
      "|    std                  | 0.0138       |\n",
      "|    value_loss           | 5.83e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7056000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00957     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7056000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029005806 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.91         |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.000445     |\n",
      "|    loss                 | -0.00123     |\n",
      "|    n_updates            | 3490         |\n",
      "|    policy_gradient_loss | -0.000712    |\n",
      "|    std                  | 0.0137       |\n",
      "|    value_loss           | 7.52e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 7741     |\n",
      "|    total_timesteps | 7056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7076160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00962     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7076160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016165427 |\n",
      "|    clip_fraction        | 0.00991      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.91         |\n",
      "|    explained_variance   | 0.178        |\n",
      "|    learning_rate        | 0.000442     |\n",
      "|    loss                 | -2.78e-06    |\n",
      "|    n_updates            | 3500         |\n",
      "|    policy_gradient_loss | -0.000456    |\n",
      "|    std                  | 0.0136       |\n",
      "|    value_loss           | 6.62e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7096320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7096320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003628307 |\n",
      "|    clip_fraction        | 0.0139      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.000439    |\n",
      "|    loss                 | -0.00195    |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | -0.000675   |\n",
      "|    std                  | 0.0136      |\n",
      "|    value_loss           | 3.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 7785     |\n",
      "|    total_timesteps | 7096320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7116480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0125      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7116480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043758946 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.92         |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.000436     |\n",
      "|    loss                 | 9.64e-05     |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    std                  | 0.0135       |\n",
      "|    value_loss           | 9.36e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7136640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00942     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7136640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027067054 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.93         |\n",
      "|    explained_variance   | 0.0693       |\n",
      "|    learning_rate        | 0.000433     |\n",
      "|    loss                 | 0.00115      |\n",
      "|    n_updates            | 3530         |\n",
      "|    policy_gradient_loss | -0.000246    |\n",
      "|    std                  | 0.0135       |\n",
      "|    value_loss           | 5.25e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 7829     |\n",
      "|    total_timesteps | 7136640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7156800, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0171    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7156800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00316543 |\n",
      "|    clip_fraction        | 0.00758    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.93       |\n",
      "|    explained_variance   | 0.214      |\n",
      "|    learning_rate        | 0.00043    |\n",
      "|    loss                 | 0.000791   |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.000146  |\n",
      "|    std                  | 0.0135     |\n",
      "|    value_loss           | 5.78e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7176960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7176960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002949955 |\n",
      "|    clip_fraction        | 0.00736     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.000426    |\n",
      "|    loss                 | 0.00071     |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.000215   |\n",
      "|    std                  | 0.0134      |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 7873     |\n",
      "|    total_timesteps | 7176960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7197120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7197120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007423876 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.000423    |\n",
      "|    loss                 | 0.00315     |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.00074    |\n",
      "|    std                  | 0.0135      |\n",
      "|    value_loss           | 5.35e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7217280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00944    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7217280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004304016 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.0338      |\n",
      "|    learning_rate        | 0.00042     |\n",
      "|    loss                 | -0.00202    |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    std                  | 0.0135      |\n",
      "|    value_loss           | 1.28e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 7917     |\n",
      "|    total_timesteps | 7217280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7237440, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0183     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7237440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002986358 |\n",
      "|    clip_fraction        | 0.00486     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.000417    |\n",
      "|    loss                 | 0.000324    |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.000257   |\n",
      "|    std                  | 0.0135      |\n",
      "|    value_loss           | 3.57e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7257600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7257600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002492229 |\n",
      "|    clip_fraction        | 0.0105      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 0.000414    |\n",
      "|    loss                 | 0.000916    |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.000343   |\n",
      "|    std                  | 0.0134      |\n",
      "|    value_loss           | 8.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 7961     |\n",
      "|    total_timesteps | 7257600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7277760, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0141      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7277760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035942763 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.000411     |\n",
      "|    loss                 | -0.00082     |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -0.000768    |\n",
      "|    std                  | 0.0133       |\n",
      "|    value_loss           | 3.51e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7297920, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0177      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7297920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029259105 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | 0.0628       |\n",
      "|    learning_rate        | 0.000408     |\n",
      "|    loss                 | -0.000717    |\n",
      "|    n_updates            | 3610         |\n",
      "|    policy_gradient_loss | -0.00038     |\n",
      "|    std                  | 0.0133       |\n",
      "|    value_loss           | 8.12e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 8005     |\n",
      "|    total_timesteps | 7297920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7318080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0123      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7318080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032456573 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.97         |\n",
      "|    explained_variance   | 0.062        |\n",
      "|    learning_rate        | 0.000405     |\n",
      "|    loss                 | 0.000491     |\n",
      "|    n_updates            | 3620         |\n",
      "|    policy_gradient_loss | -0.000401    |\n",
      "|    std                  | 0.0133       |\n",
      "|    value_loss           | 1.14e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7338240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0138      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7338240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037794837 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.97         |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.000402     |\n",
      "|    loss                 | -0.000935    |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.000716    |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 8.29e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 8049     |\n",
      "|    total_timesteps | 7338240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7358400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0102      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7358400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019982262 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.98         |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.000399     |\n",
      "|    loss                 | -0.000701    |\n",
      "|    n_updates            | 3640         |\n",
      "|    policy_gradient_loss | -0.000476    |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 3.53e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7378560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7378560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003110679 |\n",
      "|    clip_fraction        | 0.00706     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.0975      |\n",
      "|    learning_rate        | 0.000396    |\n",
      "|    loss                 | 0.00131     |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | -0.000207   |\n",
      "|    std                  | 0.0133      |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 8094     |\n",
      "|    total_timesteps | 7378560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7398720, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7398720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040563615 |\n",
      "|    clip_fraction        | 0.00934      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.97         |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 0.000393     |\n",
      "|    loss                 | 0.000494     |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.000436    |\n",
      "|    std                  | 0.0133       |\n",
      "|    value_loss           | 4.28e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7418880, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0152      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7418880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028184163 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.97         |\n",
      "|    explained_variance   | 0.0603       |\n",
      "|    learning_rate        | 0.00039      |\n",
      "|    loss                 | -0.000712    |\n",
      "|    n_updates            | 3670         |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    std                  | 0.0133       |\n",
      "|    value_loss           | 1.16e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 8138     |\n",
      "|    total_timesteps | 7418880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7439040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0136      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7439040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019726432 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.000387     |\n",
      "|    loss                 | -0.00193     |\n",
      "|    n_updates            | 3680         |\n",
      "|    policy_gradient_loss | -0.000158    |\n",
      "|    std                  | 0.0134       |\n",
      "|    value_loss           | 4.4e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7459200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7459200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034698981 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.95         |\n",
      "|    explained_variance   | 0.0425       |\n",
      "|    learning_rate        | 0.000384     |\n",
      "|    loss                 | 0.000303     |\n",
      "|    n_updates            | 3690         |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    std                  | 0.0134       |\n",
      "|    value_loss           | 1.41e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 8182     |\n",
      "|    total_timesteps | 7459200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7479360, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.017       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7479360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024222247 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | 0.0928       |\n",
      "|    learning_rate        | 0.000381     |\n",
      "|    loss                 | -0.000588    |\n",
      "|    n_updates            | 3700         |\n",
      "|    policy_gradient_loss | -0.000436    |\n",
      "|    std                  | 0.0134       |\n",
      "|    value_loss           | 6.39e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7499520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00939     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7499520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018040922 |\n",
      "|    clip_fraction        | 0.0011       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.000378     |\n",
      "|    loss                 | -0.00083     |\n",
      "|    n_updates            | 3710         |\n",
      "|    policy_gradient_loss | 6.14e-06     |\n",
      "|    std                  | 0.0134       |\n",
      "|    value_loss           | 2.97e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 8226     |\n",
      "|    total_timesteps | 7499520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7519680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.00972      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 7519680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027273476 |\n",
      "|    clip_fraction        | 0.00893       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 5.96          |\n",
      "|    explained_variance   | 0.128         |\n",
      "|    learning_rate        | 0.000375      |\n",
      "|    loss                 | -0.000371     |\n",
      "|    n_updates            | 3720          |\n",
      "|    policy_gradient_loss | -1.23e-05     |\n",
      "|    std                  | 0.0134        |\n",
      "|    value_loss           | 5.99e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=7539840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7539840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002706594 |\n",
      "|    clip_fraction        | 0.0153      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.0769      |\n",
      "|    learning_rate        | 0.000372    |\n",
      "|    loss                 | 0.00138     |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.000459   |\n",
      "|    std                  | 0.0132      |\n",
      "|    value_loss           | 9.28e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 8270     |\n",
      "|    total_timesteps | 7539840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7560000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0145      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7560000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023502796 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.99         |\n",
      "|    explained_variance   | 0.0982       |\n",
      "|    learning_rate        | 0.000369     |\n",
      "|    loss                 | -2.78e-05    |\n",
      "|    n_updates            | 3740         |\n",
      "|    policy_gradient_loss | -0.000258    |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 7.42e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7580160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7580160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027703529 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.99         |\n",
      "|    explained_variance   | 0.186        |\n",
      "|    learning_rate        | 0.000366     |\n",
      "|    loss                 | -0.000853    |\n",
      "|    n_updates            | 3750         |\n",
      "|    policy_gradient_loss | -0.000819    |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 5.17e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 8314     |\n",
      "|    total_timesteps | 7580160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7600320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0102      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7600320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014432466 |\n",
      "|    clip_fraction        | 0.01         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6            |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.000363     |\n",
      "|    loss                 | -0.000263    |\n",
      "|    n_updates            | 3760         |\n",
      "|    policy_gradient_loss | -0.000134    |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 7.38e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7620480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7620480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047717025 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.99         |\n",
      "|    explained_variance   | 0.0591       |\n",
      "|    learning_rate        | 0.00036      |\n",
      "|    loss                 | -0.00147     |\n",
      "|    n_updates            | 3770         |\n",
      "|    policy_gradient_loss | -0.000888    |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 1.14e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 8358     |\n",
      "|    total_timesteps | 7620480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7640640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7640640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037285022 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6            |\n",
      "|    explained_variance   | 0.177        |\n",
      "|    learning_rate        | 0.000357     |\n",
      "|    loss                 | -0.00272     |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 4.67e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7660800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00996     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7660800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022149503 |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.01         |\n",
      "|    explained_variance   | 0.0946       |\n",
      "|    learning_rate        | 0.000354     |\n",
      "|    loss                 | -0.000398    |\n",
      "|    n_updates            | 3790         |\n",
      "|    policy_gradient_loss | -0.000332    |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 6.76e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 8402     |\n",
      "|    total_timesteps | 7660800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7680960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7680960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029543377 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6            |\n",
      "|    explained_variance   | 0.0816       |\n",
      "|    learning_rate        | 0.000351     |\n",
      "|    loss                 | -3.65e-05    |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 8.4e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7701120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7701120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004493134 |\n",
      "|    clip_fraction        | 0.0174      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.000348    |\n",
      "|    loss                 | -0.000633   |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.00081    |\n",
      "|    std                  | 0.0131      |\n",
      "|    value_loss           | 5.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 8446     |\n",
      "|    total_timesteps | 7701120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7721280, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0179     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7721280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002344988 |\n",
      "|    clip_fraction        | 0.00495     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.0728      |\n",
      "|    learning_rate        | 0.000345    |\n",
      "|    loss                 | -0.000617   |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | -1.87e-05   |\n",
      "|    std                  | 0.013       |\n",
      "|    value_loss           | 1.34e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7741440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7741440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030558584 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.02         |\n",
      "|    explained_variance   | 0.0843       |\n",
      "|    learning_rate        | 0.000342     |\n",
      "|    loss                 | 0.000764     |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -0.000311    |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 8490     |\n",
      "|    total_timesteps | 7741440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7761600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7761600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041664885 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.02         |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.000339     |\n",
      "|    loss                 | -0.00114     |\n",
      "|    n_updates            | 3840         |\n",
      "|    policy_gradient_loss | -0.000875    |\n",
      "|    std                  | 0.013        |\n",
      "|    value_loss           | 4.13e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7781760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0146      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7781760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014586898 |\n",
      "|    clip_fraction        | 0.000392     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.03         |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 0.000336     |\n",
      "|    loss                 | -0.00111     |\n",
      "|    n_updates            | 3850         |\n",
      "|    policy_gradient_loss | 7.19e-05     |\n",
      "|    std                  | 0.013        |\n",
      "|    value_loss           | 3.84e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 8534     |\n",
      "|    total_timesteps | 7781760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7801920, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7801920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003068018 |\n",
      "|    clip_fraction        | 0.0114      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.03        |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.000333    |\n",
      "|    loss                 | -0.00154    |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.0005     |\n",
      "|    std                  | 0.013       |\n",
      "|    value_loss           | 9.39e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7822080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7822080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041179056 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.04         |\n",
      "|    explained_variance   | 0.0982       |\n",
      "|    learning_rate        | 0.00033      |\n",
      "|    loss                 | -0.000516    |\n",
      "|    n_updates            | 3870         |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    std                  | 0.0129       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 8578     |\n",
      "|    total_timesteps | 7822080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7842240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.01        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7842240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045767636 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.04         |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.000327     |\n",
      "|    loss                 | 0.000968     |\n",
      "|    n_updates            | 3880         |\n",
      "|    policy_gradient_loss | -0.000798    |\n",
      "|    std                  | 0.0129       |\n",
      "|    value_loss           | 3.75e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7862400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0111      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7862400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042891037 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.05         |\n",
      "|    explained_variance   | 0.185        |\n",
      "|    learning_rate        | 0.000324     |\n",
      "|    loss                 | -0.00164     |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    std                  | 0.0128       |\n",
      "|    value_loss           | 3.99e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 8623     |\n",
      "|    total_timesteps | 7862400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7882560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00879     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7882560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025923783 |\n",
      "|    clip_fraction        | 0.00858      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.06         |\n",
      "|    explained_variance   | 0.096        |\n",
      "|    learning_rate        | 0.000321     |\n",
      "|    loss                 | -0.0015      |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.000172    |\n",
      "|    std                  | 0.0128       |\n",
      "|    value_loss           | 6.37e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7902720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7902720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028722775 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.06         |\n",
      "|    explained_variance   | 0.0558       |\n",
      "|    learning_rate        | 0.000318     |\n",
      "|    loss                 | 0.000309     |\n",
      "|    n_updates            | 3910         |\n",
      "|    policy_gradient_loss | -0.000924    |\n",
      "|    std                  | 0.0128       |\n",
      "|    value_loss           | 1.17e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 8668     |\n",
      "|    total_timesteps | 7902720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7922880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7922880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002627274 |\n",
      "|    clip_fraction        | 0.00484     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.06        |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.000315    |\n",
      "|    loss                 | -0.000445   |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.000318   |\n",
      "|    std                  | 0.0127      |\n",
      "|    value_loss           | 6.52e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7943040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7943040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004880225 |\n",
      "|    clip_fraction        | 0.0372      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.07        |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.000312    |\n",
      "|    loss                 | -0.000258   |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.000529   |\n",
      "|    std                  | 0.0127      |\n",
      "|    value_loss           | 6.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 8713     |\n",
      "|    total_timesteps | 7943040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7963200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00846     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7963200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035324427 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.07         |\n",
      "|    explained_variance   | 0.0972       |\n",
      "|    learning_rate        | 0.000309     |\n",
      "|    loss                 | -0.00232     |\n",
      "|    n_updates            | 3940         |\n",
      "|    policy_gradient_loss | -0.000718    |\n",
      "|    std                  | 0.0127       |\n",
      "|    value_loss           | 8.84e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7983360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7983360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029597403 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.07         |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.000306     |\n",
      "|    loss                 | -0.00206     |\n",
      "|    n_updates            | 3950         |\n",
      "|    policy_gradient_loss | -0.000511    |\n",
      "|    std                  | 0.0127       |\n",
      "|    value_loss           | 7.94e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 8758     |\n",
      "|    total_timesteps | 7983360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8003520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8003520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012997597 |\n",
      "|    clip_fraction        | 0.000308     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.07         |\n",
      "|    explained_variance   | 0.221        |\n",
      "|    learning_rate        | 0.000302     |\n",
      "|    loss                 | -0.000641    |\n",
      "|    n_updates            | 3960         |\n",
      "|    policy_gradient_loss | 1.23e-05     |\n",
      "|    std                  | 0.0127       |\n",
      "|    value_loss           | 3.32e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8023680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8023680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044575837 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.08         |\n",
      "|    explained_variance   | 0.072        |\n",
      "|    learning_rate        | 0.000299     |\n",
      "|    loss                 | -0.00158     |\n",
      "|    n_updates            | 3970         |\n",
      "|    policy_gradient_loss | -0.000448    |\n",
      "|    std                  | 0.0127       |\n",
      "|    value_loss           | 1.08e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 8802     |\n",
      "|    total_timesteps | 8023680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8043840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.012       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8043840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050405846 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.08         |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 0.000296     |\n",
      "|    loss                 | -0.00238     |\n",
      "|    n_updates            | 3980         |\n",
      "|    policy_gradient_loss | -0.000746    |\n",
      "|    std                  | 0.0126       |\n",
      "|    value_loss           | 2.48e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8064000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00893   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8064000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00369325 |\n",
      "|    clip_fraction        | 0.0308     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.09       |\n",
      "|    explained_variance   | 0.0816     |\n",
      "|    learning_rate        | 0.000293   |\n",
      "|    loss                 | -0.00183   |\n",
      "|    n_updates            | 3990       |\n",
      "|    policy_gradient_loss | -0.00128   |\n",
      "|    std                  | 0.0126     |\n",
      "|    value_loss           | 8.11e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 8847     |\n",
      "|    total_timesteps | 8064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8084160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00992     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8084160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039493917 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.1          |\n",
      "|    explained_variance   | 0.104        |\n",
      "|    learning_rate        | 0.00029      |\n",
      "|    loss                 | -0.000882    |\n",
      "|    n_updates            | 4000         |\n",
      "|    policy_gradient_loss | -0.000774    |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 9.53e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8104320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8104320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015107547 |\n",
      "|    clip_fraction        | 0.00419      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.1          |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 0.000287     |\n",
      "|    loss                 | -0.000637    |\n",
      "|    n_updates            | 4010         |\n",
      "|    policy_gradient_loss | 3.67e-05     |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 4.49e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 8891     |\n",
      "|    total_timesteps | 8104320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8124480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8124480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035786678 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.1          |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 0.000284     |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 4020         |\n",
      "|    policy_gradient_loss | -0.000661    |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 3.95e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8144640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0123      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8144640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019549886 |\n",
      "|    clip_fraction        | 0.00809      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.1          |\n",
      "|    explained_variance   | 0.198        |\n",
      "|    learning_rate        | 0.000281     |\n",
      "|    loss                 | -0.000513    |\n",
      "|    n_updates            | 4030         |\n",
      "|    policy_gradient_loss | -0.000161    |\n",
      "|    std                  | 0.0125       |\n",
      "|    value_loss           | 4.33e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 8935     |\n",
      "|    total_timesteps | 8144640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8164800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8164800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004174403 |\n",
      "|    clip_fraction        | 0.0166      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.11        |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.000278    |\n",
      "|    loss                 | -0.000674   |\n",
      "|    n_updates            | 4040        |\n",
      "|    policy_gradient_loss | -0.000756   |\n",
      "|    std                  | 0.0124      |\n",
      "|    value_loss           | 5.42e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8184960, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0164      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8184960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036501668 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.11         |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.000275     |\n",
      "|    loss                 | -0.00109     |\n",
      "|    n_updates            | 4050         |\n",
      "|    policy_gradient_loss | -0.000617    |\n",
      "|    std                  | 0.0124       |\n",
      "|    value_loss           | 7.06e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 8980     |\n",
      "|    total_timesteps | 8184960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8205120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8205120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021666996 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.12         |\n",
      "|    explained_variance   | 0.147        |\n",
      "|    learning_rate        | 0.000272     |\n",
      "|    loss                 | -0.000963    |\n",
      "|    n_updates            | 4060         |\n",
      "|    policy_gradient_loss | -0.000792    |\n",
      "|    std                  | 0.0123       |\n",
      "|    value_loss           | 6.08e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8225280, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0178      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8225280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039714053 |\n",
      "|    clip_fraction        | 0.00789      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.000269     |\n",
      "|    loss                 | 1.31e-05     |\n",
      "|    n_updates            | 4070         |\n",
      "|    policy_gradient_loss | -0.000177    |\n",
      "|    std                  | 0.0123       |\n",
      "|    value_loss           | 8.2e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 9024     |\n",
      "|    total_timesteps | 8225280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8245440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00999     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8245440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025885908 |\n",
      "|    clip_fraction        | 0.00152      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | 0.198        |\n",
      "|    learning_rate        | 0.000266     |\n",
      "|    loss                 | 0.000944     |\n",
      "|    n_updates            | 4080         |\n",
      "|    policy_gradient_loss | -8.28e-05    |\n",
      "|    std                  | 0.0123       |\n",
      "|    value_loss           | 2.87e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8265600, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0137      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8265600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062554213 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.000263     |\n",
      "|    loss                 | -0.00209     |\n",
      "|    n_updates            | 4090         |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 5.38e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0116  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 9069     |\n",
      "|    total_timesteps | 8265600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8285760, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0171      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8285760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036709658 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.079        |\n",
      "|    learning_rate        | 0.00026      |\n",
      "|    loss                 | -0.000192    |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 9.17e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8305920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8305920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014181428 |\n",
      "|    clip_fraction        | 0.0034       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.0796       |\n",
      "|    learning_rate        | 0.000257     |\n",
      "|    loss                 | 7.74e-06     |\n",
      "|    n_updates            | 4110         |\n",
      "|    policy_gradient_loss | -8.92e-05    |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 1.29e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 9113     |\n",
      "|    total_timesteps | 8305920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8326080, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0155      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8326080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018290362 |\n",
      "|    clip_fraction        | 0.00422      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.000254     |\n",
      "|    loss                 | -0.000503    |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -0.00021     |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 8.66e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8346240, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.017       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8346240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045291693 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.13         |\n",
      "|    explained_variance   | 0.0773       |\n",
      "|    learning_rate        | 0.000251     |\n",
      "|    loss                 | 0.000368     |\n",
      "|    n_updates            | 4130         |\n",
      "|    policy_gradient_loss | -0.000202    |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 1.08e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 9157     |\n",
      "|    total_timesteps | 8346240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8366400, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.011       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8366400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026044832 |\n",
      "|    clip_fraction        | 0.0084       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | -0.000946    |\n",
      "|    n_updates            | 4140         |\n",
      "|    policy_gradient_loss | -0.000323    |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 8.47e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8386560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8386560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008821081 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | -0.000148    |\n",
      "|    n_updates            | 4150         |\n",
      "|    policy_gradient_loss | -1.33e-05    |\n",
      "|    std                  | 0.0122       |\n",
      "|    value_loss           | 4.65e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 9202     |\n",
      "|    total_timesteps | 8386560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8406720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8406720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033119721 |\n",
      "|    clip_fraction        | 0.00337      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.14         |\n",
      "|    explained_variance   | 0.105        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | 0.000193     |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | -8.25e-05    |\n",
      "|    std                  | 0.0121       |\n",
      "|    value_loss           | 8.47e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8426880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.012       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8426880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020690123 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.15         |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | -0.00079     |\n",
      "|    n_updates            | 4170         |\n",
      "|    policy_gradient_loss | -0.000613    |\n",
      "|    std                  | 0.0121       |\n",
      "|    value_loss           | 5.89e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 9246     |\n",
      "|    total_timesteps | 8426880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8447040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0112      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8447040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024946062 |\n",
      "|    clip_fraction        | 0.00351      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.15         |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.000236     |\n",
      "|    loss                 | -0.00133     |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | 3.2e-05      |\n",
      "|    std                  | 0.0121       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8467200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0124      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8467200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022502025 |\n",
      "|    clip_fraction        | 0.00401      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.15         |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.000233     |\n",
      "|    loss                 | -0.000194    |\n",
      "|    n_updates            | 4190         |\n",
      "|    policy_gradient_loss | -0.000257    |\n",
      "|    std                  | 0.0121       |\n",
      "|    value_loss           | 3.21e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 9291     |\n",
      "|    total_timesteps | 8467200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8487360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8487360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034845567 |\n",
      "|    clip_fraction        | 0.00741      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.16         |\n",
      "|    explained_variance   | 0.0645       |\n",
      "|    learning_rate        | 0.00023      |\n",
      "|    loss                 | -0.00154     |\n",
      "|    n_updates            | 4200         |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    std                  | 0.0121       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8507520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8507520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00453235 |\n",
      "|    clip_fraction        | 0.0195     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.16       |\n",
      "|    explained_variance   | 0.096      |\n",
      "|    learning_rate        | 0.000227   |\n",
      "|    loss                 | -0.00257   |\n",
      "|    n_updates            | 4210       |\n",
      "|    policy_gradient_loss | -0.00036   |\n",
      "|    std                  | 0.012      |\n",
      "|    value_loss           | 7.06e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 9335     |\n",
      "|    total_timesteps | 8507520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8527680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8527680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026314463 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.000224     |\n",
      "|    loss                 | -0.000326    |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.000703    |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 6.98e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8547840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8547840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026736252 |\n",
      "|    clip_fraction        | 0.00842      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.158        |\n",
      "|    learning_rate        | 0.000221     |\n",
      "|    loss                 | 0.000228     |\n",
      "|    n_updates            | 4230         |\n",
      "|    policy_gradient_loss | -0.00033     |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 5.23e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 9379     |\n",
      "|    total_timesteps | 8547840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8568000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0132      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8568000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029436243 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.16         |\n",
      "|    explained_variance   | 0.0799       |\n",
      "|    learning_rate        | 0.000218     |\n",
      "|    loss                 | -0.00153     |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.000787    |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8588160, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0144       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8588160       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043992602 |\n",
      "|    clip_fraction        | 0.000685      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.17          |\n",
      "|    explained_variance   | 0.209         |\n",
      "|    learning_rate        | 0.000215      |\n",
      "|    loss                 | 0.000304      |\n",
      "|    n_updates            | 4250          |\n",
      "|    policy_gradient_loss | 4.55e-05      |\n",
      "|    std                  | 0.012         |\n",
      "|    value_loss           | 4.06e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 9424     |\n",
      "|    total_timesteps | 8588160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8608320, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8608320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034087896 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.16         |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -0.000186    |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -0.000809    |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 5.29e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8628480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8628480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018333192 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.141        |\n",
      "|    learning_rate        | 0.000209     |\n",
      "|    loss                 | -0.000891    |\n",
      "|    n_updates            | 4270         |\n",
      "|    policy_gradient_loss | -0.000481    |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 5.95e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 9468     |\n",
      "|    total_timesteps | 8628480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8648640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8648640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031590844 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.0724       |\n",
      "|    learning_rate        | 0.000206     |\n",
      "|    loss                 | -0.00152     |\n",
      "|    n_updates            | 4280         |\n",
      "|    policy_gradient_loss | -0.000619    |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 1.73e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8668800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00823    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8668800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002863266 |\n",
      "|    clip_fraction        | 0.00696     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.000203    |\n",
      "|    loss                 | 9.45e-05    |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | -0.000339   |\n",
      "|    std                  | 0.012       |\n",
      "|    value_loss           | 5.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 9512     |\n",
      "|    total_timesteps | 8668800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8688960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8688960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026436318 |\n",
      "|    clip_fraction        | 0.00711      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.0002       |\n",
      "|    loss                 | -0.00064     |\n",
      "|    n_updates            | 4300         |\n",
      "|    policy_gradient_loss | -0.000274    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 3.02e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8709120, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.017       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8709120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034017917 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.17         |\n",
      "|    explained_variance   | 0.0895       |\n",
      "|    learning_rate        | 0.000197     |\n",
      "|    loss                 | -0.000204    |\n",
      "|    n_updates            | 4310         |\n",
      "|    policy_gradient_loss | -0.000823    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 9557     |\n",
      "|    total_timesteps | 8709120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8729280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8729280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003939147 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.18        |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.000194    |\n",
      "|    loss                 | -0.00121    |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 5.7e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8749440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.00898      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8749440       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00078298885 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.18          |\n",
      "|    explained_variance   | 0.0878        |\n",
      "|    learning_rate        | 0.000191      |\n",
      "|    loss                 | -0.000362     |\n",
      "|    n_updates            | 4330          |\n",
      "|    policy_gradient_loss | -0.000164     |\n",
      "|    std                  | 0.0119        |\n",
      "|    value_loss           | 1.17e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 9601     |\n",
      "|    total_timesteps | 8749440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8769600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0122      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8769600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017452526 |\n",
      "|    clip_fraction        | 0.00218      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.000188     |\n",
      "|    loss                 | -3.68e-05    |\n",
      "|    n_updates            | 4340         |\n",
      "|    policy_gradient_loss | -0.000148    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 5.65e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8789760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8789760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021821405 |\n",
      "|    clip_fraction        | 0.00546      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.000185     |\n",
      "|    loss                 | -0.000909    |\n",
      "|    n_updates            | 4350         |\n",
      "|    policy_gradient_loss | -0.000249    |\n",
      "|    std                  | 0.0118       |\n",
      "|    value_loss           | 8.86e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 9645     |\n",
      "|    total_timesteps | 8789760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8809920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8809920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040800166 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.0959       |\n",
      "|    learning_rate        | 0.000182     |\n",
      "|    loss                 | -0.00335     |\n",
      "|    n_updates            | 4360         |\n",
      "|    policy_gradient_loss | -0.000874    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8830080, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8830080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002892822 |\n",
      "|    clip_fraction        | 0.0103      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.19        |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.000179    |\n",
      "|    loss                 | -0.000439   |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | -0.000459   |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 5.85e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0107  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 9690     |\n",
      "|    total_timesteps | 8830080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8850240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0104      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8850240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020859079 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.000175     |\n",
      "|    loss                 | 0.000105     |\n",
      "|    n_updates            | 4380         |\n",
      "|    policy_gradient_loss | -5.24e-05    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 2.86e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8870400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00897     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8870400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029406417 |\n",
      "|    clip_fraction        | 0.00191      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 0.000172     |\n",
      "|    loss                 | 0.000551     |\n",
      "|    n_updates            | 4390         |\n",
      "|    policy_gradient_loss | -5e-05       |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 3.09e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 9734     |\n",
      "|    total_timesteps | 8870400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8890560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00957     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8890560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027010718 |\n",
      "|    clip_fraction        | 0.00285      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.000169     |\n",
      "|    loss                 | 0.000372     |\n",
      "|    n_updates            | 4400         |\n",
      "|    policy_gradient_loss | -0.000203    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 5.7e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8910720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8910720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001627505 |\n",
      "|    clip_fraction        | 0.00499     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.18        |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.000166    |\n",
      "|    loss                 | -0.00178    |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.000389   |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 3.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 9779     |\n",
      "|    total_timesteps | 8910720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8930880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8930880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031995599 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.18         |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.000163     |\n",
      "|    loss                 | -0.00043     |\n",
      "|    n_updates            | 4420         |\n",
      "|    policy_gradient_loss | -0.000512    |\n",
      "|    std                  | 0.012        |\n",
      "|    value_loss           | 6.32e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8951040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0138       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8951040       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057741074 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.18          |\n",
      "|    explained_variance   | 0.142         |\n",
      "|    learning_rate        | 0.00016       |\n",
      "|    loss                 | -3.75e-05     |\n",
      "|    n_updates            | 4430          |\n",
      "|    policy_gradient_loss | 8.92e-07      |\n",
      "|    std                  | 0.0119        |\n",
      "|    value_loss           | 5.32e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 9823     |\n",
      "|    total_timesteps | 8951040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8971200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00928    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8971200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003527971 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.18        |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.000157    |\n",
      "|    loss                 | 0.00123     |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.0009     |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 9.69e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8991360, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0134     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8991360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002630481 |\n",
      "|    clip_fraction        | 0.00752     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.19        |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.000154    |\n",
      "|    loss                 | 8.2e-06     |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.000287   |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 8.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 9868     |\n",
      "|    total_timesteps | 8991360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9011520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0102      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9011520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034294534 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.19         |\n",
      "|    explained_variance   | 0.173        |\n",
      "|    learning_rate        | 0.000151     |\n",
      "|    loss                 | -0.00169     |\n",
      "|    n_updates            | 4460         |\n",
      "|    policy_gradient_loss | -0.00075     |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 7.15e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9031680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0108   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9031680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0041966 |\n",
      "|    clip_fraction        | 0.0164    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 6.19      |\n",
      "|    explained_variance   | 0.148     |\n",
      "|    learning_rate        | 0.000148  |\n",
      "|    loss                 | -0.00287  |\n",
      "|    n_updates            | 4470      |\n",
      "|    policy_gradient_loss | -0.000964 |\n",
      "|    std                  | 0.0119    |\n",
      "|    value_loss           | 4.7e-08   |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 9912     |\n",
      "|    total_timesteps | 9031680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9051840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9051840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039325156 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.2          |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.0026      |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.000653    |\n",
      "|    std                  | 0.0119       |\n",
      "|    value_loss           | 9.98e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9072000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9072000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049880287 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.2          |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.000142     |\n",
      "|    loss                 | 0.00079      |\n",
      "|    n_updates            | 4490         |\n",
      "|    policy_gradient_loss | -0.000518    |\n",
      "|    std                  | 0.0118       |\n",
      "|    value_loss           | 7.11e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.011   |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 9956     |\n",
      "|    total_timesteps | 9072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9092160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00893     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9092160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023464235 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.2          |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.000139     |\n",
      "|    loss                 | -0.00116     |\n",
      "|    n_updates            | 4500         |\n",
      "|    policy_gradient_loss | -0.000397    |\n",
      "|    std                  | 0.0118       |\n",
      "|    value_loss           | 3.58e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9112320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0116      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9112320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028969953 |\n",
      "|    clip_fraction        | 0.00311      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.2          |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.000136     |\n",
      "|    loss                 | 0.000413     |\n",
      "|    n_updates            | 4510         |\n",
      "|    policy_gradient_loss | -0.000247    |\n",
      "|    std                  | 0.0118       |\n",
      "|    value_loss           | 4.45e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0109  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 10001    |\n",
      "|    total_timesteps | 9112320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9132480, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9132480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003741757 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.21        |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.000133    |\n",
      "|    loss                 | -0.00188    |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | -0.000714   |\n",
      "|    std                  | 0.0118      |\n",
      "|    value_loss           | 5.32e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9152640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9152640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003051008 |\n",
      "|    clip_fraction        | 0.00641     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.21        |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00013     |\n",
      "|    loss                 | 0.000147    |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.000256   |\n",
      "|    std                  | 0.0118      |\n",
      "|    value_loss           | 5.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 10045    |\n",
      "|    total_timesteps | 9152640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9172800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9172800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002501949 |\n",
      "|    clip_fraction        | 0.00878     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.21        |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.000127    |\n",
      "|    loss                 | -0.000738   |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.000459   |\n",
      "|    std                  | 0.0117      |\n",
      "|    value_loss           | 3.15e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9192960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0103       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 9192960       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8884067e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.21          |\n",
      "|    explained_variance   | 0.175         |\n",
      "|    learning_rate        | 0.000124      |\n",
      "|    loss                 | -1.4e-05      |\n",
      "|    n_updates            | 4550          |\n",
      "|    policy_gradient_loss | 1.29e-05      |\n",
      "|    std                  | 0.0117        |\n",
      "|    value_loss           | 5.98e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 10090    |\n",
      "|    total_timesteps | 9192960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9213120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9213120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023473753 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.000121     |\n",
      "|    loss                 | -0.000691    |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -0.000224    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9233280, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0151      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9233280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029141179 |\n",
      "|    clip_fraction        | 0.00235      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.0901       |\n",
      "|    learning_rate        | 0.000118     |\n",
      "|    loss                 | 0.000207     |\n",
      "|    n_updates            | 4570         |\n",
      "|    policy_gradient_loss | -0.000161    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 9.13e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 10134    |\n",
      "|    total_timesteps | 9233280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9253440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0114     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9253440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003161226 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.22        |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.000115    |\n",
      "|    loss                 | -8.09e-05   |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.000372   |\n",
      "|    std                  | 0.0117      |\n",
      "|    value_loss           | 4.11e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9273600, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0152      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9273600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030450148 |\n",
      "|    clip_fraction        | 0.0041       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.000112     |\n",
      "|    loss                 | -0.000707    |\n",
      "|    n_updates            | 4590         |\n",
      "|    policy_gradient_loss | -0.000127    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 9.53e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 10178    |\n",
      "|    total_timesteps | 9273600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9293760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00981     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9293760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030750951 |\n",
      "|    clip_fraction        | 0.00483      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.000109     |\n",
      "|    loss                 | -0.000607    |\n",
      "|    n_updates            | 4600         |\n",
      "|    policy_gradient_loss | -0.000251    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 4.92e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9313920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0102      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9313920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037056245 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | -0.00132     |\n",
      "|    n_updates            | 4610         |\n",
      "|    policy_gradient_loss | -0.00079     |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 6.09e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 10222    |\n",
      "|    total_timesteps | 9313920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9334080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0129      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9334080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019314347 |\n",
      "|    clip_fraction        | 0.0027       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.185        |\n",
      "|    learning_rate        | 0.000103     |\n",
      "|    loss                 | -0.00046     |\n",
      "|    n_updates            | 4620         |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 5.64e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9354240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9354240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041699777 |\n",
      "|    clip_fraction        | 0.00533      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.22         |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 9.99e-05     |\n",
      "|    loss                 | 0.000486     |\n",
      "|    n_updates            | 4630         |\n",
      "|    policy_gradient_loss | -0.000253    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 7.93e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 10266    |\n",
      "|    total_timesteps | 9354240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9374400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9374400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037148884 |\n",
      "|    clip_fraction        | 0.0077       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 9.69e-05     |\n",
      "|    loss                 | -0.00301     |\n",
      "|    n_updates            | 4640         |\n",
      "|    policy_gradient_loss | -0.000481    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 6.1e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9394560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00983     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9394560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029573003 |\n",
      "|    clip_fraction        | 0.00652      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.0904       |\n",
      "|    learning_rate        | 9.38e-05     |\n",
      "|    loss                 | -0.00105     |\n",
      "|    n_updates            | 4650         |\n",
      "|    policy_gradient_loss | -0.000307    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 7.66e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 10311    |\n",
      "|    total_timesteps | 9394560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9414720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00936     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9414720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019967845 |\n",
      "|    clip_fraction        | 0.00583      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 9.08e-05     |\n",
      "|    loss                 | -0.000473    |\n",
      "|    n_updates            | 4660         |\n",
      "|    policy_gradient_loss | -0.000258    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 6.04e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9434880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9434880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040999223 |\n",
      "|    clip_fraction        | 0.00917      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.0941       |\n",
      "|    learning_rate        | 8.78e-05     |\n",
      "|    loss                 | 0.00018      |\n",
      "|    n_updates            | 4670         |\n",
      "|    policy_gradient_loss | -0.000481    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 10355    |\n",
      "|    total_timesteps | 9434880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9455040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0116      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9455040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033076662 |\n",
      "|    clip_fraction        | 0.00446      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.18         |\n",
      "|    learning_rate        | 8.48e-05     |\n",
      "|    loss                 | 0.00017      |\n",
      "|    n_updates            | 4680         |\n",
      "|    policy_gradient_loss | -0.000283    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 6.02e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9475200, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0154      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9475200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036281592 |\n",
      "|    clip_fraction        | 0.00551      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 8.17e-05     |\n",
      "|    loss                 | 0.000819     |\n",
      "|    n_updates            | 4690         |\n",
      "|    policy_gradient_loss | -0.000285    |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 6.33e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 10399    |\n",
      "|    total_timesteps | 9475200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9495360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0112      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9495360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036161807 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.0999       |\n",
      "|    learning_rate        | 7.87e-05     |\n",
      "|    loss                 | -0.000707    |\n",
      "|    n_updates            | 4700         |\n",
      "|    policy_gradient_loss | -0.000638    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 1.15e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9515520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9515520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001946788 |\n",
      "|    clip_fraction        | 0.000263    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 7.57e-05    |\n",
      "|    loss                 | -5.68e-05   |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | -0.000122   |\n",
      "|    std                  | 0.0116      |\n",
      "|    value_loss           | 3.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 10444    |\n",
      "|    total_timesteps | 9515520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9535680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9535680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030068087 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 7.27e-05     |\n",
      "|    loss                 | -1.54e-06    |\n",
      "|    n_updates            | 4720         |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 5.3e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9555840, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9555840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003139833 |\n",
      "|    clip_fraction        | 0.00363     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 6.96e-05    |\n",
      "|    loss                 | -6.09e-05   |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | -0.00033    |\n",
      "|    std                  | 0.0116      |\n",
      "|    value_loss           | 3.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0106  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 10488    |\n",
      "|    total_timesteps | 9555840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9576000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.018        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 9576000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00077323127 |\n",
      "|    clip_fraction        | 9.92e-06      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.23          |\n",
      "|    explained_variance   | 0.182         |\n",
      "|    learning_rate        | 6.66e-05      |\n",
      "|    loss                 | 0.000123      |\n",
      "|    n_updates            | 4740          |\n",
      "|    policy_gradient_loss | -9.7e-05      |\n",
      "|    std                  | 0.0116        |\n",
      "|    value_loss           | 4.99e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=9596160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9596160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027329973 |\n",
      "|    clip_fraction        | 0.000774     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 6.36e-05     |\n",
      "|    loss                 | -0.000857    |\n",
      "|    n_updates            | 4750         |\n",
      "|    policy_gradient_loss | -0.00031     |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 5.08e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 10532    |\n",
      "|    total_timesteps | 9596160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9616320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00907    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9616320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002014999 |\n",
      "|    clip_fraction        | 0.000213    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.0781      |\n",
      "|    learning_rate        | 6.06e-05    |\n",
      "|    loss                 | -0.000309   |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.000214   |\n",
      "|    std                  | 0.0116      |\n",
      "|    value_loss           | 1.39e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9636480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00975     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9636480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029103328 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 5.76e-05     |\n",
      "|    loss                 | -0.00288     |\n",
      "|    n_updates            | 4770         |\n",
      "|    policy_gradient_loss | -0.000957    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 3.49e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 10577    |\n",
      "|    total_timesteps | 9636480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9656640, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0192      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9656640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036401898 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 5.45e-05     |\n",
      "|    loss                 | -0.00102     |\n",
      "|    n_updates            | 4780         |\n",
      "|    policy_gradient_loss | -0.000728    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 7.19e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9676800, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0162      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9676800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026451717 |\n",
      "|    clip_fraction        | 0.00153      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 5.15e-05     |\n",
      "|    loss                 | -0.00182     |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | -0.000146    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 7.86e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0111  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 10621    |\n",
      "|    total_timesteps | 9676800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9696960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00914     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9696960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034091459 |\n",
      "|    clip_fraction        | 0.00282      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.151        |\n",
      "|    learning_rate        | 4.85e-05     |\n",
      "|    loss                 | -0.00196     |\n",
      "|    n_updates            | 4800         |\n",
      "|    policy_gradient_loss | -0.000545    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 7.83e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9717120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9717120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024482808 |\n",
      "|    clip_fraction        | 0.00109      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.23         |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 4.55e-05     |\n",
      "|    loss                 | 0.000164     |\n",
      "|    n_updates            | 4810         |\n",
      "|    policy_gradient_loss | -0.000149    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 7.92e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 10665    |\n",
      "|    total_timesteps | 9717120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9737280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9737280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032859682 |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 4.24e-05     |\n",
      "|    loss                 | 0.00036      |\n",
      "|    n_updates            | 4820         |\n",
      "|    policy_gradient_loss | -0.000548    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 5.76e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9757440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00948     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9757440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006717282 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 3.94e-05     |\n",
      "|    loss                 | -0.000496    |\n",
      "|    n_updates            | 4830         |\n",
      "|    policy_gradient_loss | -0.000107    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 7.12e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0112  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 10710    |\n",
      "|    total_timesteps | 9757440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9777600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0102      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9777600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.380823e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 3.64e-05     |\n",
      "|    loss                 | -0.000221    |\n",
      "|    n_updates            | 4840         |\n",
      "|    policy_gradient_loss | 8.02e-06     |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 2.64e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9797760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9797760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010197897 |\n",
      "|    clip_fraction        | 9.92e-06     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.0789       |\n",
      "|    learning_rate        | 3.34e-05     |\n",
      "|    loss                 | 0.000133     |\n",
      "|    n_updates            | 4850         |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 1.6e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 10754    |\n",
      "|    total_timesteps | 9797760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9817920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00935   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9817920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00238533 |\n",
      "|    clip_fraction        | 0.0006     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.24       |\n",
      "|    explained_variance   | 0.12       |\n",
      "|    learning_rate        | 3.03e-05   |\n",
      "|    loss                 | -0.00107   |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.000486  |\n",
      "|    std                  | 0.0116     |\n",
      "|    value_loss           | 7.88e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9838080, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0162      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9838080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032356738 |\n",
      "|    clip_fraction        | 0.00228      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 2.73e-05     |\n",
      "|    loss                 | -0.00157     |\n",
      "|    n_updates            | 4870         |\n",
      "|    policy_gradient_loss | -0.000937    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 8.84e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0108  |\n",
      "| time/              |          |\n",
      "|    fps             | 910      |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 10799    |\n",
      "|    total_timesteps | 9838080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9858240, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0148      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9858240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011902122 |\n",
      "|    clip_fraction        | 9.92e-06     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 2.43e-05     |\n",
      "|    loss                 | 0.000603     |\n",
      "|    n_updates            | 4880         |\n",
      "|    policy_gradient_loss | -0.000124    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 4.15e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9878400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0121       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 9878400       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051279424 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.24          |\n",
      "|    explained_variance   | 0.2           |\n",
      "|    learning_rate        | 2.13e-05      |\n",
      "|    loss                 | -0.000963     |\n",
      "|    n_updates            | 4890          |\n",
      "|    policy_gradient_loss | -0.000117     |\n",
      "|    std                  | 0.0116        |\n",
      "|    value_loss           | 4.02e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 910      |\n",
      "|    iterations      | 490      |\n",
      "|    time_elapsed    | 10843    |\n",
      "|    total_timesteps | 9878400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9898560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0115      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9898560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.020328e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 1.82e-05     |\n",
      "|    loss                 | -0.000107    |\n",
      "|    n_updates            | 4900         |\n",
      "|    policy_gradient_loss | -1.99e-05    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 7.29e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9918720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00897     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9918720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004623681 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 1.52e-05     |\n",
      "|    loss                 | -0.000358    |\n",
      "|    n_updates            | 4910         |\n",
      "|    policy_gradient_loss | -7.94e-05    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 6.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 910      |\n",
      "|    iterations      | 492      |\n",
      "|    time_elapsed    | 10894    |\n",
      "|    total_timesteps | 9918720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9938880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0104       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 9938880       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010696988 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.24          |\n",
      "|    explained_variance   | 0.182         |\n",
      "|    learning_rate        | 1.22e-05      |\n",
      "|    loss                 | 9.44e-06      |\n",
      "|    n_updates            | 4920          |\n",
      "|    policy_gradient_loss | -2.82e-05     |\n",
      "|    std                  | 0.0116        |\n",
      "|    value_loss           | 7.44e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=9959040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.01        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9959040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.879989e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.24         |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 9.17e-06     |\n",
      "|    loss                 | -1.44e-05    |\n",
      "|    n_updates            | 4930         |\n",
      "|    policy_gradient_loss | -1.91e-05    |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 4.26e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0109  |\n",
      "| time/              |          |\n",
      "|    fps             | 909      |\n",
      "|    iterations      | 494      |\n",
      "|    time_elapsed    | 10944    |\n",
      "|    total_timesteps | 9959040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9979200, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0167       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 9979200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0183375e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.24          |\n",
      "|    explained_variance   | 0.156         |\n",
      "|    learning_rate        | 6.14e-06      |\n",
      "|    loss                 | -8.01e-05     |\n",
      "|    n_updates            | 4940          |\n",
      "|    policy_gradient_loss | -3.87e-05     |\n",
      "|    std                  | 0.0116        |\n",
      "|    value_loss           | 4.86e-08      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=9999360, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0187       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 9999360       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9979557e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.24          |\n",
      "|    explained_variance   | 0.157         |\n",
      "|    learning_rate        | 3.12e-06      |\n",
      "|    loss                 | -0.000272     |\n",
      "|    n_updates            | 4950          |\n",
      "|    policy_gradient_loss | -5.25e-05     |\n",
      "|    std                  | 0.0116        |\n",
      "|    value_loss           | 4.53e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 909      |\n",
      "|    iterations      | 496      |\n",
      "|    time_elapsed    | 10988    |\n",
      "|    total_timesteps | 9999360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10019520, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0137       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10019520      |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5965532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 6.24          |\n",
      "|    explained_variance   | 0.0914        |\n",
      "|    learning_rate        | 9.6e-08       |\n",
      "|    loss                 | -1.15e-06     |\n",
      "|    n_updates            | 4960          |\n",
      "|    policy_gradient_loss | -1.1e-07      |\n",
      "|    std                  | 0.0116        |\n",
      "|    value_loss           | 1e-07         |\n",
      "-------------------------------------------\n",
      "Using cpu device\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5040 is out of bounds for axis 0 with size 5040",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m policy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(activation_fn\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLeakyReLU,\n\u001b[0;32m     80\u001b[0m                      net_arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(pi\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m36\u001b[39m,\u001b[38;5;241m18\u001b[39m], vf\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m36\u001b[39m,\u001b[38;5;241m18\u001b[39m], optimizers_class \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam, log_std_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.005\u001b[39m)) \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     81\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, envs, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m252\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mlinear_schedule(\u001b[38;5;241m0.0015\u001b[39m), policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m252\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m, normalize_advantage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m---> 83\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e7\u001b[39m, log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, callback\u001b[38;5;241m=\u001b[39meval_callback)\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    312\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    313\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    314\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[0;32m    315\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[0;32m    316\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    317\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m    318\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer, n_rollout_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(clipped_actions)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv\u001b[38;5;241m.\u001b[39mstep_wait()\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[0;32m     61\u001b[0m         )\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\osc16\\cva_risk_management_thesis\\PPO\\dev_env.py:308\u001b[0m, in \u001b[0;36mtradingEng.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    304\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m#print(reward)\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# End the environment after we reach year 9\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrpth\u001b[38;5;241m.\u001b[39mt_s[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtIDX \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresetDate \n\u001b[0;32m    309\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5040 is out of bounds for axis 0 with size 5040"
     ]
    }
   ],
   "source": [
    "# Horizons\n",
    "\n",
    "# Default\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]))\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model',\n",
    "    log_path='./logs/eval_logs/',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# 1 yr\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs', resetdate=1.0),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs',resetdate=1.0)\n",
    "]))\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs',resetdate=1.0),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model1',\n",
    "    log_path='./logs/eval_logs1',\n",
    "    eval_freq=252*8*1,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*1, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*1, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# 20 yr\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs',resetdate=20.0),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs',resetdate=20.0)\n",
    "]))\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs',resetdate=20.0),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model20',\n",
    "    log_path='./logs/eval_logs20',\n",
    "    eval_freq=252*8*20,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*20, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*20, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.26e+03 |\n",
      "|    mean_reward     | -0.121   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40320, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0907      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031787264 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -0.412       |\n",
      "|    learning_rate        | 0.0015       |\n",
      "|    loss                 | -0.00109     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000844    |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.000484     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.485   |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0776      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019190868 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.0215       |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | -0.000855    |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000728    |\n",
      "|    std                  | 0.952        |\n",
      "|    value_loss           | 0.000217     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80640, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0829     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005951937 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.0822      |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.0042     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 9.65e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.423   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0764     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006910512 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.0321      |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00449    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    std                  | 0.879       |\n",
      "|    value_loss           | 4.09e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120960, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0823      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050300364 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.53        |\n",
      "|    explained_variance   | 0.092        |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00519     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 0.845        |\n",
      "|    value_loss           | 4.7e-06      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.403   |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.106       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 141120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043805027 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.44        |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.00148      |\n",
      "|    loss                 | -0.00287     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 4.78e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0945     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012972985 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.0218      |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    std                  | 0.754       |\n",
      "|    value_loss           | 4.83e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.343   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006526979 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.2        |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00148     |\n",
      "|    loss                 | -0.00745    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 1.5e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0935     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004002101 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.00147     |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 5.94e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.329   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 347      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 221760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063189953 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.03        |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 0.00147      |\n",
      "|    loss                 | -0.00402     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    std                  | 0.66         |\n",
      "|    value_loss           | 1.76e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.079       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 241920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055425153 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.0894       |\n",
      "|    learning_rate        | 0.00147      |\n",
      "|    loss                 | -0.00576     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    std                  | 0.627        |\n",
      "|    value_loss           | 2.37e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.303   |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 415      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.079      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007329678 |\n",
      "|    clip_fraction        | 0.0251      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.00761    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    std                  | 0.595       |\n",
      "|    value_loss           | 1.22e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0872     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008182604 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.0889      |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.00881    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0056     |\n",
      "|    std                  | 0.562       |\n",
      "|    value_loss           | 5.89e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.293   |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 490      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 302400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049047805 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | -0.00198     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    std                  | 0.543        |\n",
      "|    value_loss           | 8.78e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0791     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003893035 |\n",
      "|    clip_fraction        | 0.0313      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | -0.00403    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    std                  | 0.522       |\n",
      "|    value_loss           | 3.28e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.266   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 556      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0832      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 342720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054094177 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.38         |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.00689     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00367     |\n",
      "|    std                  | 0.499        |\n",
      "|    value_loss           | 1.06e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.119       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 362880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095008835 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.284        |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.00905     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00573     |\n",
      "|    std                  | 0.467        |\n",
      "|    value_loss           | 5.59e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.252   |\n",
      "| time/              |          |\n",
      "|    fps             | 583      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 622      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 383040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056056827 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.00145      |\n",
      "|    loss                 | -0.00348     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 3.6e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.114       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 403200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022514672 |\n",
      "|    clip_fraction        | 0.00359      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.0821       |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00148     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000685    |\n",
      "|    std                  | 0.437        |\n",
      "|    value_loss           | 1.43e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.225   |\n",
      "| time/              |          |\n",
      "|    fps             | 589      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 684      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0889     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008844396 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | -0.00944    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    std                  | 0.41        |\n",
      "|    value_loss           | 1.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0844      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035218315 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00415     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    std                  | 0.394        |\n",
      "|    value_loss           | 2.3e-06      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.208   |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0768     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004931628 |\n",
      "|    clip_fraction        | 0.0267      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.866      |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00482    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 7.86e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007650858 |\n",
      "|    clip_fraction        | 0.0266      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.758      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00692    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    std                  | 0.355       |\n",
      "|    value_loss           | 5.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.197   |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 806      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0776     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002911021 |\n",
      "|    clip_fraction        | 0.0213      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | -0.00307    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 1.61e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0767     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005144147 |\n",
      "|    clip_fraction        | 0.0327      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.00532    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    std                  | 0.334       |\n",
      "|    value_loss           | 1.63e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.186   |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 864      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0999      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064043403 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.00615     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.321        |\n",
      "|    value_loss           | 1.4e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.102       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075729685 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.264        |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.00368     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 0.308        |\n",
      "|    value_loss           | 9.03e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.171   |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 924      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0739     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010930212 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00828    |\n",
      "|    std                  | 0.291       |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=604800, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0682      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056592003 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.00141      |\n",
      "|    loss                 | -0.00287     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 0.281        |\n",
      "|    value_loss           | 9.15e-07     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.159   |\n",
      "| time/              |          |\n",
      "|    fps             | 615      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 983      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0726     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008111204 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.159      |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.00324    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    std                  | 0.27        |\n",
      "|    value_loss           | 2.94e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0832     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008498051 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0738     |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.00231    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00259    |\n",
      "|    std                  | 0.259       |\n",
      "|    value_loss           | 2.29e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.144   |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 1039     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.067      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008512514 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0196      |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | -0.00625    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    std                  | 0.247       |\n",
      "|    value_loss           | 1.87e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=685440, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0918      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 685440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037901162 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0898       |\n",
      "|    explained_variance   | 0.329        |\n",
      "|    learning_rate        | 0.0014       |\n",
      "|    loss                 | -0.00261     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    std                  | 0.242        |\n",
      "|    value_loss           | 5.94e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.135   |\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 1096     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009977078 |\n",
      "|    clip_fraction        | 0.0952      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.152       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | -0.00484    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    std                  | 0.232       |\n",
      "|    value_loss           | 5.24e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.071     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 725760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01253546 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.234      |\n",
      "|    explained_variance   | 0.225      |\n",
      "|    learning_rate        | 0.00139    |\n",
      "|    loss                 | -0.00116   |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | 7.98e-05   |\n",
      "|    std                  | 0.223      |\n",
      "|    value_loss           | 9.77e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.126   |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 1153     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.085       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 745920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063148374 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.3          |\n",
      "|    explained_variance   | -0.123       |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00226     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.218        |\n",
      "|    value_loss           | 9.19e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0645      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 766080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033107782 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.343        |\n",
      "|    explained_variance   | -0.00732     |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.000202    |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000908    |\n",
      "|    std                  | 0.214        |\n",
      "|    value_loss           | 1.57e-06     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.125   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1209     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0603     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005716452 |\n",
      "|    clip_fraction        | 0.0639      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.396       |\n",
      "|    explained_variance   | 0.07        |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.00421    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    std                  | 0.207       |\n",
      "|    value_loss           | 3.39e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=806400, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0928     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005593057 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.457       |\n",
      "|    explained_variance   | 0.0744      |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00496    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    std                  | 0.202       |\n",
      "|    value_loss           | 3.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.119   |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1266     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0557     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007877171 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.501       |\n",
      "|    explained_variance   | 0.0548      |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.0058     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    std                  | 0.198       |\n",
      "|    value_loss           | 4.09e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=846720, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.083      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006424715 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.554       |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | -0.00228    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.000493    |\n",
      "|    std                  | 0.193       |\n",
      "|    value_loss           | 2.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.113   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1322     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0837      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 866880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058957245 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.606        |\n",
      "|    explained_variance   | 0.036        |\n",
      "|    learning_rate        | 0.00137      |\n",
      "|    loss                 | -0.00456     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    std                  | 0.188        |\n",
      "|    value_loss           | 2.39e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0649     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015331218 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.666       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | 0.0054      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.00061     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 1.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1378     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.061     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 907200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00469732 |\n",
      "|    clip_fraction        | 0.0758     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.708      |\n",
      "|    explained_variance   | 0.115      |\n",
      "|    learning_rate        | 0.00137    |\n",
      "|    loss                 | -0.00304   |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.00115   |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 7.08e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0482     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010880144 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.75        |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | -0.00471    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 9.83e-06    |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 1.77e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1435     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0854     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008983798 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.796       |\n",
      "|    explained_variance   | 0.0401      |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | -0.00205    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00302     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 1.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0571     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032147534 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.827       |\n",
      "|    explained_variance   | -0.22       |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.00863     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 4.23e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0995  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1492     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0631     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008907802 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.846       |\n",
      "|    explained_variance   | -0.9        |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | 0.00642     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.00747     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 2.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.053      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003755854 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.867       |\n",
      "|    explained_variance   | 0.0179      |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.00299    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.00313     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 9.17e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0938  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1549     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0759     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017655613 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.906       |\n",
      "|    explained_variance   | 0.0602      |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | 0.0035      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | 0.0029      |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 3.59e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0841     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007952915 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.939       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.00243    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00265     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0913  |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1605     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0583     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1068480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004412564 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.968       |\n",
      "|    explained_variance   | -0.0587     |\n",
      "|    learning_rate        | 0.00134     |\n",
      "|    loss                 | -0.00175    |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | 0.00919     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 1.6e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0496      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1088640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063183806 |\n",
      "|    clip_fraction        | 0.085        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1            |\n",
      "|    explained_variance   | 0.0629       |\n",
      "|    learning_rate        | 0.00134      |\n",
      "|    loss                 | -0.00275     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 0.000115     |\n",
      "|    std                  | 0.155        |\n",
      "|    value_loss           | 6.39e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0936  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 1662     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0822    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1108800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01812367 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.03       |\n",
      "|    explained_variance   | 0.157      |\n",
      "|    learning_rate        | 0.00134    |\n",
      "|    loss                 | -0.00208   |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | 0.00373    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 3.42e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0493      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1128960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074312873 |\n",
      "|    clip_fraction        | 0.0851       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.06         |\n",
      "|    explained_variance   | 0.178        |\n",
      "|    learning_rate        | 0.00133      |\n",
      "|    loss                 | -0.00677     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    std                  | 0.15         |\n",
      "|    value_loss           | 1.41e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0981  |\n",
      "| time/              |          |\n",
      "|    fps             | 656      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1718     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0666     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016223729 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.1         |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.00133     |\n",
      "|    loss                 | 0.00703     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | 0.000427    |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 1.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0749    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1169280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03168501 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.12       |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.00133    |\n",
      "|    loss                 | 0.00981    |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | 0.00977    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 4.92e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 658      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1774     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0634    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1189440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03306315 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.14       |\n",
      "|    explained_variance   | 0.139      |\n",
      "|    learning_rate        | 0.00132    |\n",
      "|    loss                 | 0.00913    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | 0.00216    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 4.28e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0805     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011661022 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.16        |\n",
      "|    explained_variance   | 0.0849      |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0023     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00455     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 4.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.098   |\n",
      "| time/              |          |\n",
      "|    fps             | 660      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 1831     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0793     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010568563 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.18        |\n",
      "|    explained_variance   | 0.0293      |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00168    |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | 0.00746     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 1.48e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0967     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013211729 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.2         |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00453     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 1.28e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0935  |\n",
      "| time/              |          |\n",
      "|    fps             | 662      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 1887     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0615     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012202023 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.23        |\n",
      "|    explained_variance   | 0.0269      |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.00121    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | 0.00879     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 2.68e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0844     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012383353 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.26        |\n",
      "|    explained_variance   | 0.0159      |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | 0.00157     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 1.33e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0902  |\n",
      "| time/              |          |\n",
      "|    fps             | 663      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1943     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0624     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009810543 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.29        |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.00112    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | 0.000689    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 3.13e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0762     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030559342 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.31        |\n",
      "|    explained_variance   | 0.0275      |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00585     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 7.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0855  |\n",
      "| time/              |          |\n",
      "|    fps             | 665      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1999     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0583     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008955701 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.35        |\n",
      "|    explained_variance   | -0.366      |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | -0.00183    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.000653    |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 4.45e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0615     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013071655 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.38        |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | -0.00134    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | 0.00431     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 1.82e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.083   |\n",
      "| time/              |          |\n",
      "|    fps             | 666      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 2055     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0654     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013889633 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | 0.00139     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | 0.0074      |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 9.74e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0607     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011129076 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.42        |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | 0.00426     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00591     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 1.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0844  |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 2112     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.058      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014234037 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.45        |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | 0.0017      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | 0.00429     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 2.07e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0532     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020270253 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.47        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | 0.00544     |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.00182     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 2.53e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0846  |\n",
      "| time/              |          |\n",
      "|    fps             | 669      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 2169     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0546    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1471680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00851072 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.5        |\n",
      "|    explained_variance   | 0.201      |\n",
      "|    learning_rate        | 0.00128    |\n",
      "|    loss                 | 0.00364    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | 0.00115    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 2.89e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0592     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017088458 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.52        |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | 0.00587     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.000759    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 3.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0838  |\n",
      "| time/              |          |\n",
      "|    fps             | 670      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 2225     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.073     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1512000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00570016 |\n",
      "|    clip_fraction        | 0.0927     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.53       |\n",
      "|    explained_variance   | 0.173      |\n",
      "|    learning_rate        | 0.00128    |\n",
      "|    loss                 | -0.00106   |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | 0.000937   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 1.68e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0736     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014024579 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.57        |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | -0.000846   |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00362     |\n",
      "|    std                  | 0.117       |\n",
      "|    value_loss           | 1.31e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0846  |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 2281     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0536      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1552320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075736963 |\n",
      "|    clip_fraction        | 0.223        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.6          |\n",
      "|    explained_variance   | 0.266        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | -0.000832    |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | 0.00675      |\n",
      "|    std                  | 0.115        |\n",
      "|    value_loss           | 4.28e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0586     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013522148 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.63        |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 0.00931     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.00591     |\n",
      "|    std                  | 0.113       |\n",
      "|    value_loss           | 6.85e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0818  |\n",
      "| time/              |          |\n",
      "|    fps             | 672      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 2337     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0668     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024902925 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.66        |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.0191      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | 0.00126     |\n",
      "|    std                  | 0.112       |\n",
      "|    value_loss           | 6.73e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0756     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025579957 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.69        |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.00296     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.00645     |\n",
      "|    std                  | 0.111       |\n",
      "|    value_loss           | 1.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.079   |\n",
      "| time/              |          |\n",
      "|    fps             | 673      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 2394     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0817     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013587239 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.7         |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.0149      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    std                  | 0.11        |\n",
      "|    value_loss           | 4.36e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0691     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027580108 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.72        |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | 0.00488     |\n",
      "|    std                  | 0.109       |\n",
      "|    value_loss           | 1.18e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0757  |\n",
      "| time/              |          |\n",
      "|    fps             | 674      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 2449     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0569     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011345259 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.75        |\n",
      "|    explained_variance   | 0.066       |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.00152    |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | 0.0018      |\n",
      "|    std                  | 0.107       |\n",
      "|    value_loss           | 4.33e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0543      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1693440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064790905 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.79         |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.00125      |\n",
      "|    loss                 | 0.00343      |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.00545      |\n",
      "|    std                  | 0.104        |\n",
      "|    value_loss           | 6.22e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0724  |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 2505     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0477     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009135438 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.83        |\n",
      "|    explained_variance   | 0.0803      |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.00304    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | 0.00306     |\n",
      "|    std                  | 0.102       |\n",
      "|    value_loss           | 8.17e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1733760, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0572    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1733760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03993974 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.85       |\n",
      "|    explained_variance   | 0.0879     |\n",
      "|    learning_rate        | 0.00124    |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | 0.00854    |\n",
      "|    std                  | 0.102      |\n",
      "|    value_loss           | 2.25e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0719  |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 2562     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0892     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1753920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028769376 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.87        |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | 0.0199      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.00655     |\n",
      "|    std                  | 0.101       |\n",
      "|    value_loss           | 1.61e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018271044 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.89        |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | -0.000426   |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.00313     |\n",
      "|    std                  | 0.0997      |\n",
      "|    value_loss           | 9.69e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0724  |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 2618     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0715     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1794240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024570264 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.91        |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | 0.00352     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | 0.00387     |\n",
      "|    std                  | 0.0986      |\n",
      "|    value_loss           | 9.2e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0517     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034395285 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.93        |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | -0.000871   |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.0978      |\n",
      "|    value_loss           | 1.55e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0734  |\n",
      "| time/              |          |\n",
      "|    fps             | 678      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 2675     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0601     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007955702 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.94        |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | 0.000213    |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.000316   |\n",
      "|    std                  | 0.0977      |\n",
      "|    value_loss           | 2.58e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0828      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1854720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063098413 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.94         |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.00122      |\n",
      "|    loss                 | -0.00278     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | 0.000945     |\n",
      "|    std                  | 0.0973       |\n",
      "|    value_loss           | 1.44e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0743  |\n",
      "| time/              |          |\n",
      "|    fps             | 678      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 2732     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.052      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011827961 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.95        |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.000664    |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | 0.00591     |\n",
      "|    std                  | 0.0969      |\n",
      "|    value_loss           | 2.41e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0558    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1895040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00958853 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.97       |\n",
      "|    explained_variance   | 0.101      |\n",
      "|    learning_rate        | 0.00122    |\n",
      "|    loss                 | 0.00212    |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | 0.00176    |\n",
      "|    std                  | 0.0957     |\n",
      "|    value_loss           | 1.05e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0735  |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 2787     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0501     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014571325 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2           |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -0.00302    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | 0.00794     |\n",
      "|    std                  | 0.0944      |\n",
      "|    value_loss           | 6.21e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0524     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012566477 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.02        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | 0.000559    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    std                  | 0.0941      |\n",
      "|    value_loss           | 1.9e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0757  |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 2843     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0604     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013437527 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.03        |\n",
      "|    explained_variance   | 0.0865      |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | 0.000439    |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.0054      |\n",
      "|    std                  | 0.0929      |\n",
      "|    value_loss           | 5.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0433     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062879 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.06        |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00121     |\n",
      "|    loss                 | -0.000684   |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.00388     |\n",
      "|    std                  | 0.0921      |\n",
      "|    value_loss           | 1.58e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0724  |\n",
      "| time/              |          |\n",
      "|    fps             | 681      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 2899     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0612    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1995840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00994909 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.09       |\n",
      "|    explained_variance   | 0.165      |\n",
      "|    learning_rate        | 0.0012     |\n",
      "|    loss                 | 0.000175   |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | 0.00161    |\n",
      "|    std                  | 0.0905     |\n",
      "|    value_loss           | 6.03e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.042      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012200506 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.11        |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | 0.00793     |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00463     |\n",
      "|    std                  | 0.0898      |\n",
      "|    value_loss           | 1.57e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0734  |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 2954     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.053     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2036160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05686942 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.13       |\n",
      "|    explained_variance   | 0.173      |\n",
      "|    learning_rate        | 0.0012     |\n",
      "|    loss                 | 0.00744    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | 0.00545    |\n",
      "|    std                  | 0.0888     |\n",
      "|    value_loss           | 1.62e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0659    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2056320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00979004 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.15       |\n",
      "|    explained_variance   | 0.236      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | -0.000757  |\n",
      "|    n_updates            | 1010       |\n",
      "|    policy_gradient_loss | 0.00266    |\n",
      "|    std                  | 0.0879     |\n",
      "|    value_loss           | 2.09e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0746  |\n",
      "| time/              |          |\n",
      "|    fps             | 682      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 3011     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0681     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008079606 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.17        |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.000766   |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | 0.000125    |\n",
      "|    std                  | 0.0873      |\n",
      "|    value_loss           | 3.43e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0538     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038832977 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.19        |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00537     |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.00759     |\n",
      "|    std                  | 0.0863      |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0738  |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 3067     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0572    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2116800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04045082 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.2        |\n",
      "|    explained_variance   | 0.354      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | 0.029      |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | 0.0265     |\n",
      "|    std                  | 0.0859     |\n",
      "|    value_loss           | 5.93e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0527     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013878858 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.21        |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.0858      |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0716  |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 3124     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0593      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073182355 |\n",
      "|    clip_fraction        | 0.207        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.22         |\n",
      "|    explained_variance   | 0.332        |\n",
      "|    learning_rate        | 0.00118      |\n",
      "|    loss                 | 5.9e-05      |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | 0.00931      |\n",
      "|    std                  | 0.0855       |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0514     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011615638 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.23        |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.00236     |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.00205     |\n",
      "|    std                  | 0.0848      |\n",
      "|    value_loss           | 2.49e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0707  |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 3180     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0798    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2197440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05085846 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.25       |\n",
      "|    explained_variance   | 0.301      |\n",
      "|    learning_rate        | 0.00117    |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | 0.0145     |\n",
      "|    std                  | 0.0836     |\n",
      "|    value_loss           | 1.29e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0715     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008871533 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.28        |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | 0.000882    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.00279     |\n",
      "|    std                  | 0.0822      |\n",
      "|    value_loss           | 1.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0722  |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 3236     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.057       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2237760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074811834 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.3          |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 0.00117      |\n",
      "|    loss                 | -0.00347     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | 0.00215      |\n",
      "|    std                  | 0.0817       |\n",
      "|    value_loss           | 2.5e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0672     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009209714 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.32        |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | 0.00124     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | 0.00233     |\n",
      "|    std                  | 0.0805      |\n",
      "|    value_loss           | 1.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0776  |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 3292     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0504      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2278080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040248157 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.35         |\n",
      "|    explained_variance   | 0.333        |\n",
      "|    learning_rate        | 0.00116      |\n",
      "|    loss                 | -0.00119     |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | 0.00115      |\n",
      "|    std                  | 0.0796       |\n",
      "|    value_loss           | 2.68e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0516      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071438104 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.37         |\n",
      "|    explained_variance   | 0.381        |\n",
      "|    learning_rate        | 0.00116      |\n",
      "|    loss                 | 0.00126      |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | 0.00752      |\n",
      "|    std                  | 0.0788       |\n",
      "|    value_loss           | 6.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0765  |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 3348     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0529     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009207162 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.39        |\n",
      "|    explained_variance   | 0.0875      |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | 0.00619     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | 0.00146     |\n",
      "|    std                  | 0.0782      |\n",
      "|    value_loss           | 1.13e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0806    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2338560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03467597 |\n",
      "|    clip_fraction        | 0.224      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.41       |\n",
      "|    explained_variance   | 0.366      |\n",
      "|    learning_rate        | 0.00115    |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 1150       |\n",
      "|    policy_gradient_loss | 0.00676    |\n",
      "|    std                  | 0.0776     |\n",
      "|    value_loss           | 1.41e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0754  |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 3405     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0693     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022186082 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.42        |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.00754     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | 0.00631     |\n",
      "|    std                  | 0.0769      |\n",
      "|    value_loss           | 1.59e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0906    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2378880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10308779 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.44       |\n",
      "|    explained_variance   | 0.142      |\n",
      "|    learning_rate        | 0.00115    |\n",
      "|    loss                 | 0.00341    |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | 0.013      |\n",
      "|    std                  | 0.0766     |\n",
      "|    value_loss           | 7.83e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0743  |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 3461     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0518    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2399040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03814884 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.45       |\n",
      "|    explained_variance   | 0.306      |\n",
      "|    learning_rate        | 0.00114    |\n",
      "|    loss                 | 0.00383    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | 0.0208     |\n",
      "|    std                  | 0.0761     |\n",
      "|    value_loss           | 5.98e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0882     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014517214 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.46        |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.00262     |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.0754      |\n",
      "|    value_loss           | 1.25e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0749  |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 3518     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0609     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032473046 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.48        |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | -0.000445   |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | 0.00501     |\n",
      "|    std                  | 0.0749      |\n",
      "|    value_loss           | 2.21e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0626     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098427854 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.48        |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.0243      |\n",
      "|    std                  | 0.0753      |\n",
      "|    value_loss           | 1.6e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.073   |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 3574     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0606     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012992236 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.48        |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | -0.00171    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    std                  | 0.0748      |\n",
      "|    value_loss           | 1.66e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0611     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027176224 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.5         |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 0.00658     |\n",
      "|    std                  | 0.0746      |\n",
      "|    value_loss           | 8.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0723  |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 3631     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0857     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021499958 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.51        |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | -4.26e-05   |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | 0.00709     |\n",
      "|    std                  | 0.0743      |\n",
      "|    value_loss           | 9.73e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0587     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2540160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021294544 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.52        |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.0168      |\n",
      "|    std                  | 0.0736      |\n",
      "|    value_loss           | 3.6e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0699  |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 3687     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0753     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023934413 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.53        |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.00946     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    std                  | 0.0732      |\n",
      "|    value_loss           | 9.74e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0797    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2580480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06365163 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.54       |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.00112    |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 1270       |\n",
      "|    policy_gradient_loss | 0.00541    |\n",
      "|    std                  | 0.073      |\n",
      "|    value_loss           | 6.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0731  |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 3743     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.05       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014074711 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.55        |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | 0.00646     |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | 0.00308     |\n",
      "|    std                  | 0.0726      |\n",
      "|    value_loss           | 2.13e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0963     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057832785 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.57        |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | 0.00395     |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | 0.00868     |\n",
      "|    std                  | 0.0721      |\n",
      "|    value_loss           | 1.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0761  |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 3798     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.13 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.133      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046029326 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.58        |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | -0.00187    |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | 0.0202      |\n",
      "|    std                  | 0.0718      |\n",
      "|    value_loss           | 9.06e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0677     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020301044 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.58        |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | 0.000221    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | 0.0071      |\n",
      "|    std                  | 0.0716      |\n",
      "|    value_loss           | 9.66e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0826  |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 3854     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0918     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2681280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019047344 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.59        |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.0011      |\n",
      "|    loss                 | 0.00988     |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | 0.00739     |\n",
      "|    std                  | 0.0717      |\n",
      "|    value_loss           | 1.26e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0659    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2701440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09827958 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.59       |\n",
      "|    explained_variance   | 0.242      |\n",
      "|    learning_rate        | 0.0011     |\n",
      "|    loss                 | 0.00149    |\n",
      "|    n_updates            | 1330       |\n",
      "|    policy_gradient_loss | 0.0131     |\n",
      "|    std                  | 0.0713     |\n",
      "|    value_loss           | 1.05e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0844  |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 3910     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0739     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012184707 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.6         |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.00663     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | 0.0061      |\n",
      "|    std                  | 0.0707      |\n",
      "|    value_loss           | 8.1e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.064     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2741760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00598804 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.62       |\n",
      "|    explained_variance   | 0.187      |\n",
      "|    learning_rate        | 0.00109    |\n",
      "|    loss                 | -0.000125  |\n",
      "|    n_updates            | 1350       |\n",
      "|    policy_gradient_loss | 6.71e-05   |\n",
      "|    std                  | 0.0705     |\n",
      "|    value_loss           | 3.5e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0852  |\n",
      "| time/              |          |\n",
      "|    fps             | 691      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 3965     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0841    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2761920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02366026 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.62       |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.00109    |\n",
      "|    loss                 | 0.00687    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | 0.00452    |\n",
      "|    std                  | 0.0703     |\n",
      "|    value_loss           | 1.12e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0744     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016284537 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.64        |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.00759     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | 0.00512     |\n",
      "|    std                  | 0.0691      |\n",
      "|    value_loss           | 1.26e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0837  |\n",
      "| time/              |          |\n",
      "|    fps             | 691      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 4021     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0666      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053085675 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.67         |\n",
      "|    explained_variance   | 0.282        |\n",
      "|    learning_rate        | 0.00108      |\n",
      "|    loss                 | -0.00157     |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | 0.00151      |\n",
      "|    std                  | 0.068        |\n",
      "|    value_loss           | 1.66e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0994     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018944165 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.7         |\n",
      "|    explained_variance   | 0.0665      |\n",
      "|    learning_rate        | 0.00108     |\n",
      "|    loss                 | 0.00145     |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | 0.00169     |\n",
      "|    std                  | 0.0672      |\n",
      "|    value_loss           | 5.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0801  |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 4077     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0661     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2842560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011041086 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.72        |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.00108     |\n",
      "|    loss                 | -0.00273    |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    std                  | 0.0666      |\n",
      "|    value_loss           | 5.66e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0863    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2862720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03618831 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.74       |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | 0.00663    |\n",
      "|    std                  | 0.066      |\n",
      "|    value_loss           | 1.2e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0803  |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 4132     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0868     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2882880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055227496 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.75        |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.00107     |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | 0.00847     |\n",
      "|    std                  | 0.0658      |\n",
      "|    value_loss           | 8.13e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0658     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2903040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006515673 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.76        |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00107     |\n",
      "|    loss                 | 0.000793    |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | 0.00724     |\n",
      "|    std                  | 0.0653      |\n",
      "|    value_loss           | 7.39e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0794  |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 4189     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0859     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018230274 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.78        |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.0076      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    std                  | 0.0644      |\n",
      "|    value_loss           | 8.38e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.116      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2943360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012532697 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.8         |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.000102    |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | 0.00745     |\n",
      "|    std                  | 0.0639      |\n",
      "|    value_loss           | 8.45e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0828  |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 4245     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0704    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2963520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00837689 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.8        |\n",
      "|    explained_variance   | 0.199      |\n",
      "|    learning_rate        | 0.00106    |\n",
      "|    loss                 | 0.00439    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | 0.0129     |\n",
      "|    std                  | 0.0639     |\n",
      "|    value_loss           | 1.25e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0732     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2983680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013566762 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.82        |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00106     |\n",
      "|    loss                 | 0.00224     |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.00423     |\n",
      "|    std                  | 0.0635      |\n",
      "|    value_loss           | 7.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0798  |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 4301     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0761     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3003840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017014263 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.82        |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.00932     |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | 0.00413     |\n",
      "|    std                  | 0.0631      |\n",
      "|    value_loss           | 4.86e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3024000, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.11       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008962398 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.84        |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.0038      |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | 0.000173    |\n",
      "|    std                  | 0.063       |\n",
      "|    value_loss           | 3.95e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0809  |\n",
      "| time/              |          |\n",
      "|    fps             | 694      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 4357     |\n",
      "|    total_timesteps | 3024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3044160, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0702     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3044160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028194334 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.84        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | 0.00423     |\n",
      "|    std                  | 0.0627      |\n",
      "|    value_loss           | 3.7e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3064320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0604     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3064320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014667921 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.86        |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | -0.00364    |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.00307     |\n",
      "|    std                  | 0.0622      |\n",
      "|    value_loss           | 4.18e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0826  |\n",
      "| time/              |          |\n",
      "|    fps             | 694      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 4412     |\n",
      "|    total_timesteps | 3064320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3084480, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0644     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3084480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031001361 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.87        |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | -0.000159   |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.0617      |\n",
      "|    value_loss           | 2.08e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3104640, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0664     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3104640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012539069 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.88        |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | 0.00276     |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | 0.00714     |\n",
      "|    std                  | 0.0618      |\n",
      "|    value_loss           | 8.44e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.082   |\n",
      "| time/              |          |\n",
      "|    fps             | 694      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 4467     |\n",
      "|    total_timesteps | 3104640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3124800, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0636     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3124800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009661914 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.88        |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.00103     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | 0.00787     |\n",
      "|    std                  | 0.062       |\n",
      "|    value_loss           | 9.91e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3144960, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0819     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3144960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012110116 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.89        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.000314    |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | 0.0041      |\n",
      "|    std                  | 0.0618      |\n",
      "|    value_loss           | 2.38e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0806  |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 4524     |\n",
      "|    total_timesteps | 3144960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3165120, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0945     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3165120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050085224 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.89        |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.0615      |\n",
      "|    value_loss           | 1.02e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3185280, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0953     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3185280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016354555 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.91        |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.00575     |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | 0.00781     |\n",
      "|    std                  | 0.0609      |\n",
      "|    value_loss           | 1.2e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.074   |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 4580     |\n",
      "|    total_timesteps | 3185280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3205440, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.084      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3205440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004068599 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.92        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | -0.00132    |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | 0.00512     |\n",
      "|    std                  | 0.0606      |\n",
      "|    value_loss           | 7.42e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3225600, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0879     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3225600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008619509 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.93        |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | 0.00497     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    std                  | 0.0608      |\n",
      "|    value_loss           | 1.41e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0729  |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 4635     |\n",
      "|    total_timesteps | 3225600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3245760, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0808     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3245760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005246764 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.95        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | 0.000847    |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | 0.00314     |\n",
      "|    std                  | 0.0601      |\n",
      "|    value_loss           | 8e-08       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3265920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0631      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3265920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109230755 |\n",
      "|    clip_fraction        | 0.195        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.96         |\n",
      "|    explained_variance   | 0.42         |\n",
      "|    learning_rate        | 0.00101      |\n",
      "|    loss                 | 0.00176      |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | 0.00539      |\n",
      "|    std                  | 0.0599       |\n",
      "|    value_loss           | 1.31e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0764  |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 4691     |\n",
      "|    total_timesteps | 3265920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3286080, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0578     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3286080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011358888 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.96        |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.000986    |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | 0.00433     |\n",
      "|    std                  | 0.0603      |\n",
      "|    value_loss           | 3.01e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3306240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0494      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3306240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073264586 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.98         |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.00101      |\n",
      "|    loss                 | 0.000143     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | 0.00197      |\n",
      "|    std                  | 0.0595       |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0785  |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 4747     |\n",
      "|    total_timesteps | 3306240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3326400, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0597     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3326400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036818665 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3           |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00994     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | 0.0238      |\n",
      "|    std                  | 0.0591      |\n",
      "|    value_loss           | 4.82e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3346560, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3346560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023718929 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3           |\n",
      "|    explained_variance   | 0.0222      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.00259    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.00314     |\n",
      "|    std                  | 0.0587      |\n",
      "|    value_loss           | 8.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0767  |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 4803     |\n",
      "|    total_timesteps | 3346560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3366720, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0643     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3366720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014959501 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.02        |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.000998    |\n",
      "|    loss                 | 0.00781     |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | 0.00278     |\n",
      "|    std                  | 0.0579      |\n",
      "|    value_loss           | 1.71e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3386880, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0448     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3386880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029576553 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.03        |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.000995    |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.0088      |\n",
      "|    std                  | 0.0577      |\n",
      "|    value_loss           | 1.17e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0735  |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 4859     |\n",
      "|    total_timesteps | 3386880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3407040, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0695      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3407040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067814207 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.04         |\n",
      "|    explained_variance   | 0.599        |\n",
      "|    learning_rate        | 0.000992     |\n",
      "|    loss                 | -0.00173     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | 0.00232      |\n",
      "|    std                  | 0.0575       |\n",
      "|    value_loss           | 9.66e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3427200, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.084      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3427200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008523932 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.05        |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.000989    |\n",
      "|    loss                 | -0.000198   |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.00496     |\n",
      "|    std                  | 0.057       |\n",
      "|    value_loss           | 1.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0701  |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 4914     |\n",
      "|    total_timesteps | 3427200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3447360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0721     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3447360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051592685 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.06        |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.000986    |\n",
      "|    loss                 | 0.00857     |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | 0.0358      |\n",
      "|    std                  | 0.0569      |\n",
      "|    value_loss           | 7.1e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3467520, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0702     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3467520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013152057 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.07        |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.000983    |\n",
      "|    loss                 | 0.000133    |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.00537     |\n",
      "|    std                  | 0.0563      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0693  |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 4970     |\n",
      "|    total_timesteps | 3467520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3487680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0468     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3487680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030127931 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.08        |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.00098     |\n",
      "|    loss                 | 0.0042      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | 0.00823     |\n",
      "|    std                  | 0.0562      |\n",
      "|    value_loss           | 1.99e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3507840, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.06       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3507840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025216516 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.09        |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.000977    |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | 0.00354     |\n",
      "|    std                  | 0.0561      |\n",
      "|    value_loss           | 3.8e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0681  |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 5026     |\n",
      "|    total_timesteps | 3507840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3528000, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.119      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3528000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010870849 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.09        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.000974    |\n",
      "|    loss                 | 0.00043     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | 0.00513     |\n",
      "|    std                  | 0.0558      |\n",
      "|    value_loss           | 4.51e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3548160, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0683     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3548160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008338967 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.11        |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.000971    |\n",
      "|    loss                 | -0.00183    |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | 0.00673     |\n",
      "|    std                  | 0.0552      |\n",
      "|    value_loss           | 3.63e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.068   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 5084     |\n",
      "|    total_timesteps | 3548160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3568320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0462     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3568320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007888183 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.13        |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.000968    |\n",
      "|    loss                 | 0.00112     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | 0.00698     |\n",
      "|    std                  | 0.0547      |\n",
      "|    value_loss           | 2.11e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3588480, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0554     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3588480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012104674 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.14        |\n",
      "|    explained_variance   | -0.0285     |\n",
      "|    learning_rate        | 0.000965    |\n",
      "|    loss                 | -0.00371    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | 0.00202     |\n",
      "|    std                  | 0.0544      |\n",
      "|    value_loss           | 7.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0675  |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 5140     |\n",
      "|    total_timesteps | 3588480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3608640, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0794     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3608640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020434698 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.15        |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.000962    |\n",
      "|    loss                 | 0.00943     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | 0.00526     |\n",
      "|    std                  | 0.0544      |\n",
      "|    value_loss           | 7.75e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3628800, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0678     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3628800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023377355 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.16        |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.000959    |\n",
      "|    loss                 | 0.00693     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.054       |\n",
      "|    value_loss           | 3.38e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0685  |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 5195     |\n",
      "|    total_timesteps | 3628800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3648960, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.066      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3648960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019830357 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.17        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.000956    |\n",
      "|    loss                 | 0.00266     |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | 0.00606     |\n",
      "|    std                  | 0.0538      |\n",
      "|    value_loss           | 1.25e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3669120, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.078      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3669120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021342544 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.18        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.000953    |\n",
      "|    loss                 | 0.00559     |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.0535      |\n",
      "|    value_loss           | 3.45e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0666  |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 5251     |\n",
      "|    total_timesteps | 3669120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3689280, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0572     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3689280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008339882 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.2         |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00095     |\n",
      "|    loss                 | 0.00257     |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | 0.00165     |\n",
      "|    std                  | 0.0529      |\n",
      "|    value_loss           | 4.33e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3709440, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0747      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3709440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134270005 |\n",
      "|    clip_fraction        | 0.188        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.22         |\n",
      "|    explained_variance   | -0.0272      |\n",
      "|    learning_rate        | 0.000947     |\n",
      "|    loss                 | 0.00122      |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | 0.00296      |\n",
      "|    std                  | 0.0521       |\n",
      "|    value_loss           | 1.13e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0673  |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 5306     |\n",
      "|    total_timesteps | 3709440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3729600, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0431     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3729600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028549831 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.25        |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.000944    |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | 0.00697     |\n",
      "|    std                  | 0.0517      |\n",
      "|    value_loss           | 1.32e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3749760, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0733    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3749760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11752255 |\n",
      "|    clip_fraction        | 0.497      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.25       |\n",
      "|    explained_variance   | 0.344      |\n",
      "|    learning_rate        | 0.000941   |\n",
      "|    loss                 | 0.0353     |\n",
      "|    n_updates            | 1850       |\n",
      "|    policy_gradient_loss | 0.0321     |\n",
      "|    std                  | 0.0518     |\n",
      "|    value_loss           | 2.44e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0666  |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 5362     |\n",
      "|    total_timesteps | 3749760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3769920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0609    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3769920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00867981 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.25       |\n",
      "|    explained_variance   | 0.265      |\n",
      "|    learning_rate        | 0.000938   |\n",
      "|    loss                 | 0.00765    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | 0.00828    |\n",
      "|    std                  | 0.0519     |\n",
      "|    value_loss           | 2.38e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3790080, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0783     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3790080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019846067 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.25        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.000935    |\n",
      "|    loss                 | 0.0034      |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.0516      |\n",
      "|    value_loss           | 8.66e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.071   |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 5418     |\n",
      "|    total_timesteps | 3790080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810240, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0852     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3810240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006800079 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.26        |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.000931    |\n",
      "|    loss                 | 0.00153     |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | 0.00468     |\n",
      "|    std                  | 0.0516      |\n",
      "|    value_loss           | 2.73e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3830400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.054     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3830400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04943608 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.28       |\n",
      "|    explained_variance   | 0.359      |\n",
      "|    learning_rate        | 0.000928   |\n",
      "|    loss                 | 0.00656    |\n",
      "|    n_updates            | 1890       |\n",
      "|    policy_gradient_loss | 0.00748    |\n",
      "|    std                  | 0.0512     |\n",
      "|    value_loss           | 1.74e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.073   |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 5475     |\n",
      "|    total_timesteps | 3830400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850560, episode_reward=-0.08 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0778     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3850560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042481586 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.3         |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.000925    |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | 0.0123      |\n",
      "|    std                  | 0.0508      |\n",
      "|    value_loss           | 4.37e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3870720, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.087       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3870720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077506043 |\n",
      "|    clip_fraction        | 0.176        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.32         |\n",
      "|    explained_variance   | 0.558        |\n",
      "|    learning_rate        | 0.000922     |\n",
      "|    loss                 | 0.00292      |\n",
      "|    n_updates            | 1910         |\n",
      "|    policy_gradient_loss | 0.00194      |\n",
      "|    std                  | 0.0506       |\n",
      "|    value_loss           | 2.99e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0705  |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 5530     |\n",
      "|    total_timesteps | 3870720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890880, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0748     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3890880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011701416 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.32        |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.000919    |\n",
      "|    loss                 | -0.000368   |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | 0.00478     |\n",
      "|    std                  | 0.0505      |\n",
      "|    value_loss           | 8.52e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3911040, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.059      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3911040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009376341 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.34        |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.000916    |\n",
      "|    loss                 | 0.00118     |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | 0.00102     |\n",
      "|    std                  | 0.0499      |\n",
      "|    value_loss           | 3.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0756  |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 5585     |\n",
      "|    total_timesteps | 3911040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3931200, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0584      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3931200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106979385 |\n",
      "|    clip_fraction        | 0.154        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.37         |\n",
      "|    explained_variance   | 0.347        |\n",
      "|    learning_rate        | 0.000913     |\n",
      "|    loss                 | 2.47e-05     |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | 0.00138      |\n",
      "|    std                  | 0.0495       |\n",
      "|    value_loss           | 4.38e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3951360, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0551     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3951360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058790307 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.39        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00091     |\n",
      "|    loss                 | 0.00672     |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.00925     |\n",
      "|    std                  | 0.0488      |\n",
      "|    value_loss           | 2.51e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0738  |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 5641     |\n",
      "|    total_timesteps | 3951360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3971520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 1.26e+03 |\n",
      "|    mean_reward          | -0.055   |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3971520  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.04954  |\n",
      "|    clip_fraction        | 0.325    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 3.41     |\n",
      "|    explained_variance   | 0.403    |\n",
      "|    learning_rate        | 0.000907 |\n",
      "|    loss                 | -0.00183 |\n",
      "|    n_updates            | 1960     |\n",
      "|    policy_gradient_loss | 0.0125   |\n",
      "|    std                  | 0.0487   |\n",
      "|    value_loss           | 9.63e-08 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=3991680, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0742     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3991680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007903096 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.41        |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.000904    |\n",
      "|    loss                 | 0.000177    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | 0.00697     |\n",
      "|    std                  | 0.0486      |\n",
      "|    value_loss           | 1.61e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0735  |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 5697     |\n",
      "|    total_timesteps | 3991680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4011840, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4011840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012780471 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.42        |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000901    |\n",
      "|    loss                 | 0.00164     |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | 0.00875     |\n",
      "|    std                  | 0.0481      |\n",
      "|    value_loss           | 8.07e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4032000, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0716    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4032000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03202445 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.44       |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.000898   |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 1990       |\n",
      "|    policy_gradient_loss | 0.00431    |\n",
      "|    std                  | 0.0478     |\n",
      "|    value_loss           | 1.52e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0726  |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 5753     |\n",
      "|    total_timesteps | 4032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4052160, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0509     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4052160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018525355 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.44        |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.000895    |\n",
      "|    loss                 | 0.00327     |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | 0.0027      |\n",
      "|    std                  | 0.0476      |\n",
      "|    value_loss           | 5.99e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4072320, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0643     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4072320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011382073 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.45        |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.000892    |\n",
      "|    loss                 | 0.00272     |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | 0.00229     |\n",
      "|    std                  | 0.0473      |\n",
      "|    value_loss           | 1.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0726  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 5808     |\n",
      "|    total_timesteps | 4072320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4092480, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0654     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4092480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018514182 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.47        |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.000889    |\n",
      "|    loss                 | 0.00797     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.047       |\n",
      "|    value_loss           | 4.39e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4112640, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0709     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4112640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012949258 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.48        |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.000886    |\n",
      "|    loss                 | 0.00301     |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    std                  | 0.0466      |\n",
      "|    value_loss           | 3.45e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0703  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 5864     |\n",
      "|    total_timesteps | 4112640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4132800, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0745      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4132800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082399435 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.51         |\n",
      "|    explained_variance   | 0.318        |\n",
      "|    learning_rate        | 0.000883     |\n",
      "|    loss                 | 0.00524      |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | 0.0129       |\n",
      "|    std                  | 0.0462       |\n",
      "|    value_loss           | 5.02e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4152960, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0675     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4152960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018855464 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.52        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00088     |\n",
      "|    loss                 | 0.00344     |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    std                  | 0.0458      |\n",
      "|    value_loss           | 1.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0727  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 5919     |\n",
      "|    total_timesteps | 4152960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4173120, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.101     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4173120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05663749 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.53       |\n",
      "|    explained_variance   | 0.193      |\n",
      "|    learning_rate        | 0.000877   |\n",
      "|    loss                 | 0.0253     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | 0.00274    |\n",
      "|    std                  | 0.0457     |\n",
      "|    value_loss           | 2.38e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4193280, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0839    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4193280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00829231 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.53       |\n",
      "|    explained_variance   | 0.305      |\n",
      "|    learning_rate        | 0.000874   |\n",
      "|    loss                 | 0.00118    |\n",
      "|    n_updates            | 2070       |\n",
      "|    policy_gradient_loss | 0.00316    |\n",
      "|    std                  | 0.0457     |\n",
      "|    value_loss           | 1.47e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0769  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 5975     |\n",
      "|    total_timesteps | 4193280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4213440, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0665      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4213440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130207185 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.55         |\n",
      "|    explained_variance   | 0.526        |\n",
      "|    learning_rate        | 0.000871     |\n",
      "|    loss                 | 0.000367     |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | 0.00462      |\n",
      "|    std                  | 0.0455       |\n",
      "|    value_loss           | 1.72e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4233600, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4233600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059965447 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.56        |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.000868    |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.00331     |\n",
      "|    std                  | 0.0452      |\n",
      "|    value_loss           | 6.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0842  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 6032     |\n",
      "|    total_timesteps | 4233600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4253760, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.108     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4253760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05638077 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.55       |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.000865   |\n",
      "|    loss                 | 0.00975    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | 0.011      |\n",
      "|    std                  | 0.0455     |\n",
      "|    value_loss           | 2.27e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4273920, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0823     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4273920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035111442 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.55        |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.000862    |\n",
      "|    loss                 | 0.029       |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | 0.00713     |\n",
      "|    std                  | 0.0452      |\n",
      "|    value_loss           | 6.45e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0844  |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 6088     |\n",
      "|    total_timesteps | 4273920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4294080, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0723    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4294080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03852297 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.56       |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.000859   |\n",
      "|    loss                 | 0.0253     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | 0.0174     |\n",
      "|    std                  | 0.0452     |\n",
      "|    value_loss           | 7.58e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4314240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0646      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4314240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059849243 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.57         |\n",
      "|    explained_variance   | 0.538        |\n",
      "|    learning_rate        | 0.000856     |\n",
      "|    loss                 | -0.00226     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | 0.00216      |\n",
      "|    std                  | 0.0449       |\n",
      "|    value_loss           | 7.66e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.084   |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 6143     |\n",
      "|    total_timesteps | 4314240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4334400, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0879     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4334400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019406872 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.57        |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.000853    |\n",
      "|    loss                 | -0.00462    |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | 0.00625     |\n",
      "|    std                  | 0.0453      |\n",
      "|    value_loss           | 2.48e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4354560, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0779   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4354560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0103544 |\n",
      "|    clip_fraction        | 0.287     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.57      |\n",
      "|    explained_variance   | 0.341     |\n",
      "|    learning_rate        | 0.00085   |\n",
      "|    loss                 | 0.0125    |\n",
      "|    n_updates            | 2150      |\n",
      "|    policy_gradient_loss | 0.019     |\n",
      "|    std                  | 0.045     |\n",
      "|    value_loss           | 6.31e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0781  |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 6199     |\n",
      "|    total_timesteps | 4354560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4374720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0751     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4374720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010005468 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.58        |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.000847    |\n",
      "|    loss                 | -0.00148    |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.0449      |\n",
      "|    value_loss           | 5.76e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4394880, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0895     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4394880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013711515 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.58        |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.000844    |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.0447      |\n",
      "|    value_loss           | 9.34e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0752  |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 6254     |\n",
      "|    total_timesteps | 4394880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4415040, episode_reward=-0.09 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0898     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4415040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006608519 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.59        |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.000841    |\n",
      "|    loss                 | -0.00277    |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | 0.000395    |\n",
      "|    std                  | 0.0447      |\n",
      "|    value_loss           | 4.34e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4435200, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0708     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4435200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014225284 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.6         |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.000838    |\n",
      "|    loss                 | 0.00111     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.00506     |\n",
      "|    std                  | 0.0446      |\n",
      "|    value_loss           | 1.2e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0773  |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 6309     |\n",
      "|    total_timesteps | 4435200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4455360, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0784     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4455360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941273 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.61        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.000835    |\n",
      "|    loss                 | 0.0015      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | 0.000475    |\n",
      "|    std                  | 0.0441      |\n",
      "|    value_loss           | 3.43e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4475520, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0759     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4475520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019220192 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.63        |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.000832    |\n",
      "|    loss                 | -0.00119    |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | 0.00852     |\n",
      "|    std                  | 0.0438      |\n",
      "|    value_loss           | 5.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0787  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 6365     |\n",
      "|    total_timesteps | 4475520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4495680, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0969    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4495680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01015695 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.64       |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.000829   |\n",
      "|    loss                 | 0.00902    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | 0.00576    |\n",
      "|    std                  | 0.0435     |\n",
      "|    value_loss           | 1.62e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4515840, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0658     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4515840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019768283 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.65        |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.000826    |\n",
      "|    loss                 | 0.00205     |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 0.00483     |\n",
      "|    std                  | 0.0434      |\n",
      "|    value_loss           | 8.45e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0795  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 6421     |\n",
      "|    total_timesteps | 4515840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4536000, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.075      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4536000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010616269 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.65        |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.000823    |\n",
      "|    loss                 | 0.00262     |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | 0.00193     |\n",
      "|    std                  | 0.0435      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4556160, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.094      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4556160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019774636 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.66        |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.00082     |\n",
      "|    loss                 | -0.00295    |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.00461     |\n",
      "|    std                  | 0.0431      |\n",
      "|    value_loss           | 6.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0784  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 6476     |\n",
      "|    total_timesteps | 4556160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4576320, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0695     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4576320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019283542 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.67        |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.000817    |\n",
      "|    loss                 | 0.005       |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | 0.0031      |\n",
      "|    std                  | 0.0429      |\n",
      "|    value_loss           | 1.82e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4596480, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0973     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4596480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019202728 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.67        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.000814    |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.00415     |\n",
      "|    std                  | 0.0431      |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.079   |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 6532     |\n",
      "|    total_timesteps | 4596480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4616640, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0806    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4616640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02770963 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.67       |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.000811   |\n",
      "|    loss                 | 0.0299     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | 0.0197     |\n",
      "|    std                  | 0.0431     |\n",
      "|    value_loss           | 7.28e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4636800, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0766     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4636800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007914467 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.67        |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.000808    |\n",
      "|    loss                 | -0.000873   |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | 0.0015      |\n",
      "|    std                  | 0.0427      |\n",
      "|    value_loss           | 9.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0803  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 6588     |\n",
      "|    total_timesteps | 4636800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4656960, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0754     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4656960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024143405 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.69        |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.000804    |\n",
      "|    loss                 | -0.00171    |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | 0.00625     |\n",
      "|    std                  | 0.0425      |\n",
      "|    value_loss           | 9.41e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4677120, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0636     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4677120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018227944 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.000801    |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.00164     |\n",
      "|    std                  | 0.0421      |\n",
      "|    value_loss           | 6.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0814  |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 6644     |\n",
      "|    total_timesteps | 4677120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4697280, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0693     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4697280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009640296 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.72        |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.000798    |\n",
      "|    loss                 | -0.00172    |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | 0.000191    |\n",
      "|    std                  | 0.042       |\n",
      "|    value_loss           | 3.77e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4717440, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0609     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4717440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017512416 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.72        |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.000795    |\n",
      "|    loss                 | 0.0051      |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.0043      |\n",
      "|    std                  | 0.042       |\n",
      "|    value_loss           | 3.27e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0789  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 6700     |\n",
      "|    total_timesteps | 4717440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4737600, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0564     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4737600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012081217 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.72        |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.000792    |\n",
      "|    loss                 | 0.00652     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | 0.00338     |\n",
      "|    std                  | 0.0421      |\n",
      "|    value_loss           | 4.94e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4757760, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0899     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4757760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021653121 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.000789    |\n",
      "|    loss                 | 0.00419     |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | 0.00563     |\n",
      "|    std                  | 0.0423      |\n",
      "|    value_loss           | 2.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0773  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 6756     |\n",
      "|    total_timesteps | 4757760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4777920, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0931     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4777920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022647414 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.000786    |\n",
      "|    loss                 | -0.00213    |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | 0.00223     |\n",
      "|    std                  | 0.0423      |\n",
      "|    value_loss           | 6.21e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4798080, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0686     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4798080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018807925 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.000783    |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    std                  | 0.0422      |\n",
      "|    value_loss           | 1.85e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0759  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 6811     |\n",
      "|    total_timesteps | 4798080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4818240, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0678    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4818240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01107731 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.71       |\n",
      "|    explained_variance   | 0.407      |\n",
      "|    learning_rate        | 0.00078    |\n",
      "|    loss                 | 0.00794    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | 0.00314    |\n",
      "|    std                  | 0.0424     |\n",
      "|    value_loss           | 8.39e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4838400, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4838400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036592986 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.000777    |\n",
      "|    loss                 | 0.00556     |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.009       |\n",
      "|    std                  | 0.0424      |\n",
      "|    value_loss           | 9.21e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0718  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 6867     |\n",
      "|    total_timesteps | 4838400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4858560, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.077      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4858560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014453094 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.000774    |\n",
      "|    loss                 | -0.00141    |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | 0.00859     |\n",
      "|    std                  | 0.0423      |\n",
      "|    value_loss           | 2.93e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4878720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0795    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4878720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01942629 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.72       |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.000771   |\n",
      "|    loss                 | 0.00771    |\n",
      "|    n_updates            | 2410       |\n",
      "|    policy_gradient_loss | 0.00584    |\n",
      "|    std                  | 0.0419     |\n",
      "|    value_loss           | 2.37e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0756  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 6923     |\n",
      "|    total_timesteps | 4878720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4898880, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0674     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4898880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021376798 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.74        |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.000768    |\n",
      "|    loss                 | -0.00118    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | 0.00852     |\n",
      "|    std                  | 0.0414      |\n",
      "|    value_loss           | 4.61e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4919040, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0695     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4919040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012166524 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.76        |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.000765    |\n",
      "|    loss                 | -0.000483   |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | 0.0015      |\n",
      "|    std                  | 0.0411      |\n",
      "|    value_loss           | 1.29e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0724  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 6979     |\n",
      "|    total_timesteps | 4919040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4939200, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0511     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4939200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012791105 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.76        |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.000762    |\n",
      "|    loss                 | 0.00495     |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | 0.00423     |\n",
      "|    std                  | 0.0409      |\n",
      "|    value_loss           | 4.8e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4959360, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0858     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4959360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016810996 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.78        |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.000759    |\n",
      "|    loss                 | 0.00186     |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | 0.00514     |\n",
      "|    std                  | 0.0406      |\n",
      "|    value_loss           | 3.79e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0764  |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 7035     |\n",
      "|    total_timesteps | 4959360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4979520, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.075      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4979520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040153697 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.79        |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.000756    |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | 0.00975     |\n",
      "|    std                  | 0.0405      |\n",
      "|    value_loss           | 1.33e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4999680, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0719     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4999680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042796303 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.8         |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.000753    |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.0403      |\n",
      "|    value_loss           | 9.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0776  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 7091     |\n",
      "|    total_timesteps | 4999680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5019840, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0712      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5019840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051902416 |\n",
      "|    clip_fraction        | 0.245        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.8          |\n",
      "|    explained_variance   | 0.489        |\n",
      "|    learning_rate        | 0.00075      |\n",
      "|    loss                 | 0.00137      |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | 0.0119       |\n",
      "|    std                  | 0.0403       |\n",
      "|    value_loss           | 1.98e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5040000, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.122      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023209551 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.81        |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.000747    |\n",
      "|    loss                 | -0.000624   |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | 0.00598     |\n",
      "|    std                  | 0.0402      |\n",
      "|    value_loss           | 7.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0814  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 7146     |\n",
      "|    total_timesteps | 5040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5060160, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.0644   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5060160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0595067 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.82      |\n",
      "|    explained_variance   | 0.458     |\n",
      "|    learning_rate        | 0.000744  |\n",
      "|    loss                 | 0.0057    |\n",
      "|    n_updates            | 2500      |\n",
      "|    policy_gradient_loss | 0.00888   |\n",
      "|    std                  | 0.0401    |\n",
      "|    value_loss           | 1.03e-07  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=5080320, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0773     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5080320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029327184 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.82        |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.000741    |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.0399      |\n",
      "|    value_loss           | 1.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0821  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 7203     |\n",
      "|    total_timesteps | 5080320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5100480, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0624    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5100480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02073009 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.83       |\n",
      "|    explained_variance   | 0.297      |\n",
      "|    learning_rate        | 0.000738   |\n",
      "|    loss                 | 0.00101    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | 0.00138    |\n",
      "|    std                  | 0.0398     |\n",
      "|    value_loss           | 1.46e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5120640, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0961     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5120640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008005854 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.83        |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.000735    |\n",
      "|    loss                 | -0.00231    |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    std                  | 0.0397      |\n",
      "|    value_loss           | 3.85e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0789  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 7259     |\n",
      "|    total_timesteps | 5120640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5140800, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0587     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5140800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004347837 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.82        |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.000732    |\n",
      "|    loss                 | -0.00101    |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | 0.000553    |\n",
      "|    std                  | 0.0396      |\n",
      "|    value_loss           | 6.08e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5160960, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0913     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5160960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006711401 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.83        |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.000729    |\n",
      "|    loss                 | -0.00175    |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.00915     |\n",
      "|    std                  | 0.0396      |\n",
      "|    value_loss           | 1.38e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0725  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 7315     |\n",
      "|    total_timesteps | 5160960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5181120, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0692     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5181120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012972871 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.84        |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.000726    |\n",
      "|    loss                 | 0.00627     |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | 0.00567     |\n",
      "|    std                  | 0.0393      |\n",
      "|    value_loss           | 5.1e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5201280, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5201280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010982641 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.84        |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.000723    |\n",
      "|    loss                 | 0.0004      |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | 0.00376     |\n",
      "|    std                  | 0.0394      |\n",
      "|    value_loss           | 7.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0684  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 7371     |\n",
      "|    total_timesteps | 5201280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5221440, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0714     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5221440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008198121 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.85        |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00072     |\n",
      "|    loss                 | 0.00106     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | 0.00269     |\n",
      "|    std                  | 0.039       |\n",
      "|    value_loss           | 5.18e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5241600, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0984     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5241600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027543016 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.86        |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.000717    |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.0145      |\n",
      "|    std                  | 0.0387      |\n",
      "|    value_loss           | 9.35e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0706  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 7427     |\n",
      "|    total_timesteps | 5241600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5261760, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0743      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5261760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075892825 |\n",
      "|    clip_fraction        | 0.0905       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.88         |\n",
      "|    explained_variance   | 0.591        |\n",
      "|    learning_rate        | 0.000714     |\n",
      "|    loss                 | -0.000458    |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | 0.000436     |\n",
      "|    std                  | 0.0382       |\n",
      "|    value_loss           | 1.52e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5281920, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0471     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5281920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059809215 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.89        |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.000711    |\n",
      "|    loss                 | 0.0523      |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.0249      |\n",
      "|    std                  | 0.038       |\n",
      "|    value_loss           | 5.33e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.069   |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 7483     |\n",
      "|    total_timesteps | 5281920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5302080, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0819     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5302080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026698831 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.9         |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.000708    |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | 0.0185      |\n",
      "|    std                  | 0.0379      |\n",
      "|    value_loss           | 3.35e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5322240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0649     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5322240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045653373 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.9         |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.000705    |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.0176      |\n",
      "|    std                  | 0.0378      |\n",
      "|    value_loss           | 9.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0729  |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 7539     |\n",
      "|    total_timesteps | 5322240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5342400, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0624      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5342400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075154062 |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.91         |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.000702     |\n",
      "|    loss                 | 0.00389      |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | 0.00835      |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 1.14e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5362560, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.108      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5362560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006303804 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.92        |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.000699    |\n",
      "|    loss                 | -0.00153    |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.000692   |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 5.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0688  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 7595     |\n",
      "|    total_timesteps | 5362560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5382720, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.071      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5382720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009138731 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.92        |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.000696    |\n",
      "|    loss                 | 0.00263     |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | 0.00323     |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 6.97e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5402880, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0612     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5402880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026076922 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.94        |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.000693    |\n",
      "|    loss                 | 0.0066      |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | 0.00524     |\n",
      "|    std                  | 0.0371      |\n",
      "|    value_loss           | 4.71e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0743  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 7650     |\n",
      "|    total_timesteps | 5402880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5423040, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0974      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5423040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043724356 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.96         |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.00069      |\n",
      "|    loss                 | -0.00129     |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | 0.00251      |\n",
      "|    std                  | 0.0368       |\n",
      "|    value_loss           | 2.49e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5443200, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0611     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5443200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010303013 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.96        |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.000687    |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | 0.00898     |\n",
      "|    std                  | 0.0369      |\n",
      "|    value_loss           | 5.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.073   |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 7706     |\n",
      "|    total_timesteps | 5443200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5463360, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0592    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5463360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01211834 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.96       |\n",
      "|    explained_variance   | 0.307      |\n",
      "|    learning_rate        | 0.000684   |\n",
      "|    loss                 | 0.00844    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | 0.00421    |\n",
      "|    std                  | 0.037      |\n",
      "|    value_loss           | 5.95e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5483520, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0691     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5483520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017453495 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00068     |\n",
      "|    loss                 | 0.0132      |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | 0.00388     |\n",
      "|    std                  | 0.0368      |\n",
      "|    value_loss           | 3.88e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0727  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 7762     |\n",
      "|    total_timesteps | 5483520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5503680, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0789     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5503680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021201987 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.000677    |\n",
      "|    loss                 | 0.00812     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | 0.00707     |\n",
      "|    std                  | 0.0366      |\n",
      "|    value_loss           | 8.42e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5523840, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0693      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5523840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073448503 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.98         |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.000674     |\n",
      "|    loss                 | 0.00349      |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | 0.00233      |\n",
      "|    std                  | 0.0365       |\n",
      "|    value_loss           | 9.52e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 7819     |\n",
      "|    total_timesteps | 5523840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5544000, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.079      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5544000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078392014 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.99        |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.000671    |\n",
      "|    loss                 | 0.0386      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | 0.00376     |\n",
      "|    std                  | 0.0367      |\n",
      "|    value_loss           | 1.95e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5564160, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0754      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5564160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101965945 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.97         |\n",
      "|    explained_variance   | 0.381        |\n",
      "|    learning_rate        | 0.000668     |\n",
      "|    loss                 | -0.00336     |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | 0.000129     |\n",
      "|    std                  | 0.0371       |\n",
      "|    value_loss           | 6.68e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0674  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 7874     |\n",
      "|    total_timesteps | 5564160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5584320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0599     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5584320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010696413 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.97        |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.000665    |\n",
      "|    loss                 | 0.000386    |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | 0.00419     |\n",
      "|    std                  | 0.0369      |\n",
      "|    value_loss           | 1.97e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5604480, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.107     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5604480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07388304 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.98       |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.000662   |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 2770       |\n",
      "|    policy_gradient_loss | 0.014      |\n",
      "|    std                  | 0.0369     |\n",
      "|    value_loss           | 7.3e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0696  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 7930     |\n",
      "|    total_timesteps | 5604480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5624640, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0769     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5624640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004192084 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.98        |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.000659    |\n",
      "|    loss                 | 0.000686    |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | 0.0022      |\n",
      "|    std                  | 0.0367      |\n",
      "|    value_loss           | 6.5e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5644800, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0748     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5644800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043806583 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.99        |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.000656    |\n",
      "|    loss                 | 0.0321      |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | 0.00705     |\n",
      "|    std                  | 0.0368      |\n",
      "|    value_loss           | 5.32e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0664  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 7986     |\n",
      "|    total_timesteps | 5644800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5664960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0464     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5664960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051399305 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.99        |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.000653    |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | 0.00625     |\n",
      "|    std                  | 0.0368      |\n",
      "|    value_loss           | 1.39e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5685120, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0948     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5685120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019511653 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.99        |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00065     |\n",
      "|    loss                 | 0.00704     |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | 0.00554     |\n",
      "|    std                  | 0.0369      |\n",
      "|    value_loss           | 1.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0644  |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 8042     |\n",
      "|    total_timesteps | 5685120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5705280, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0452      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5705280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075679263 |\n",
      "|    clip_fraction        | 0.221        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4            |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.000647     |\n",
      "|    loss                 | -0.0016      |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | 0.00861      |\n",
      "|    std                  | 0.0367       |\n",
      "|    value_loss           | 2.96e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5725440, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0713     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5725440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034549825 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.000644    |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.0367      |\n",
      "|    value_loss           | 2.11e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0637  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 8097     |\n",
      "|    total_timesteps | 5725440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5745600, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0659     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5745600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028182765 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.000641    |\n",
      "|    loss                 | 0.00507     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.0368      |\n",
      "|    value_loss           | 3.2e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5765760, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0839     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5765760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011589281 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.99        |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.000638    |\n",
      "|    loss                 | 0.00549     |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | 0.00163     |\n",
      "|    std                  | 0.0367      |\n",
      "|    value_loss           | 1.97e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.066   |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 8153     |\n",
      "|    total_timesteps | 5765760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5785920, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0435      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5785920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101637915 |\n",
      "|    clip_fraction        | 0.17         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4            |\n",
      "|    explained_variance   | 0.458        |\n",
      "|    learning_rate        | 0.000635     |\n",
      "|    loss                 | 0.00709      |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | 0.00689      |\n",
      "|    std                  | 0.0367       |\n",
      "|    value_loss           | 8.33e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5806080, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0618     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5806080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017635832 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.000632    |\n",
      "|    loss                 | 0.0044      |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | 0.00497     |\n",
      "|    std                  | 0.0367      |\n",
      "|    value_loss           | 9.49e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0689  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 8209     |\n",
      "|    total_timesteps | 5806080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5826240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0622    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5826240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01841511 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.99       |\n",
      "|    explained_variance   | 0.639      |\n",
      "|    learning_rate        | 0.000629   |\n",
      "|    loss                 | 0.00352    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | 0.0026     |\n",
      "|    std                  | 0.0369     |\n",
      "|    value_loss           | 8.19e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5846400, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0647     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5846400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022004409 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4           |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.000626    |\n",
      "|    loss                 | 0.0062      |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | 0.00234     |\n",
      "|    std                  | 0.0365      |\n",
      "|    value_loss           | 5.39e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0672  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 8264     |\n",
      "|    total_timesteps | 5846400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5866560, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0594     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5866560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011143126 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.01        |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.000623    |\n",
      "|    loss                 | 0.00125     |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    std                  | 0.0364      |\n",
      "|    value_loss           | 5.16e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5886720, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0696    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5886720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01739795 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.01       |\n",
      "|    explained_variance   | 0.463      |\n",
      "|    learning_rate        | 0.00062    |\n",
      "|    loss                 | 0.00601    |\n",
      "|    n_updates            | 2910       |\n",
      "|    policy_gradient_loss | 0.00177    |\n",
      "|    std                  | 0.0364     |\n",
      "|    value_loss           | 5.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0613  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 8320     |\n",
      "|    total_timesteps | 5886720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5906880, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0614     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5906880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016792933 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.02        |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.000617    |\n",
      "|    loss                 | 0.00702     |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | 0.00305     |\n",
      "|    std                  | 0.0362      |\n",
      "|    value_loss           | 7.35e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5927040, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0495    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5927040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00869764 |\n",
      "|    clip_fraction        | 0.0835     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.03       |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.000614   |\n",
      "|    loss                 | -0.0012    |\n",
      "|    n_updates            | 2930       |\n",
      "|    policy_gradient_loss | 6.71e-05   |\n",
      "|    std                  | 0.0362     |\n",
      "|    value_loss           | 6.22e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0582  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 8376     |\n",
      "|    total_timesteps | 5927040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5947200, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0665     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5947200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014767214 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.03        |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.000611    |\n",
      "|    loss                 | 0.00249     |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | 0.00595     |\n",
      "|    std                  | 0.0361      |\n",
      "|    value_loss           | 4.47e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5967360, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0625     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5967360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015137693 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.05        |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.000608    |\n",
      "|    loss                 | -0.00119    |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | 0.00152     |\n",
      "|    std                  | 0.0358      |\n",
      "|    value_loss           | 1.59e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 8432     |\n",
      "|    total_timesteps | 5967360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5987520, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.076     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5987520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02807882 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.07       |\n",
      "|    explained_variance   | 0.406      |\n",
      "|    learning_rate        | 0.000605   |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | 0.00797    |\n",
      "|    std                  | 0.0356     |\n",
      "|    value_loss           | 1.03e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6007680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0508    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6007680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06251824 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.07       |\n",
      "|    explained_variance   | 0.563      |\n",
      "|    learning_rate        | 0.000602   |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 2970       |\n",
      "|    policy_gradient_loss | 0.00642    |\n",
      "|    std                  | 0.0355     |\n",
      "|    value_loss           | 1.12e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0595  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 8488     |\n",
      "|    total_timesteps | 6007680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6027840, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0776      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6027840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073254425 |\n",
      "|    clip_fraction        | 0.135        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.08         |\n",
      "|    explained_variance   | 0.354        |\n",
      "|    learning_rate        | 0.000599     |\n",
      "|    loss                 | -6.37e-05    |\n",
      "|    n_updates            | 2980         |\n",
      "|    policy_gradient_loss | 0.00337      |\n",
      "|    std                  | 0.0352       |\n",
      "|    value_loss           | 7.24e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6048000, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0796      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6048000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076570935 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.1          |\n",
      "|    explained_variance   | 0.379        |\n",
      "|    learning_rate        | 0.000596     |\n",
      "|    loss                 | -0.000413    |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | 0.00246      |\n",
      "|    std                  | 0.035        |\n",
      "|    value_loss           | 2.15e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0635  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 8544     |\n",
      "|    total_timesteps | 6048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6068160, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0526    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6068160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04487773 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.11       |\n",
      "|    explained_variance   | 0.438      |\n",
      "|    learning_rate        | 0.000593   |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | 0.0177     |\n",
      "|    std                  | 0.035      |\n",
      "|    value_loss           | 4.56e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6088320, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0573      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6088320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060938243 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.1          |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 0.00059      |\n",
      "|    loss                 | -0.00215     |\n",
      "|    n_updates            | 3010         |\n",
      "|    policy_gradient_loss | 0.00202      |\n",
      "|    std                  | 0.0352       |\n",
      "|    value_loss           | 2.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0655  |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 8599     |\n",
      "|    total_timesteps | 6088320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6108480, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.072      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6108480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006294341 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.1         |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.000587    |\n",
      "|    loss                 | -0.000976   |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | 0.00372     |\n",
      "|    std                  | 0.0352      |\n",
      "|    value_loss           | 1.85e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6128640, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0532     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6128640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014049986 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.11        |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.000584    |\n",
      "|    loss                 | 0.00801     |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    std                  | 0.0348      |\n",
      "|    value_loss           | 7.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0669  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 8655     |\n",
      "|    total_timesteps | 6128640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6148800, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0506     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6148800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012293283 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.13        |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.000581    |\n",
      "|    loss                 | -0.000959   |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | 0.016       |\n",
      "|    std                  | 0.0346      |\n",
      "|    value_loss           | 8.85e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6168960, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0678     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6168960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015978735 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.14        |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.000578    |\n",
      "|    loss                 | 0.000723    |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | 0.00484     |\n",
      "|    std                  | 0.0342      |\n",
      "|    value_loss           | 4.78e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0725  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 8711     |\n",
      "|    total_timesteps | 6168960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6189120, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0564      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6189120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075940276 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.15         |\n",
      "|    explained_variance   | 0.413        |\n",
      "|    learning_rate        | 0.000575     |\n",
      "|    loss                 | 0.00285      |\n",
      "|    n_updates            | 3060         |\n",
      "|    policy_gradient_loss | 0.00121      |\n",
      "|    std                  | 0.0342       |\n",
      "|    value_loss           | 1.81e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6209280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0604     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6209280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012073816 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.15        |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.000572    |\n",
      "|    loss                 | -0.000127   |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | 0.00486     |\n",
      "|    std                  | 0.0342      |\n",
      "|    value_loss           | 4.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0698  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 8766     |\n",
      "|    total_timesteps | 6209280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6229440, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0603     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6229440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003994683 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.16        |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.000569    |\n",
      "|    loss                 | 0.000682    |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | 0.00259     |\n",
      "|    std                  | 0.0341      |\n",
      "|    value_loss           | 6.86e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6249600, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0884      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6249600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061248406 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.16         |\n",
      "|    explained_variance   | 0.396        |\n",
      "|    learning_rate        | 0.000566     |\n",
      "|    loss                 | -5.4e-05     |\n",
      "|    n_updates            | 3090         |\n",
      "|    policy_gradient_loss | 0.00236      |\n",
      "|    std                  | 0.034        |\n",
      "|    value_loss           | 2.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0717  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 8821     |\n",
      "|    total_timesteps | 6249600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6269760, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0766     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6269760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018623717 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.000563    |\n",
      "|    loss                 | 0.0108      |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | 0.00279     |\n",
      "|    std                  | 0.0339      |\n",
      "|    value_loss           | 8.54e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6289920, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0696     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6289920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030828785 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 0.00056     |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | 0.0027      |\n",
      "|    std                  | 0.034       |\n",
      "|    value_loss           | 5.46e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0675  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 8876     |\n",
      "|    total_timesteps | 6289920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6310080, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0538     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6310080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028755143 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.000557    |\n",
      "|    loss                 | 0.00574     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.0339      |\n",
      "|    value_loss           | 7.66e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6330240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0546      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6330240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069874264 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.17         |\n",
      "|    explained_variance   | 0.543        |\n",
      "|    learning_rate        | 0.000553     |\n",
      "|    loss                 | 0.00177      |\n",
      "|    n_updates            | 3130         |\n",
      "|    policy_gradient_loss | 0.00183      |\n",
      "|    std                  | 0.034        |\n",
      "|    value_loss           | 5.31e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0658  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 8933     |\n",
      "|    total_timesteps | 6330240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6350400, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0881    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6350400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01213946 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.17       |\n",
      "|    explained_variance   | 0.39       |\n",
      "|    learning_rate        | 0.00055    |\n",
      "|    loss                 | 0.0014     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | 0.00312    |\n",
      "|    std                  | 0.0338     |\n",
      "|    value_loss           | 1.09e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6370560, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0769      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6370560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093893595 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.18         |\n",
      "|    explained_variance   | 0.406        |\n",
      "|    learning_rate        | 0.000547     |\n",
      "|    loss                 | -0.00147     |\n",
      "|    n_updates            | 3150         |\n",
      "|    policy_gradient_loss | 0.00396      |\n",
      "|    std                  | 0.0337       |\n",
      "|    value_loss           | 4.18e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0652  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 8988     |\n",
      "|    total_timesteps | 6370560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6390720, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0695      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6390720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055183084 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.18         |\n",
      "|    explained_variance   | 0.553        |\n",
      "|    learning_rate        | 0.000544     |\n",
      "|    loss                 | -0.00202     |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | 0.00364      |\n",
      "|    std                  | 0.0337       |\n",
      "|    value_loss           | 9.4e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6410880, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0469     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6410880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010325208 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.000541    |\n",
      "|    loss                 | 0.00295     |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | 0.00121     |\n",
      "|    std                  | 0.0339      |\n",
      "|    value_loss           | 5.94e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.062   |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 9043     |\n",
      "|    total_timesteps | 6410880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6431040, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0506      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6431040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076701045 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.17         |\n",
      "|    explained_variance   | -0.0475      |\n",
      "|    learning_rate        | 0.000538     |\n",
      "|    loss                 | 0.00207      |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | 0.0018       |\n",
      "|    std                  | 0.0339       |\n",
      "|    value_loss           | 7.95e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6451200, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6451200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021832231 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.000535    |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | 0.00473     |\n",
      "|    std                  | 0.0336      |\n",
      "|    value_loss           | 3.02e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0615  |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 9100     |\n",
      "|    total_timesteps | 6451200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6471360, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0579     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6471360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016401138 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.000532    |\n",
      "|    loss                 | 0.00498     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | 0.00812     |\n",
      "|    std                  | 0.0335      |\n",
      "|    value_loss           | 4.17e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6491520, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0976     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6491520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077555984 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.19        |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.000529    |\n",
      "|    loss                 | 0.00273     |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    std                  | 0.0335      |\n",
      "|    value_loss           | 1.58e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0606  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 9155     |\n",
      "|    total_timesteps | 6491520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6511680, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.067      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6511680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033490106 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.19        |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.000526    |\n",
      "|    loss                 | -0.00146    |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | 0.0195      |\n",
      "|    std                  | 0.0335      |\n",
      "|    value_loss           | 1.27e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6531840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0485     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6531840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009354318 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.19        |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.000523    |\n",
      "|    loss                 | -0.00155    |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | 8.38e-05    |\n",
      "|    std                  | 0.0338      |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0624  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 9210     |\n",
      "|    total_timesteps | 6531840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6552000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0482     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6552000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012163568 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.00052     |\n",
      "|    loss                 | -0.00288    |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | 0.000316    |\n",
      "|    std                  | 0.0338      |\n",
      "|    value_loss           | 2.81e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6572160, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.056      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6572160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010198543 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.18        |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.000517    |\n",
      "|    loss                 | 0.00283     |\n",
      "|    n_updates            | 3250        |\n",
      "|    policy_gradient_loss | 0.00481     |\n",
      "|    std                  | 0.0338      |\n",
      "|    value_loss           | 1.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.066   |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 9267     |\n",
      "|    total_timesteps | 6572160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6592320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0574      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6592320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034449664 |\n",
      "|    clip_fraction        | 0.0924       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.19         |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.000514     |\n",
      "|    loss                 | 0.000193     |\n",
      "|    n_updates            | 3260         |\n",
      "|    policy_gradient_loss | 0.00126      |\n",
      "|    std                  | 0.0337       |\n",
      "|    value_loss           | 9.96e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6612480, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0508     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6612480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018171474 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.19        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.000511    |\n",
      "|    loss                 | 0.00945     |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | 0.00865     |\n",
      "|    std                  | 0.0336      |\n",
      "|    value_loss           | 1.67e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0665  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 9323     |\n",
      "|    total_timesteps | 6612480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6632640, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0708     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6632640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009710849 |\n",
      "|    clip_fraction        | 0.0805      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.2         |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.000508    |\n",
      "|    loss                 | 0.00195     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | 0.00037     |\n",
      "|    std                  | 0.0336      |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6652800, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0572     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6652800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015271217 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.21        |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.000505    |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | 0.00785     |\n",
      "|    std                  | 0.0334      |\n",
      "|    value_loss           | 2.72e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0631  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 9378     |\n",
      "|    total_timesteps | 6652800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6672960, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0554     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6672960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006598103 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.22        |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.000502    |\n",
      "|    loss                 | 0.00136     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | 0.00285     |\n",
      "|    std                  | 0.0332      |\n",
      "|    value_loss           | 5.5e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6693120, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0696     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6693120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030465875 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.22        |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.000499    |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | 0.00848     |\n",
      "|    std                  | 0.0333      |\n",
      "|    value_loss           | 5.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0606  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 9434     |\n",
      "|    total_timesteps | 6693120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6713280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0591      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6713280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057516447 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.23         |\n",
      "|    explained_variance   | 0.482        |\n",
      "|    learning_rate        | 0.000496     |\n",
      "|    loss                 | 0.00247      |\n",
      "|    n_updates            | 3320         |\n",
      "|    policy_gradient_loss | 0.00225      |\n",
      "|    std                  | 0.0331       |\n",
      "|    value_loss           | 7.91e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6733440, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0408     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6733440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055024397 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.24        |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.000493    |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.0329      |\n",
      "|    value_loss           | 3.86e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0603  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 9490     |\n",
      "|    total_timesteps | 6733440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6753600, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0687      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6753600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050325226 |\n",
      "|    clip_fraction        | 0.0919       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.25         |\n",
      "|    explained_variance   | 0.514        |\n",
      "|    learning_rate        | 0.00049      |\n",
      "|    loss                 | 0.0024       |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | 0.00172      |\n",
      "|    std                  | 0.0326       |\n",
      "|    value_loss           | 5.56e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6773760, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0592      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6773760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073208674 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.27         |\n",
      "|    explained_variance   | 0.42         |\n",
      "|    learning_rate        | 0.000487     |\n",
      "|    loss                 | 0.00185      |\n",
      "|    n_updates            | 3350         |\n",
      "|    policy_gradient_loss | 0.00423      |\n",
      "|    std                  | 0.0324       |\n",
      "|    value_loss           | 3.46e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0614  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 9545     |\n",
      "|    total_timesteps | 6773760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6793920, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0528     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6793920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008803743 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.000484    |\n",
      "|    loss                 | -0.00299    |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | 0.00486     |\n",
      "|    std                  | 0.0324      |\n",
      "|    value_loss           | 7.3e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6814080, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0657     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6814080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006263295 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | 7.43e-05    |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | -4.43e-06   |\n",
      "|    std                  | 0.0324      |\n",
      "|    value_loss           | 1.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0639  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 9601     |\n",
      "|    total_timesteps | 6814080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6834240, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0765     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6834240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005675926 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.000478    |\n",
      "|    loss                 | -0.000816   |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | 0.000713    |\n",
      "|    std                  | 0.0325      |\n",
      "|    value_loss           | 3.23e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6854400, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0554     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6854400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027082453 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.27        |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.000475    |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | 0.00856     |\n",
      "|    std                  | 0.0326      |\n",
      "|    value_loss           | 8.48e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0682  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 9657     |\n",
      "|    total_timesteps | 6854400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6874560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0639    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6874560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01850587 |\n",
      "|    clip_fraction        | 0.0862     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.27       |\n",
      "|    explained_variance   | 0.394      |\n",
      "|    learning_rate        | 0.000472   |\n",
      "|    loss                 | -0.00114   |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | 0.000637   |\n",
      "|    std                  | 0.0324     |\n",
      "|    value_loss           | 1.69e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6894720, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0586     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6894720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006914842 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.28        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.000469    |\n",
      "|    loss                 | 0.00206     |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    std                  | 0.0322      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0693  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 9713     |\n",
      "|    total_timesteps | 6894720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6914880, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0606     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6914880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007272176 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.29        |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.000466    |\n",
      "|    loss                 | 0.00012     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    std                  | 0.0319      |\n",
      "|    value_loss           | 9.09e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6935040, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.063      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6935040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034090858 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.3         |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.000463    |\n",
      "|    loss                 | 0.00362     |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | 0.00293     |\n",
      "|    std                  | 0.0319      |\n",
      "|    value_loss           | 9.4e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0665  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 9769     |\n",
      "|    total_timesteps | 6935040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6955200, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0625     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6955200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010213741 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.31        |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00046     |\n",
      "|    loss                 | 2.1e-05     |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | 0.0064      |\n",
      "|    std                  | 0.0317      |\n",
      "|    value_loss           | 5.29e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6975360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0659     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6975360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032711186 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.32        |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.000457    |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | 0.00362     |\n",
      "|    std                  | 0.0315      |\n",
      "|    value_loss           | 9.82e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0632  |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 9825     |\n",
      "|    total_timesteps | 6975360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6995520, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0711     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6995520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009944124 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.33        |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.000454    |\n",
      "|    loss                 | -0.00325    |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | 0.000806    |\n",
      "|    std                  | 0.0313      |\n",
      "|    value_loss           | 1.32e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7015680, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0941     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7015680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019357232 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.34        |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.000451    |\n",
      "|    loss                 | 0.0068      |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | 0.00458     |\n",
      "|    std                  | 0.0312      |\n",
      "|    value_loss           | 7.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0617  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 9881     |\n",
      "|    total_timesteps | 7015680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7035840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0536     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7035840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009800721 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.35        |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.000448    |\n",
      "|    loss                 | 0.00279     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    std                  | 0.031       |\n",
      "|    value_loss           | 4.56e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7056000, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0928     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7056000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028541654 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.36        |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.000445    |\n",
      "|    loss                 | -0.00136    |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | 0.00391     |\n",
      "|    std                  | 0.0309      |\n",
      "|    value_loss           | 8.56e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0612  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 9937     |\n",
      "|    total_timesteps | 7056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7076160, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0539    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7076160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01341027 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.36       |\n",
      "|    explained_variance   | 0.416      |\n",
      "|    learning_rate        | 0.000442   |\n",
      "|    loss                 | 0.00199    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | 0.00123    |\n",
      "|    std                  | 0.0309     |\n",
      "|    value_loss           | 1.83e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7096320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0599     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7096320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032015763 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.37        |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.000439    |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | 0.00407     |\n",
      "|    std                  | 0.0309      |\n",
      "|    value_loss           | 8.41e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0629  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 9992     |\n",
      "|    total_timesteps | 7096320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7116480, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0427     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7116480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034228753 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.37        |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.000436    |\n",
      "|    loss                 | 0.0261      |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | 0.00788     |\n",
      "|    std                  | 0.0308      |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7136640, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0607     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7136640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017873503 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.37        |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.000433    |\n",
      "|    loss                 | 0.00408     |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | 0.000673    |\n",
      "|    std                  | 0.0308      |\n",
      "|    value_loss           | 1.13e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0623  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 10048    |\n",
      "|    total_timesteps | 7136640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7156800, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0828     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7156800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010146874 |\n",
      "|    clip_fraction        | 0.0676      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.38        |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00043     |\n",
      "|    loss                 | -0.00274    |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | 0.000458    |\n",
      "|    std                  | 0.0306      |\n",
      "|    value_loss           | 1.73e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7176960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0483     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7176960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014709583 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.39        |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.000426    |\n",
      "|    loss                 | -0.000324   |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | 0.00183     |\n",
      "|    std                  | 0.0304      |\n",
      "|    value_loss           | 2.65e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0699  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 10103    |\n",
      "|    total_timesteps | 7176960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7197120, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0492     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7197120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097260594 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.39        |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.000423    |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | 0.0209      |\n",
      "|    std                  | 0.0305      |\n",
      "|    value_loss           | 2.69e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7217280, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0612    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7217280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00455041 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.39       |\n",
      "|    explained_variance   | 0.494      |\n",
      "|    learning_rate        | 0.00042    |\n",
      "|    loss                 | -0.00253   |\n",
      "|    n_updates            | 3570       |\n",
      "|    policy_gradient_loss | 0.00096    |\n",
      "|    std                  | 0.0305     |\n",
      "|    value_loss           | 1.38e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0695  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 10159    |\n",
      "|    total_timesteps | 7217280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7237440, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0809     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7237440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005127111 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.000417    |\n",
      "|    loss                 | -0.00286    |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | 0.000132    |\n",
      "|    std                  | 0.0304      |\n",
      "|    value_loss           | 4.57e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7257600, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0622     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7257600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007017823 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.4         |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.000414    |\n",
      "|    loss                 | 0.000841    |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | 0.00321     |\n",
      "|    std                  | 0.0303      |\n",
      "|    value_loss           | 8.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0679  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 10215    |\n",
      "|    total_timesteps | 7257600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7277760, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0632     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7277760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029177476 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.41        |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.000411    |\n",
      "|    loss                 | 0.00972     |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | 0.00826     |\n",
      "|    std                  | 0.0302      |\n",
      "|    value_loss           | 9.34e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7297920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0561     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7297920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006247406 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.41        |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.000408    |\n",
      "|    loss                 | -0.000116   |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | 0.0026      |\n",
      "|    std                  | 0.0301      |\n",
      "|    value_loss           | 4.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0655  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 10271    |\n",
      "|    total_timesteps | 7297920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7318080, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0568     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7318080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023460092 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.42        |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.000405    |\n",
      "|    loss                 | -0.000325   |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | 0.00455     |\n",
      "|    std                  | 0.0298      |\n",
      "|    value_loss           | 3.13e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7338240, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0593    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7338240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00859667 |\n",
      "|    clip_fraction        | 0.0868     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.43       |\n",
      "|    explained_variance   | 0.201      |\n",
      "|    learning_rate        | 0.000402   |\n",
      "|    loss                 | -0.0027    |\n",
      "|    n_updates            | 3630       |\n",
      "|    policy_gradient_loss | 0.000449   |\n",
      "|    std                  | 0.0297     |\n",
      "|    value_loss           | 7.92e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0608  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 10326    |\n",
      "|    total_timesteps | 7338240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7358400, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0664    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7358400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00911653 |\n",
      "|    clip_fraction        | 0.0885     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.45       |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.000399   |\n",
      "|    loss                 | 0.00683    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | 0.00238    |\n",
      "|    std                  | 0.0295     |\n",
      "|    value_loss           | 3.2e-08    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7378560, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0742      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7378560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049489047 |\n",
      "|    clip_fraction        | 0.218        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.46         |\n",
      "|    explained_variance   | 0.516        |\n",
      "|    learning_rate        | 0.000396     |\n",
      "|    loss                 | -0.00363     |\n",
      "|    n_updates            | 3650         |\n",
      "|    policy_gradient_loss | 0.00848      |\n",
      "|    std                  | 0.0294       |\n",
      "|    value_loss           | 4.74e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0628  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 10382    |\n",
      "|    total_timesteps | 7378560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7398720, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0493      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7398720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045198156 |\n",
      "|    clip_fraction        | 0.0926       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.47         |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.000393     |\n",
      "|    loss                 | -0.000281    |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | 0.00243      |\n",
      "|    std                  | 0.0291       |\n",
      "|    value_loss           | 3.72e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7418880, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0667     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7418880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007881215 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.48        |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.00039     |\n",
      "|    loss                 | 0.0033      |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | 0.000409    |\n",
      "|    std                  | 0.0291      |\n",
      "|    value_loss           | 1.5e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0602  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 10438    |\n",
      "|    total_timesteps | 7418880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7439040, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0744      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7439040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098237125 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.48         |\n",
      "|    explained_variance   | 0.0425       |\n",
      "|    learning_rate        | 0.000387     |\n",
      "|    loss                 | -0.000346    |\n",
      "|    n_updates            | 3680         |\n",
      "|    policy_gradient_loss | 0.00151      |\n",
      "|    std                  | 0.0291       |\n",
      "|    value_loss           | 5.24e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7459200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0557     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7459200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007563547 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.49        |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.000384    |\n",
      "|    loss                 | 0.00223     |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | 0.00261     |\n",
      "|    std                  | 0.0289      |\n",
      "|    value_loss           | 6.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.061   |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 10493    |\n",
      "|    total_timesteps | 7459200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7479360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0452     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7479360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007571636 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.5         |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.000381    |\n",
      "|    loss                 | 0.000664    |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | 0.00209     |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7499520, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0566     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7499520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009660616 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.51        |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.000378    |\n",
      "|    loss                 | 0.000537    |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | 0.00221     |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 1.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0621  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 10549    |\n",
      "|    total_timesteps | 7499520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7519680, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0561     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7519680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008639432 |\n",
      "|    clip_fraction        | 0.0868      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.51        |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.000375    |\n",
      "|    loss                 | 0.000612    |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    std                  | 0.0288      |\n",
      "|    value_loss           | 7.42e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7539840, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0691     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7539840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010836376 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.51        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.000372    |\n",
      "|    loss                 | 0.00537     |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    std                  | 0.0286      |\n",
      "|    value_loss           | 3.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0579  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 10604    |\n",
      "|    total_timesteps | 7539840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7560000, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.063     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7560000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02456629 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.52       |\n",
      "|    explained_variance   | 0.483      |\n",
      "|    learning_rate        | 0.000369   |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | 0.00291    |\n",
      "|    std                  | 0.0285     |\n",
      "|    value_loss           | 3.27e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7580160, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0486     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7580160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010302879 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.000366    |\n",
      "|    loss                 | 0.000222    |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | 0.00126     |\n",
      "|    std                  | 0.0285      |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0591  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 10660    |\n",
      "|    total_timesteps | 7580160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7600320, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.058      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7600320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012351271 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.000363    |\n",
      "|    loss                 | 0.00158     |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | 0.00422     |\n",
      "|    std                  | 0.0284      |\n",
      "|    value_loss           | 8.89e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7620480, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0738     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7620480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010309062 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.00036     |\n",
      "|    loss                 | 0.000834    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | 0.00154     |\n",
      "|    std                  | 0.0285      |\n",
      "|    value_loss           | 3.54e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0586  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 10716    |\n",
      "|    total_timesteps | 7620480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7640640, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0565     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7640640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009184309 |\n",
      "|    clip_fraction        | 0.0902      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.000357    |\n",
      "|    loss                 | -0.00188    |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | 0.00117     |\n",
      "|    std                  | 0.0285      |\n",
      "|    value_loss           | 8.04e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7660800, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0481     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7660800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009469313 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.000354    |\n",
      "|    loss                 | 0.00391     |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.000408   |\n",
      "|    std                  | 0.0285      |\n",
      "|    value_loss           | 4.16e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0618  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 10772    |\n",
      "|    total_timesteps | 7660800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7680960, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0691     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7680960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006326687 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.000351    |\n",
      "|    loss                 | 0.000691    |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | 0.00333     |\n",
      "|    std                  | 0.0284      |\n",
      "|    value_loss           | 8.89e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7701120, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0642      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7701120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074680625 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.53         |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 0.000348     |\n",
      "|    loss                 | 0.000978     |\n",
      "|    n_updates            | 3810         |\n",
      "|    policy_gradient_loss | 0.00272      |\n",
      "|    std                  | 0.0283       |\n",
      "|    value_loss           | 5.75e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0646  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 10827    |\n",
      "|    total_timesteps | 7701120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7721280, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0759     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7721280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005342957 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.000345    |\n",
      "|    loss                 | -0.00313    |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    std                  | 0.0283      |\n",
      "|    value_loss           | 1.96e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7741440, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0634      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7741440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059547974 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.53         |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 0.000342     |\n",
      "|    loss                 | -0.000549    |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | 0.00176      |\n",
      "|    std                  | 0.0282       |\n",
      "|    value_loss           | 6.78e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0608  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 10883    |\n",
      "|    total_timesteps | 7741440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7761600, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7761600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008670848 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.54        |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.000339    |\n",
      "|    loss                 | 0.000613    |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | 0.00194     |\n",
      "|    std                  | 0.0281      |\n",
      "|    value_loss           | 3.44e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7781760, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0854     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7781760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024650937 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.55        |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.000336    |\n",
      "|    loss                 | 0.00436     |\n",
      "|    n_updates            | 3850        |\n",
      "|    policy_gradient_loss | 0.00352     |\n",
      "|    std                  | 0.028       |\n",
      "|    value_loss           | 5.16e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0621  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 10939    |\n",
      "|    total_timesteps | 7781760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7801920, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.056      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7801920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009769189 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.56        |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.000333    |\n",
      "|    loss                 | 0.0017      |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | 0.00356     |\n",
      "|    std                  | 0.0279      |\n",
      "|    value_loss           | 2.54e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7822080, episode_reward=-0.09 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0884     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7822080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027795304 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.56        |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00033     |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.00352     |\n",
      "|    std                  | 0.0279      |\n",
      "|    value_loss           | 9.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0601  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 10995    |\n",
      "|    total_timesteps | 7822080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7842240, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0724     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7842240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012363423 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.56        |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.000327    |\n",
      "|    loss                 | 6.57e-05    |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | 0.000728    |\n",
      "|    std                  | 0.0277      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7862400, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.051       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7862400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042047487 |\n",
      "|    clip_fraction        | 0.0664       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.58         |\n",
      "|    explained_variance   | 0.353        |\n",
      "|    learning_rate        | 0.000324     |\n",
      "|    loss                 | -0.0014      |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | 0.000128     |\n",
      "|    std                  | 0.0276       |\n",
      "|    value_loss           | 7.45e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0594  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 11051    |\n",
      "|    total_timesteps | 7862400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7882560, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0766     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7882560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030687183 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.59        |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.000321    |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.0275      |\n",
      "|    value_loss           | 1.6e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7902720, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0506     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7902720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021496883 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.59        |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.000318    |\n",
      "|    loss                 | 0.00809     |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | 0.0042      |\n",
      "|    std                  | 0.0274      |\n",
      "|    value_loss           | 5.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0577  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 11107    |\n",
      "|    total_timesteps | 7902720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7922880, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0583     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7922880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039165147 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.6         |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.000315    |\n",
      "|    loss                 | 0.0317      |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | 0.00485     |\n",
      "|    std                  | 0.0274      |\n",
      "|    value_loss           | 3.97e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7943040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0772     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7943040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021007983 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.6         |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.000312    |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    std                  | 0.0273      |\n",
      "|    value_loss           | 7.38e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0572  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 11162    |\n",
      "|    total_timesteps | 7943040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7963200, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0678     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7963200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010457197 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.6         |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.000309    |\n",
      "|    loss                 | -0.00246    |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | 0.00283     |\n",
      "|    std                  | 0.0273      |\n",
      "|    value_loss           | 1.16e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7983360, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0435     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7983360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006981653 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.61        |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.000306    |\n",
      "|    loss                 | 0.000868    |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | 0.00077     |\n",
      "|    std                  | 0.0272      |\n",
      "|    value_loss           | 5.7e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0567  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 11218    |\n",
      "|    total_timesteps | 7983360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8003520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0548     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8003520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020243317 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.61        |\n",
      "|    explained_variance   | 0.0749      |\n",
      "|    learning_rate        | 0.000302    |\n",
      "|    loss                 | 0.00946     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | 0.00192     |\n",
      "|    std                  | 0.0271      |\n",
      "|    value_loss           | 4.43e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8023680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0485     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8023680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007330546 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.62        |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.000299    |\n",
      "|    loss                 | 0.00327     |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | 0.00254     |\n",
      "|    std                  | 0.027       |\n",
      "|    value_loss           | 4.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0564  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 11274    |\n",
      "|    total_timesteps | 8023680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8043840, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0767     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8043840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009399915 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.62        |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.000296    |\n",
      "|    loss                 | -0.000157   |\n",
      "|    n_updates            | 3980        |\n",
      "|    policy_gradient_loss | 0.00205     |\n",
      "|    std                  | 0.027       |\n",
      "|    value_loss           | 4.42e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8064000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0466      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8064000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043426286 |\n",
      "|    clip_fraction        | 0.0796       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.63         |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.000293     |\n",
      "|    loss                 | -0.00219     |\n",
      "|    n_updates            | 3990         |\n",
      "|    policy_gradient_loss | 0.000852     |\n",
      "|    std                  | 0.0268       |\n",
      "|    value_loss           | 5.75e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0528  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 11329    |\n",
      "|    total_timesteps | 8064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8084160, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0718      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8084160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056206984 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.64         |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.00029      |\n",
      "|    loss                 | -5.12e-06    |\n",
      "|    n_updates            | 4000         |\n",
      "|    policy_gradient_loss | 0.00182      |\n",
      "|    std                  | 0.0267       |\n",
      "|    value_loss           | 3.72e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8104320, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0668     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8104320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011420965 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.65        |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.000287    |\n",
      "|    loss                 | 0.00267     |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | 0.00469     |\n",
      "|    std                  | 0.0267      |\n",
      "|    value_loss           | 7.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0583  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 11385    |\n",
      "|    total_timesteps | 8104320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8124480, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0546      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8124480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064204475 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.65         |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 0.000284     |\n",
      "|    loss                 | -0.00344     |\n",
      "|    n_updates            | 4020         |\n",
      "|    policy_gradient_loss | 0.00159      |\n",
      "|    std                  | 0.0266       |\n",
      "|    value_loss           | 4.29e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8144640, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0579      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8144640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068252226 |\n",
      "|    clip_fraction        | 0.0939       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.66         |\n",
      "|    explained_variance   | 0.293        |\n",
      "|    learning_rate        | 0.000281     |\n",
      "|    loss                 | 0.00711      |\n",
      "|    n_updates            | 4030         |\n",
      "|    policy_gradient_loss | 0.00147      |\n",
      "|    std                  | 0.0265       |\n",
      "|    value_loss           | 3.69e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0565  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 11441    |\n",
      "|    total_timesteps | 8144640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8164800, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0555    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8164800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00452969 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.67       |\n",
      "|    explained_variance   | 0.432      |\n",
      "|    learning_rate        | 0.000278   |\n",
      "|    loss                 | -0.0011    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | 0.00204    |\n",
      "|    std                  | 0.0264     |\n",
      "|    value_loss           | 1.95e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8184960, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0537      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8184960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070683793 |\n",
      "|    clip_fraction        | 0.081        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.67         |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.000275     |\n",
      "|    loss                 | 0.000127     |\n",
      "|    n_updates            | 4050         |\n",
      "|    policy_gradient_loss | -7.42e-07    |\n",
      "|    std                  | 0.0265       |\n",
      "|    value_loss           | 1.67e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0612  |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 11496    |\n",
      "|    total_timesteps | 8184960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8205120, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0485     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8205120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007107838 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.67        |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.000272    |\n",
      "|    loss                 | -0.000729   |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | 0.0029      |\n",
      "|    std                  | 0.0264      |\n",
      "|    value_loss           | 1.22e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8225280, episode_reward=-0.06 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0592    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8225280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00634507 |\n",
      "|    clip_fraction        | 0.0751     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.67       |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.000269   |\n",
      "|    loss                 | -0.00112   |\n",
      "|    n_updates            | 4070       |\n",
      "|    policy_gradient_loss | -0.000338  |\n",
      "|    std                  | 0.0264     |\n",
      "|    value_loss           | 5.81e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0571  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 11552    |\n",
      "|    total_timesteps | 8225280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8245440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0503     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8245440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008584142 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.68        |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.000266    |\n",
      "|    loss                 | -0.000876   |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | 0.00212     |\n",
      "|    std                  | 0.0263      |\n",
      "|    value_loss           | 8.75e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8265600, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0671     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8265600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004712721 |\n",
      "|    clip_fraction        | 0.0741      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.69        |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | -0.00259    |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | 0.000326    |\n",
      "|    std                  | 0.0263      |\n",
      "|    value_loss           | 6.67e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0576  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 11607    |\n",
      "|    total_timesteps | 8265600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8285760, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0846     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8285760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004993806 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.69        |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.00026     |\n",
      "|    loss                 | 0.00197     |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | 0.00183     |\n",
      "|    std                  | 0.0263      |\n",
      "|    value_loss           | 9.39e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8305920, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0676     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8305920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006251212 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.68        |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.000257    |\n",
      "|    loss                 | -0.00138    |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | 0.00273     |\n",
      "|    std                  | 0.0263      |\n",
      "|    value_loss           | 6.5e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.059   |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 11663    |\n",
      "|    total_timesteps | 8305920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8326080, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0667      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8326080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051426133 |\n",
      "|    clip_fraction        | 0.142        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.69         |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 0.000254     |\n",
      "|    loss                 | -0.000842    |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | 0.0048       |\n",
      "|    std                  | 0.0263       |\n",
      "|    value_loss           | 8.4e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8346240, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0475     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8346240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015405069 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.69        |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 0.00518     |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | 0.00446     |\n",
      "|    std                  | 0.0262      |\n",
      "|    value_loss           | 6.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0626  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 11718    |\n",
      "|    total_timesteps | 8346240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8366400, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0545     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8366400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008866955 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.7         |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.000248    |\n",
      "|    loss                 | 0.00196     |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | 0.000446    |\n",
      "|    std                  | 0.0261      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8386560, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0582     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8386560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010363799 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.7         |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.000245    |\n",
      "|    loss                 | 0.00252     |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | 0.00076     |\n",
      "|    std                  | 0.0261      |\n",
      "|    value_loss           | 1.47e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0613  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 11774    |\n",
      "|    total_timesteps | 8386560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8406720, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0566      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8406720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064120404 |\n",
      "|    clip_fraction        | 0.0809       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.71         |\n",
      "|    explained_variance   | 0.467        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | -0.00215     |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | 0.00121      |\n",
      "|    std                  | 0.0259       |\n",
      "|    value_loss           | 6.33e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8426880, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0544      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8426880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075269938 |\n",
      "|    clip_fraction        | 0.0807       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.72         |\n",
      "|    explained_variance   | 0.496        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | 0.00133      |\n",
      "|    n_updates            | 4170         |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    std                  | 0.0258       |\n",
      "|    value_loss           | 7.54e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0605  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 11830    |\n",
      "|    total_timesteps | 8426880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8447040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0773     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8447040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008238826 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.72        |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.000236    |\n",
      "|    loss                 | 0.000164    |\n",
      "|    n_updates            | 4180        |\n",
      "|    policy_gradient_loss | 0.000987    |\n",
      "|    std                  | 0.0257      |\n",
      "|    value_loss           | 3e-08       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8467200, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0628     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8467200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006205244 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.73        |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.000233    |\n",
      "|    loss                 | -0.00128    |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -4.53e-05   |\n",
      "|    std                  | 0.0256      |\n",
      "|    value_loss           | 1.18e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0594  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 11885    |\n",
      "|    total_timesteps | 8467200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8487360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.065       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8487360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036610616 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.74         |\n",
      "|    explained_variance   | 0.426        |\n",
      "|    learning_rate        | 0.00023      |\n",
      "|    loss                 | -0.000641    |\n",
      "|    n_updates            | 4200         |\n",
      "|    policy_gradient_loss | 0.000728     |\n",
      "|    std                  | 0.0255       |\n",
      "|    value_loss           | 2.49e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8507520, episode_reward=-0.08 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0806     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8507520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006291168 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.75        |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.000227    |\n",
      "|    loss                 | 0.000438    |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | 0.000738    |\n",
      "|    std                  | 0.0254      |\n",
      "|    value_loss           | 4.2e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.057   |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 11941    |\n",
      "|    total_timesteps | 8507520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8527680, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0477     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8527680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005332497 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.75        |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.000224    |\n",
      "|    loss                 | -0.000208   |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | 0.00243     |\n",
      "|    std                  | 0.0253      |\n",
      "|    value_loss           | 9.14e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8547840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0508      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8547840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125904195 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.76         |\n",
      "|    explained_variance   | 0.501        |\n",
      "|    learning_rate        | 0.000221     |\n",
      "|    loss                 | 0.00269      |\n",
      "|    n_updates            | 4230         |\n",
      "|    policy_gradient_loss | 0.00314      |\n",
      "|    std                  | 0.0252       |\n",
      "|    value_loss           | 2.16e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0561  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 11997    |\n",
      "|    total_timesteps | 8547840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8568000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0678      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8568000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076295296 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.76         |\n",
      "|    explained_variance   | 0.574        |\n",
      "|    learning_rate        | 0.000218     |\n",
      "|    loss                 | -0.00412     |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | 0.00196      |\n",
      "|    std                  | 0.0251       |\n",
      "|    value_loss           | 9.73e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8588160, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0739      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8588160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076481113 |\n",
      "|    clip_fraction        | 0.0708       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.516        |\n",
      "|    learning_rate        | 0.000215     |\n",
      "|    loss                 | 0.00515      |\n",
      "|    n_updates            | 4250         |\n",
      "|    policy_gradient_loss | 0.000679     |\n",
      "|    std                  | 0.0251       |\n",
      "|    value_loss           | 2.36e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0544  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 12052    |\n",
      "|    total_timesteps | 8588160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8608320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0502      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8608320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055402024 |\n",
      "|    clip_fraction        | 0.0951       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.478        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -0.0004      |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | 0.00134      |\n",
      "|    std                  | 0.025        |\n",
      "|    value_loss           | 6.09e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8628480, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0663     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8628480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009845755 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.78        |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.000209    |\n",
      "|    loss                 | 0.00201     |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | 0.00054     |\n",
      "|    std                  | 0.025       |\n",
      "|    value_loss           | 6.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0551  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 12108    |\n",
      "|    total_timesteps | 8628480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8648640, episode_reward=-0.07 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0653     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8648640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012197792 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.77        |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.000206    |\n",
      "|    loss                 | -0.00263    |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | 0.000849    |\n",
      "|    std                  | 0.025       |\n",
      "|    value_loss           | 6.46e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8668800, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0631      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8668800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046381657 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.641        |\n",
      "|    learning_rate        | 0.000203     |\n",
      "|    loss                 | 0.000345     |\n",
      "|    n_updates            | 4290         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    std                  | 0.0251       |\n",
      "|    value_loss           | 2.31e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0618  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 12164    |\n",
      "|    total_timesteps | 8668800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8688960, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.04       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8688960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007220456 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.77        |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.000761    |\n",
      "|    n_updates            | 4300        |\n",
      "|    policy_gradient_loss | 0.00336     |\n",
      "|    std                  | 0.0252      |\n",
      "|    value_loss           | 2.11e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8709120, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0756     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8709120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016586814 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.77        |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.000197    |\n",
      "|    loss                 | 0.0049      |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | 0.00168     |\n",
      "|    std                  | 0.0251      |\n",
      "|    value_loss           | 6.83e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0614  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 12221    |\n",
      "|    total_timesteps | 8709120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8729280, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0586      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8729280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072783977 |\n",
      "|    clip_fraction        | 0.0864       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.409        |\n",
      "|    learning_rate        | 0.000194     |\n",
      "|    loss                 | -0.000632    |\n",
      "|    n_updates            | 4320         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    std                  | 0.0251       |\n",
      "|    value_loss           | 1.37e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8749440, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0436      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8749440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043144426 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.77         |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 0.000191     |\n",
      "|    loss                 | -0.00559     |\n",
      "|    n_updates            | 4330         |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    std                  | 0.0251       |\n",
      "|    value_loss           | 2.86e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0664  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 12277    |\n",
      "|    total_timesteps | 8749440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8769600, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0702    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8769600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00497303 |\n",
      "|    clip_fraction        | 0.06       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.78       |\n",
      "|    explained_variance   | 0.46       |\n",
      "|    learning_rate        | 0.000188   |\n",
      "|    loss                 | 0.00143    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.000723  |\n",
      "|    std                  | 0.0251     |\n",
      "|    value_loss           | 1.54e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8789760, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0695     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8789760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007378849 |\n",
      "|    clip_fraction        | 0.0834      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.77        |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.000185    |\n",
      "|    loss                 | 0.00418     |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | 0.000678    |\n",
      "|    std                  | 0.0252      |\n",
      "|    value_loss           | 4.69e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0604  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 12333    |\n",
      "|    total_timesteps | 8789760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8809920, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0554     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8809920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021076696 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.77        |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.000182    |\n",
      "|    loss                 | 0.00559     |\n",
      "|    n_updates            | 4360        |\n",
      "|    policy_gradient_loss | 0.000228    |\n",
      "|    std                  | 0.0252      |\n",
      "|    value_loss           | 1.17e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8830080, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0688    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8830080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00708259 |\n",
      "|    clip_fraction        | 0.0687     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.78       |\n",
      "|    explained_variance   | 0.536      |\n",
      "|    learning_rate        | 0.000179   |\n",
      "|    loss                 | 0.00326    |\n",
      "|    n_updates            | 4370       |\n",
      "|    policy_gradient_loss | 1.27e-05   |\n",
      "|    std                  | 0.0251     |\n",
      "|    value_loss           | 1.07e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0672  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 12388    |\n",
      "|    total_timesteps | 8830080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8850240, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0627      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8850240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050727455 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.78         |\n",
      "|    explained_variance   | 0.544        |\n",
      "|    learning_rate        | 0.000175     |\n",
      "|    loss                 | 0.00141      |\n",
      "|    n_updates            | 4380         |\n",
      "|    policy_gradient_loss | -0.000126    |\n",
      "|    std                  | 0.025        |\n",
      "|    value_loss           | 1.5e-07      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8870400, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.06      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8870400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00955831 |\n",
      "|    clip_fraction        | 0.0785     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.78       |\n",
      "|    explained_variance   | 0.573      |\n",
      "|    learning_rate        | 0.000172   |\n",
      "|    loss                 | 0.00145    |\n",
      "|    n_updates            | 4390       |\n",
      "|    policy_gradient_loss | -0.0013    |\n",
      "|    std                  | 0.025      |\n",
      "|    value_loss           | 2.24e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0646  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 12444    |\n",
      "|    total_timesteps | 8870400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8890560, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0473      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8890560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077530844 |\n",
      "|    clip_fraction        | 0.0699       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.000169     |\n",
      "|    loss                 | 0.0033       |\n",
      "|    n_updates            | 4400         |\n",
      "|    policy_gradient_loss | 0.000643     |\n",
      "|    std                  | 0.025        |\n",
      "|    value_loss           | 3.81e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8910720, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0596      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8910720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047260625 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.467        |\n",
      "|    learning_rate        | 0.000166     |\n",
      "|    loss                 | -0.00217     |\n",
      "|    n_updates            | 4410         |\n",
      "|    policy_gradient_loss | -0.000712    |\n",
      "|    std                  | 0.025        |\n",
      "|    value_loss           | 7.69e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0657  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 12500    |\n",
      "|    total_timesteps | 8910720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8930880, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0636     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8930880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009246401 |\n",
      "|    clip_fraction        | 0.0598      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.78        |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.000163    |\n",
      "|    loss                 | -0.00153    |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.000677   |\n",
      "|    std                  | 0.025       |\n",
      "|    value_loss           | 8.1e-08     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8951040, episode_reward=-0.05 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0547     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8951040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009730304 |\n",
      "|    clip_fraction        | 0.0886      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.78        |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00016     |\n",
      "|    loss                 | 0.00715     |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    std                  | 0.025       |\n",
      "|    value_loss           | 4.56e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0593  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 12556    |\n",
      "|    total_timesteps | 8951040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8971200, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0765     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8971200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007491003 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.79        |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.000157    |\n",
      "|    loss                 | 0.00039     |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | 0.000446    |\n",
      "|    std                  | 0.025       |\n",
      "|    value_loss           | 4.47e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8991360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0473      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8991360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059522884 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 0.000154     |\n",
      "|    loss                 | -0.00175     |\n",
      "|    n_updates            | 4450         |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    std                  | 0.025        |\n",
      "|    value_loss           | 2.5e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0614  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 12612    |\n",
      "|    total_timesteps | 8991360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9011520, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0633      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9011520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028800173 |\n",
      "|    clip_fraction        | 0.0785       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.526        |\n",
      "|    learning_rate        | 0.000151     |\n",
      "|    loss                 | -0.000254    |\n",
      "|    n_updates            | 4460         |\n",
      "|    policy_gradient_loss | 0.000387     |\n",
      "|    std                  | 0.025        |\n",
      "|    value_loss           | 1.43e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9031680, episode_reward=-0.08 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0766      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9031680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038221525 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 0.000148     |\n",
      "|    loss                 | -0.000445    |\n",
      "|    n_updates            | 4470         |\n",
      "|    policy_gradient_loss | -0.000406    |\n",
      "|    std                  | 0.0249       |\n",
      "|    value_loss           | 1.15e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0602  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 12667    |\n",
      "|    total_timesteps | 9031680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9051840, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0721      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9051840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031690379 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.79         |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.0025      |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.000914    |\n",
      "|    std                  | 0.0249       |\n",
      "|    value_loss           | 3.11e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9072000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0543      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9072000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044489577 |\n",
      "|    clip_fraction        | 0.0672       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.8          |\n",
      "|    explained_variance   | 0.303        |\n",
      "|    learning_rate        | 0.000142     |\n",
      "|    loss                 | -0.00141     |\n",
      "|    n_updates            | 4490         |\n",
      "|    policy_gradient_loss | 0.000239     |\n",
      "|    std                  | 0.0248       |\n",
      "|    value_loss           | 7.37e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.062   |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 12724    |\n",
      "|    total_timesteps | 9072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9092160, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0876     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9092160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004712249 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.8         |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.000139    |\n",
      "|    loss                 | 0.000606    |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | 0.00127     |\n",
      "|    std                  | 0.0248      |\n",
      "|    value_loss           | 5.58e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9112320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0493     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9112320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004089694 |\n",
      "|    clip_fraction        | 0.0518      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.8         |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.000136    |\n",
      "|    loss                 | -0.000982   |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -0.000722   |\n",
      "|    std                  | 0.0247      |\n",
      "|    value_loss           | 3.81e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0594  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 12779    |\n",
      "|    total_timesteps | 9112320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9132480, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0626      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9132480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017931829 |\n",
      "|    clip_fraction        | 0.0747       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.8          |\n",
      "|    explained_variance   | 0.416        |\n",
      "|    learning_rate        | 0.000133     |\n",
      "|    loss                 | 0.000115     |\n",
      "|    n_updates            | 4520         |\n",
      "|    policy_gradient_loss | 0.00169      |\n",
      "|    std                  | 0.0247       |\n",
      "|    value_loss           | 7.29e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9152640, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0612     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9152640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003706426 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.8         |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.00013     |\n",
      "|    loss                 | -0.000445   |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | 0.000757    |\n",
      "|    std                  | 0.0247      |\n",
      "|    value_loss           | 1.79e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0575  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 12835    |\n",
      "|    total_timesteps | 9152640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9172800, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0648     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9172800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004140991 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.000127    |\n",
      "|    loss                 | -0.000951   |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.000844   |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9192960, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0649      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9192960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061783567 |\n",
      "|    clip_fraction        | 0.0756       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.316        |\n",
      "|    learning_rate        | 0.000124     |\n",
      "|    loss                 | -0.000535    |\n",
      "|    n_updates            | 4550         |\n",
      "|    policy_gradient_loss | 0.000761     |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 1.62e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 12890    |\n",
      "|    total_timesteps | 9192960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9213120, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0412     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9213120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005499494 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.000121    |\n",
      "|    loss                 | -0.00309    |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | 0.000121    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 8.34e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9233280, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.079       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9233280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044368543 |\n",
      "|    clip_fraction        | 0.0537       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.000118     |\n",
      "|    loss                 | -0.00234     |\n",
      "|    n_updates            | 4570         |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 2.84e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0623  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 12946    |\n",
      "|    total_timesteps | 9233280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9253440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0531     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9253440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009604173 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.000115    |\n",
      "|    loss                 | -0.000745   |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.000347   |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 4.88e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9273600, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0586     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9273600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002883773 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.000112    |\n",
      "|    loss                 | -0.000715   |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -6.81e-05   |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 7.2e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0619  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 13001    |\n",
      "|    total_timesteps | 9273600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9293760, episode_reward=-0.06 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0604      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9293760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033818323 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.299        |\n",
      "|    learning_rate        | 0.000109     |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 4600         |\n",
      "|    policy_gradient_loss | -0.000623    |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 7.57e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9313920, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0726      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9313920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063593616 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | -0.00282     |\n",
      "|    n_updates            | 4610         |\n",
      "|    policy_gradient_loss | -0.000536    |\n",
      "|    std                  | 0.0247       |\n",
      "|    value_loss           | 8.14e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0619  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 13057    |\n",
      "|    total_timesteps | 9313920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9334080, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0758     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9334080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002766834 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.000103    |\n",
      "|    loss                 | 0.000116    |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -7.4e-05    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 8.09e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9354240, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0555      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9354240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038730497 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 9.99e-05     |\n",
      "|    loss                 | -0.000197    |\n",
      "|    n_updates            | 4630         |\n",
      "|    policy_gradient_loss | -0.000905    |\n",
      "|    std                  | 0.0247       |\n",
      "|    value_loss           | 3.44e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0598  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 13112    |\n",
      "|    total_timesteps | 9354240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9374400, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0551     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9374400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004007755 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 9.69e-05    |\n",
      "|    loss                 | -0.000205   |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.000887   |\n",
      "|    std                  | 0.0247      |\n",
      "|    value_loss           | 4.08e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9394560, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0553      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9394560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038597581 |\n",
      "|    clip_fraction        | 0.0587       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.266        |\n",
      "|    learning_rate        | 9.38e-05     |\n",
      "|    loss                 | -0.000617    |\n",
      "|    n_updates            | 4650         |\n",
      "|    policy_gradient_loss | 0.00073      |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 4.41e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0607  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 13168    |\n",
      "|    total_timesteps | 9394560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9414720, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0531     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9414720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006597073 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 9.08e-05    |\n",
      "|    loss                 | -0.000767   |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 2.23e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9434880, episode_reward=-0.07 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0661     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9434880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003380722 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 8.78e-05    |\n",
      "|    loss                 | -0.00326    |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.000567   |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 4.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0602  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 13223    |\n",
      "|    total_timesteps | 9434880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9455040, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0577    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9455040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00480051 |\n",
      "|    clip_fraction        | 0.0358     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.81       |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 8.48e-05   |\n",
      "|    loss                 | -0.00186   |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.000178  |\n",
      "|    std                  | 0.0246     |\n",
      "|    value_loss           | 4.88e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9475200, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0522     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9475200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004908591 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 8.17e-05    |\n",
      "|    loss                 | -0.00317    |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 5.81e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0553  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 13279    |\n",
      "|    total_timesteps | 9475200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9495360, episode_reward=-0.06 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0614      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9495360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067884414 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.402        |\n",
      "|    learning_rate        | 7.87e-05     |\n",
      "|    loss                 | -0.00186     |\n",
      "|    n_updates            | 4700         |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 7.24e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9515520, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0565      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9515520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070246686 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.82         |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 7.57e-05     |\n",
      "|    loss                 | -0.00161     |\n",
      "|    n_updates            | 4710         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.0245       |\n",
      "|    value_loss           | 1.87e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0595  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 13335    |\n",
      "|    total_timesteps | 9515520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9535680, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0769      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9535680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026532272 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.82         |\n",
      "|    explained_variance   | 0.585        |\n",
      "|    learning_rate        | 7.27e-05     |\n",
      "|    loss                 | 0.000264     |\n",
      "|    n_updates            | 4720         |\n",
      "|    policy_gradient_loss | -0.000252    |\n",
      "|    std                  | 0.0245       |\n",
      "|    value_loss           | 8.5e-08      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9555840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0543      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9555840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027735399 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.82         |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 6.96e-05     |\n",
      "|    loss                 | 0.000563     |\n",
      "|    n_updates            | 4730         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 1.13e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0616  |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 13390    |\n",
      "|    total_timesteps | 9555840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9576000, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0664      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9576000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022133077 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 6.66e-05     |\n",
      "|    loss                 | -0.000113    |\n",
      "|    n_updates            | 4740         |\n",
      "|    policy_gradient_loss | -0.000254    |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 6.42e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9596160, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0523     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9596160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003868885 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 6.36e-05    |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 4750        |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 5.48e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0654  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 13461    |\n",
      "|    total_timesteps | 9596160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9616320, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0425     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9616320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004118613 |\n",
      "|    clip_fraction        | 0.0289      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 6.06e-05    |\n",
      "|    loss                 | -0.00392    |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 2.71e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9636480, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0577      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9636480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036331427 |\n",
      "|    clip_fraction        | 0.0206       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.412        |\n",
      "|    learning_rate        | 5.76e-05     |\n",
      "|    loss                 | -0.00161     |\n",
      "|    n_updates            | 4770         |\n",
      "|    policy_gradient_loss | -0.000249    |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 4.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0623  |\n",
      "| time/              |          |\n",
      "|    fps             | 712      |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 13531    |\n",
      "|    total_timesteps | 9636480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9656640, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0714     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9656640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003723448 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 5.45e-05    |\n",
      "|    loss                 | -0.00111    |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 1.59e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9676800, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0555      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9676800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031169492 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 5.15e-05     |\n",
      "|    loss                 | -0.00456     |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 9.87e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.061   |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 13608    |\n",
      "|    total_timesteps | 9676800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9696960, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.061       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9696960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036505088 |\n",
      "|    clip_fraction        | 0.0249       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.417        |\n",
      "|    learning_rate        | 4.85e-05     |\n",
      "|    loss                 | -0.00125     |\n",
      "|    n_updates            | 4800         |\n",
      "|    policy_gradient_loss | -0.000707    |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 3.73e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9717120, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0415     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9717120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002190026 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 4.55e-05    |\n",
      "|    loss                 | -0.00317    |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -0.00066    |\n",
      "|    std                  | 0.0246      |\n",
      "|    value_loss           | 3.91e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0589  |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 13685    |\n",
      "|    total_timesteps | 9717120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9737280, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0708      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9737280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025654072 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.81         |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 4.24e-05     |\n",
      "|    loss                 | -0.00121     |\n",
      "|    n_updates            | 4820         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    std                  | 0.0246       |\n",
      "|    value_loss           | 8.29e-08     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Big observation\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'small'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'small')\n",
    "]))\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'small'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigObs',\n",
    "    log_path='./logs/eval_logsBigObs',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Action\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs')\n",
    "]))\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigAct',\n",
    "    log_path='./logs/eval_logsBigAct',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 2 Big\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'small'),\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'small')\n",
    "]))\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'small'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigAct',\n",
    "    log_path='./logs/eval_logsBigAct',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.0015), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward l2\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small', obs = 'xs', reward = 'L2'),\n",
    "    lambda: tradingEng(paths2,action = 'small', obs = 'xs', reward = 'L2')\n",
    "]))\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs', reward = 'L2'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelL2',\n",
    "    log_path='./logs/eval_logsL2',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "# Save the agent\n",
    "model.save(\"0.7basfall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations 0.7\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "with open(\"0.7Corr1Half.pkl\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"0.7Corr2Half.pkl\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "\n",
    "with open(\"0.7CorrTest.pkl\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n",
    "\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small', obs = 'xs')\n",
    "]))\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model07',\n",
    "    log_path='./logs/eval_logs07',\n",
    "    eval_freq=252*8*5*20,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0015), policy_kwargs=policy_kwargs, n_steps=252*8*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=1e7, log_interval=2, callback=eval_callback) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
