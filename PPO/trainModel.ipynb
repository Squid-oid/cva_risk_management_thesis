{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\osc16\\miniconda3\\envs\\d2d\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Paths\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## LR schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.0005)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006457979 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.0325      |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.0012     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -4.53    |\n",
      "| time/              |          |\n",
      "|    fps             | 655      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.17 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.165      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021500068 |\n",
      "|    clip_fraction        | 0.0773      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.0343      |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.00491    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 0.000385    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -4.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.16 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021098383 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | -0.044      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.00883    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    std                  | 0.76        |\n",
      "|    value_loss           | 0.000251    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 617      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.14 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.139      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022769073 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    std                  | 0.681       |\n",
      "|    value_loss           | 9.34e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0871     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024218734 |\n",
      "|    clip_fraction        | 0.0996      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.0098     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00764    |\n",
      "|    std                  | 0.586       |\n",
      "|    value_loss           | 6.9e-05     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.071      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022931242 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.496       |\n",
      "|    value_loss           | 3.23e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 621      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0917    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 141120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01963803 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.819     |\n",
      "|    explained_variance   | 0.265      |\n",
      "|    learning_rate        | 0.00478    |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.419      |\n",
      "|    value_loss           | 6.49e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0983     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016940035 |\n",
      "|    clip_fraction        | 0.0809      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.0421      |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.00887    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    std                  | 0.35        |\n",
      "|    value_loss           | 0.000222    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.13 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.132      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027360838 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.286       |\n",
      "|    value_loss           | 8.57e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026638601 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.331       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.229       |\n",
      "|    value_loss           | 4.26e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 319      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0931     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024262158 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.704       |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.00967    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 6.09e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 351      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0853     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026662648 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.17        |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 1.04e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.967   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 382      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0831    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 262080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02182355 |\n",
      "|    clip_fraction        | 0.0741     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.55       |\n",
      "|    explained_variance   | 0.225      |\n",
      "|    learning_rate        | 0.00458    |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00652   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 4.12e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.806   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 414      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.114    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 282240    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0079531 |\n",
      "|    clip_fraction        | 0.0498    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 1.89      |\n",
      "|    explained_variance   | 0.0586    |\n",
      "|    learning_rate        | 0.00455   |\n",
      "|    loss                 | -0.00405  |\n",
      "|    n_updates            | 270       |\n",
      "|    policy_gradient_loss | -0.0022   |\n",
      "|    std                  | 0.105     |\n",
      "|    value_loss           | 1.21e-05  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 445      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0981     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021945462 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.18        |\n",
      "|    explained_variance   | 0.0284      |\n",
      "|    learning_rate        | 0.00451     |\n",
      "|    loss                 | -0.00336    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    std                  | 0.093       |\n",
      "|    value_loss           | 2.42e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.537   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 477      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0939    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 322560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02406827 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.45       |\n",
      "|    explained_variance   | 0.191      |\n",
      "|    learning_rate        | 0.00448    |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00986   |\n",
      "|    std                  | 0.079      |\n",
      "|    value_loss           | 6.72e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.452   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 509      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0773     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010465594 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.8         |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.00564    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    std                  | 0.0666      |\n",
      "|    value_loss           | 1.02e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.389   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 540      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0934    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 362880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01853054 |\n",
      "|    clip_fraction        | 0.068      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.11       |\n",
      "|    explained_variance   | 0.066      |\n",
      "|    learning_rate        | 0.00441    |\n",
      "|    loss                 | -0.00499   |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0025    |\n",
      "|    std                  | 0.0583     |\n",
      "|    value_loss           | 4.07e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.342   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 572      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0937    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 383040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02954478 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.33       |\n",
      "|    explained_variance   | 0.0234     |\n",
      "|    learning_rate        | 0.00438    |\n",
      "|    loss                 | 0.000517   |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | 0.000656   |\n",
      "|    std                  | 0.0529     |\n",
      "|    value_loss           | 1.04e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.298   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 603      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0965     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011407392 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.61        |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.00434     |\n",
      "|    loss                 | 0.00301     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    std                  | 0.0454      |\n",
      "|    value_loss           | 1.76e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.269   |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 638      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0789     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010334339 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.82        |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.00431     |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    std                  | 0.0409      |\n",
      "|    value_loss           | 2.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.232   |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 674      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0921     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013587864 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.1         |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.00428     |\n",
      "|    loss                 | -0.00738    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    std                  | 0.036       |\n",
      "|    value_loss           | 9.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.212   |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 708      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008188263 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.28        |\n",
      "|    explained_variance   | 0.0792      |\n",
      "|    learning_rate        | 0.00424     |\n",
      "|    loss                 | 0.00269     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.000601    |\n",
      "|    std                  | 0.033       |\n",
      "|    value_loss           | 8.14e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.195   |\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 740      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 483840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053322357 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.48         |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 0.00421      |\n",
      "|    loss                 | -0.00313     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    std                  | 0.0297       |\n",
      "|    value_loss           | 1.79e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.176   |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 772      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006663082 |\n",
      "|    clip_fraction        | 0.085       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.6         |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.00418     |\n",
      "|    loss                 | -0.00168    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.0283      |\n",
      "|    value_loss           | 8.91e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 803      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0668     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004805971 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.75        |\n",
      "|    explained_variance   | -0.116      |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | -0.00194    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -6.19e-05   |\n",
      "|    std                  | 0.0262      |\n",
      "|    value_loss           | 8.24e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 836      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009331651 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.91        |\n",
      "|    explained_variance   | 0.0671      |\n",
      "|    learning_rate        | 0.00411     |\n",
      "|    loss                 | 0.00016     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | 0.000382    |\n",
      "|    std                  | 0.0244      |\n",
      "|    value_loss           | 3.22e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.142   |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 867      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0861     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011709847 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.03        |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | 0.0016      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.000154   |\n",
      "|    std                  | 0.023       |\n",
      "|    value_loss           | 1.8e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.136   |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 899      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0972      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 584640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021964128 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.15         |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.00404      |\n",
      "|    loss                 | -0.00407     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | 0.00141      |\n",
      "|    std                  | 0.0216       |\n",
      "|    value_loss           | 1.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.131   |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 930      |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.086     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 604800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02383684 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.23       |\n",
      "|    explained_variance   | 0.243      |\n",
      "|    learning_rate        | 0.00401    |\n",
      "|    loss                 | 0.00381    |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | 0.00628    |\n",
      "|    std                  | 0.0209     |\n",
      "|    value_loss           | 2.7e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.127   |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 962      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.102       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118542025 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.33         |\n",
      "|    explained_variance   | 0.0242       |\n",
      "|    learning_rate        | 0.00398      |\n",
      "|    loss                 | -0.00603     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000469    |\n",
      "|    std                  | 0.0197       |\n",
      "|    value_loss           | 7.98e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.122   |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 993      |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0958     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015759658 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.42        |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00394     |\n",
      "|    loss                 | 0.00534     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.0189      |\n",
      "|    value_loss           | 1.59e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.119   |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1024     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.116       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034219404 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.52         |\n",
      "|    explained_variance   | 0.288        |\n",
      "|    learning_rate        | 0.00391      |\n",
      "|    loss                 | -0.00246     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | 0.00196      |\n",
      "|    std                  | 0.018        |\n",
      "|    value_loss           | 1.64e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.117   |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1055     |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0763    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 685440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01396939 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.58       |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.00387    |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | 0.00234    |\n",
      "|    std                  | 0.0176     |\n",
      "|    value_loss           | 1.02e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.118   |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1086     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0919      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 705600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082651535 |\n",
      "|    clip_fraction        | 0.235        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.67         |\n",
      "|    explained_variance   | 0.0306       |\n",
      "|    learning_rate        | 0.00384      |\n",
      "|    loss                 | 0.00195      |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | 0.00911      |\n",
      "|    std                  | 0.0169       |\n",
      "|    value_loss           | 1.38e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.115   |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1117     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0962      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048709856 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.72         |\n",
      "|    explained_variance   | 0.0634       |\n",
      "|    learning_rate        | 0.00381      |\n",
      "|    loss                 | 0.00265      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | 0.00346      |\n",
      "|    std                  | 0.0165       |\n",
      "|    value_loss           | 1.76e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.112   |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1148     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.13 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.133      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027023861 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.78        |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.00377     |\n",
      "|    loss                 | 0.0068      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | 0.00246     |\n",
      "|    std                  | 0.016       |\n",
      "|    value_loss           | 3.85e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.112   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1179     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0852     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004699967 |\n",
      "|    clip_fraction        | 0.086       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.83        |\n",
      "|    explained_variance   | -0.0122     |\n",
      "|    learning_rate        | 0.00374     |\n",
      "|    loss                 | -0.00322    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.000216    |\n",
      "|    std                  | 0.0156      |\n",
      "|    value_loss           | 4.06e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1210     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0871      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 786240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022417707 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.91         |\n",
      "|    explained_variance   | 0.336        |\n",
      "|    learning_rate        | 0.00371      |\n",
      "|    loss                 | 0.00239      |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | 0.00177      |\n",
      "|    std                  | 0.015        |\n",
      "|    value_loss           | 4.91e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.108   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1241     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0898     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016608017 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.0941      |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | -0.000152   |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.00159     |\n",
      "|    std                  | 0.0146      |\n",
      "|    value_loss           | 3.95e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.108   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1272     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0803     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008610252 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.085       |\n",
      "|    learning_rate        | 0.00364     |\n",
      "|    loss                 | 0.00546     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | 0.00563     |\n",
      "|    std                  | 0.0141      |\n",
      "|    value_loss           | 1.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1304     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 846720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024204128 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.09         |\n",
      "|    explained_variance   | 0.281        |\n",
      "|    learning_rate        | 0.00361      |\n",
      "|    loss                 | -0.00109     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.0024       |\n",
      "|    std                  | 0.0138       |\n",
      "|    value_loss           | 8.08e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1335     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0834     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005439436 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.15        |\n",
      "|    explained_variance   | 0.0883      |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00243     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    std                  | 0.0133      |\n",
      "|    value_loss           | 1.49e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1366     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0713     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009683125 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.19        |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.00257     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.00155     |\n",
      "|    std                  | 0.0131      |\n",
      "|    value_loss           | 9.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1397     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.102       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 907200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068675335 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.26         |\n",
      "|    explained_variance   | 0.27         |\n",
      "|    learning_rate        | 0.0035       |\n",
      "|    loss                 | -0.000506    |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | 0.00324      |\n",
      "|    std                  | 0.0126       |\n",
      "|    value_loss           | 1.24e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1429     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0747     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007953379 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | -0.0952     |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | 0.00653     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00393     |\n",
      "|    std                  | 0.0123      |\n",
      "|    value_loss           | 6.96e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1460     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0989    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 947520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04986868 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.37       |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.00344    |\n",
      "|    loss                 | 0.0221     |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | 0.00328    |\n",
      "|    std                  | 0.0119     |\n",
      "|    value_loss           | 1.01e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1492     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.098      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011053893 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.41        |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | 0.00133     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00264     |\n",
      "|    std                  | 0.0117      |\n",
      "|    value_loss           | 1.32e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1523     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0822     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012611675 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.46        |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.00337     |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.0115      |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1555     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0715     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005768563 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.00334     |\n",
      "|    loss                 | 0.0014      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00402     |\n",
      "|    std                  | 0.0113      |\n",
      "|    value_loss           | 1.72e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1585     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.108      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035675596 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.51        |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0033      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | 0.00851     |\n",
      "|    std                  | 0.0111      |\n",
      "|    value_loss           | 1.42e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1618     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.13 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.128      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006454395 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.53        |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00327     |\n",
      "|    loss                 | -0.000546   |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.00633     |\n",
      "|    std                  | 0.011       |\n",
      "|    value_loss           | 6.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0983  |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1649     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0977     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1068480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024025945 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.58        |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00324     |\n",
      "|    loss                 | 0.00128     |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    std                  | 0.0107      |\n",
      "|    value_loss           | 1.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.099   |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1679     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0845     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004845773 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.63        |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0032      |\n",
      "|    loss                 | -4.09e-05   |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.00574     |\n",
      "|    std                  | 0.0105      |\n",
      "|    value_loss           | 2.63e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0982  |\n",
      "| time/              |          |\n",
      "|    fps             | 637      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1708     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0808      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1108800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102770785 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.7          |\n",
      "|    explained_variance   | 0.309        |\n",
      "|    learning_rate        | 0.00317      |\n",
      "|    loss                 | -0.00095     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | 0.00311      |\n",
      "|    std                  | 0.00998      |\n",
      "|    value_loss           | 1.29e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0968  |\n",
      "| time/              |          |\n",
      "|    fps             | 637      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1738     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0742      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1128960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058179954 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.74         |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.00314      |\n",
      "|    loss                 | -0.00258     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | 0.00106      |\n",
      "|    std                  | 0.00988      |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0988  |\n",
      "| time/              |          |\n",
      "|    fps             | 638      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1767     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015808012 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.78        |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0031      |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.00711     |\n",
      "|    std                  | 0.0097      |\n",
      "|    value_loss           | 1.97e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0982  |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1797     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0887     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1169280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008647083 |\n",
      "|    clip_fraction        | 0.0974      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.8         |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00307     |\n",
      "|    loss                 | 3.47e-05    |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | 0.00158     |\n",
      "|    std                  | 0.00958     |\n",
      "|    value_loss           | 3.19e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1827     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0823     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1189440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020474365 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.85        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00303     |\n",
      "|    loss                 | 0.0118      |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | 0.000922    |\n",
      "|    std                  | 0.00934     |\n",
      "|    value_loss           | 2.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1857     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0865    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1209600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02381331 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.87       |\n",
      "|    explained_variance   | 0.204      |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | 0.0218     |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | 0.0168     |\n",
      "|    std                  | 0.00921    |\n",
      "|    value_loss           | 8.37e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1887     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009429455 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.88        |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00297     |\n",
      "|    loss                 | -0.000318   |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    std                  | 0.00918     |\n",
      "|    value_loss           | 1.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 1916     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0827      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1249920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027253772 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.93         |\n",
      "|    explained_variance   | -0.0037      |\n",
      "|    learning_rate        | 0.00293      |\n",
      "|    loss                 | 0.000462     |\n",
      "|    n_updates            | 1230         |\n",
      "|    policy_gradient_loss | 0.00378      |\n",
      "|    std                  | 0.00905      |\n",
      "|    value_loss           | 8.96e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0995  |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 1948     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.13 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.127       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1270080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070096534 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.94         |\n",
      "|    explained_variance   | 0.366        |\n",
      "|    learning_rate        | 0.0029       |\n",
      "|    loss                 | 0.00101      |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | 0.00161      |\n",
      "|    std                  | 0.00896      |\n",
      "|    value_loss           | 1.17e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0991  |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 1978     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0829     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004918381 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.02        |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00287     |\n",
      "|    loss                 | -0.00246    |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.000795    |\n",
      "|    std                  | 0.00854     |\n",
      "|    value_loss           | 1.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0977  |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 2010     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.102     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1310400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01282069 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.08       |\n",
      "|    explained_variance   | 0.31       |\n",
      "|    learning_rate        | 0.00283    |\n",
      "|    loss                 | 0.00381    |\n",
      "|    n_updates            | 1290       |\n",
      "|    policy_gradient_loss | 0.00308    |\n",
      "|    std                  | 0.00825    |\n",
      "|    value_loss           | 1.31e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0986  |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 2041     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0802    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1330560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05579276 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.13       |\n",
      "|    explained_variance   | 0.431      |\n",
      "|    learning_rate        | 0.0028     |\n",
      "|    loss                 | 0.00591    |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | 0.0202     |\n",
      "|    std                  | 0.00815    |\n",
      "|    value_loss           | 1.67e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0995  |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2072     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0755     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033413492 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.16        |\n",
      "|    explained_variance   | -0.0955     |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | 0.0616      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.00774     |\n",
      "|    std                  | 0.00803     |\n",
      "|    value_loss           | 6.36e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2103     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031184688 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.19        |\n",
      "|    explained_variance   | -0.126      |\n",
      "|    learning_rate        | 0.00273     |\n",
      "|    loss                 | -0.000488   |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.0079      |\n",
      "|    value_loss           | 7.15e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0977  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2134     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.078      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025085052 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.22        |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0027      |\n",
      "|    loss                 | 0.00543     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | 0.00159     |\n",
      "|    std                  | 0.00783     |\n",
      "|    value_loss           | 4.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0961  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 2165     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0919      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1411200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043961927 |\n",
      "|    clip_fraction        | 0.187        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.27         |\n",
      "|    explained_variance   | 0.266        |\n",
      "|    learning_rate        | 0.00266      |\n",
      "|    loss                 | 0.00255      |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | 0.00434      |\n",
      "|    std                  | 0.00765      |\n",
      "|    value_loss           | 1.18e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0987  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2196     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0784      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1431360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048797904 |\n",
      "|    clip_fraction        | 0.278        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.31         |\n",
      "|    explained_variance   | 0.298        |\n",
      "|    learning_rate        | 0.00263      |\n",
      "|    loss                 | -0.000324    |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | 0.0131       |\n",
      "|    std                  | 0.00753      |\n",
      "|    value_loss           | 4.13e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2228     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.10 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005206594 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.3         |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0026      |\n",
      "|    loss                 | -0.000391   |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | 0.00327     |\n",
      "|    std                  | 0.00751     |\n",
      "|    value_loss           | 2.82e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0974  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2259     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0899     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020126741 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.32        |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00256     |\n",
      "|    loss                 | 0.00254     |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | 0.00254     |\n",
      "|    std                  | 0.00744     |\n",
      "|    value_loss           | 7.99e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0994  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 2290     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005372034 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.35        |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | -0.000762   |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    std                  | 0.00729     |\n",
      "|    value_loss           | 1.9e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.099   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 2322     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0729     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008240513 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.37        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.00304     |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | 0.00587     |\n",
      "|    std                  | 0.00718     |\n",
      "|    value_loss           | 1.13e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2353     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0805     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004820057 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.4         |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | -0.000492   |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.00609     |\n",
      "|    std                  | 0.00706     |\n",
      "|    value_loss           | 1.72e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0972  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2384     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0869     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018628871 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.44        |\n",
      "|    explained_variance   | 0.0397      |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | 0.00952     |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | 0.00898     |\n",
      "|    std                  | 0.00692     |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0935  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 2414     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0945     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021245364 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.46        |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.0024      |\n",
      "|    loss                 | -6.19e-06   |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | 0.0097      |\n",
      "|    std                  | 0.00692     |\n",
      "|    value_loss           | 3.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0936  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2446     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0895     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026207563 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.48        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | 0.000101    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | 0.00264     |\n",
      "|    std                  | 0.00676     |\n",
      "|    value_loss           | 1.19e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0911  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2476     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0842     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009350665 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.53        |\n",
      "|    explained_variance   | 0.0992      |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | 0.00381     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.00658     |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0906  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2509     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0861    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1632960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06932305 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.54       |\n",
      "|    explained_variance   | 0.0985     |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 0.0241     |\n",
      "|    n_updates            | 1610       |\n",
      "|    policy_gradient_loss | 0.00811    |\n",
      "|    std                  | 0.00662    |\n",
      "|    value_loss           | 1.31e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0925  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2540     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0872     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008390917 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.57        |\n",
      "|    explained_variance   | -0.206      |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | -2.81e-05   |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.003       |\n",
      "|    std                  | 0.00647     |\n",
      "|    value_loss           | 2.9e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0908  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 2569     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0824     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004100161 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.59        |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | 0.00346     |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.00272     |\n",
      "|    std                  | 0.00644     |\n",
      "|    value_loss           | 1.46e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0921  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2599     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0856    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1693440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01050639 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | 0.193      |\n",
      "|    learning_rate        | 0.00219    |\n",
      "|    loss                 | 0.000396   |\n",
      "|    n_updates            | 1670       |\n",
      "|    policy_gradient_loss | 0.00166    |\n",
      "|    std                  | 0.00646    |\n",
      "|    value_loss           | 3.94e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0921  |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2629     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008156823 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.58        |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | -0.000866   |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.00338     |\n",
      "|    std                  | 0.0064      |\n",
      "|    value_loss           | 2.17e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0933  |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 2658     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0868     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087547 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.62        |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | 0.00111     |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.0162      |\n",
      "|    std                  | 0.00633     |\n",
      "|    value_loss           | 1.96e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0957  |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 2688     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.112       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1753920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039455015 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.63         |\n",
      "|    explained_variance   | -0.291       |\n",
      "|    learning_rate        | 0.00209      |\n",
      "|    loss                 | -0.00119     |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | 0.000522     |\n",
      "|    std                  | 0.00627      |\n",
      "|    value_loss           | 6.4e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2717     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0787    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1774080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00784459 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.65       |\n",
      "|    explained_variance   | -0.0103    |\n",
      "|    learning_rate        | 0.00206    |\n",
      "|    loss                 | 0.008      |\n",
      "|    n_updates            | 1750       |\n",
      "|    policy_gradient_loss | 0.00663    |\n",
      "|    std                  | 0.00625    |\n",
      "|    value_loss           | 3.94e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0938  |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2747     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0746    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1794240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03228402 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.65       |\n",
      "|    explained_variance   | 0.307      |\n",
      "|    learning_rate        | 0.00203    |\n",
      "|    loss                 | 0.00974    |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | 0.00781    |\n",
      "|    std                  | 0.00622    |\n",
      "|    value_loss           | 2.32e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 2776     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003760062 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.66        |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | -0.000375   |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.0033      |\n",
      "|    std                  | 0.00618     |\n",
      "|    value_loss           | 1.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0914  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2806     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005008344 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.68        |\n",
      "|    explained_variance   | -0.156      |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | -0.000146   |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    std                  | 0.00614     |\n",
      "|    value_loss           | 8.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0894  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 2836     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019839443 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.68        |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | 0.00891     |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | 0.00523     |\n",
      "|    std                  | 0.00617     |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 2865     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036473423 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.7         |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | 0.00263     |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | 0.00628     |\n",
      "|    std                  | 0.00609     |\n",
      "|    value_loss           | 2.81e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0956  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2895     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0811     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008310477 |\n",
      "|    clip_fraction        | 0.0761      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.73        |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00186     |\n",
      "|    loss                 | -0.000749   |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.000216   |\n",
      "|    std                  | 0.00602     |\n",
      "|    value_loss           | 1.47e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0951  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 2925     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0793      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1915200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022805603 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.78         |\n",
      "|    explained_variance   | 0.379        |\n",
      "|    learning_rate        | 0.00182      |\n",
      "|    loss                 | -0.00128     |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | 0.00172      |\n",
      "|    std                  | 0.00586      |\n",
      "|    value_loss           | 4.9e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0946  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 2955     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0941     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005588925 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.84        |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | 0.00611     |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | 0.000666    |\n",
      "|    std                  | 0.00574     |\n",
      "|    value_loss           | 7.38e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0973  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 2984     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.11 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011029001 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.85        |\n",
      "|    explained_variance   | -0.126      |\n",
      "|    learning_rate        | 0.00176     |\n",
      "|    loss                 | 0.00048     |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | 0.00728     |\n",
      "|    std                  | 0.00571     |\n",
      "|    value_loss           | 8.49e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0984  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 3014     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0898    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1975680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02892883 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.88       |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.00172    |\n",
      "|    loss                 | 0.00144    |\n",
      "|    n_updates            | 1950       |\n",
      "|    policy_gradient_loss | 0.0067     |\n",
      "|    std                  | 0.00556    |\n",
      "|    value_loss           | 9.04e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0964  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 3043     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0824     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007201872 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.9         |\n",
      "|    explained_variance   | -0.382      |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | 0.00498     |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | 0.00802     |\n",
      "|    std                  | 0.00549     |\n",
      "|    value_loss           | 1.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0928  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 3073     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0866    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2016000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00816948 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.92       |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.00166    |\n",
      "|    loss                 | -0.000736  |\n",
      "|    n_updates            | 1990       |\n",
      "|    policy_gradient_loss | 0.00188    |\n",
      "|    std                  | 0.00548    |\n",
      "|    value_loss           | 1.6e-06    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0945  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3103     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.09        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2036160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051793684 |\n",
      "|    clip_fraction        | 0.0693       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.92         |\n",
      "|    explained_variance   | 0.375        |\n",
      "|    learning_rate        | 0.00162      |\n",
      "|    loss                 | 0.0022       |\n",
      "|    n_updates            | 2010         |\n",
      "|    policy_gradient_loss | 0.00149      |\n",
      "|    std                  | 0.00544      |\n",
      "|    value_loss           | 1.15e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0954  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3133     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0876     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008959085 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.92        |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | 0.012       |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.0029      |\n",
      "|    std                  | 0.00552     |\n",
      "|    value_loss           | 1.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.093   |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3162     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0943     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007822143 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.92        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00156     |\n",
      "|    loss                 | 0.00648     |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.00323     |\n",
      "|    std                  | 0.0055      |\n",
      "|    value_loss           | 8.21e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0933  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3192     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033442527 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.9         |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | -0.00231    |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.00413     |\n",
      "|    std                  | 0.00554     |\n",
      "|    value_loss           | 3.27e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3221     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0746      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2116800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103231715 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.92         |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.00149      |\n",
      "|    loss                 | 0.00689      |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | 0.00569      |\n",
      "|    std                  | 0.0055       |\n",
      "|    value_loss           | 8.94e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0949  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3250     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0796      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2136960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033671167 |\n",
      "|    clip_fraction        | 0.0773       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.95         |\n",
      "|    explained_variance   | 0.148        |\n",
      "|    learning_rate        | 0.00146      |\n",
      "|    loss                 | 0.000913     |\n",
      "|    n_updates            | 2110         |\n",
      "|    policy_gradient_loss | 0.000914     |\n",
      "|    std                  | 0.00534      |\n",
      "|    value_loss           | 8.58e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0947  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3280     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072557246 |\n",
      "|    clip_fraction        | 0.099        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.98         |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | 0.012        |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | 0.00179      |\n",
      "|    std                  | 0.00528      |\n",
      "|    value_loss           | 1.01e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0953  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3310     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0965     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005575937 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8           |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.000341   |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | 0.00336     |\n",
      "|    std                  | 0.0052      |\n",
      "|    value_loss           | 1.77e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0946  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 3339     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.108      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2197440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004888056 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.01        |\n",
      "|    explained_variance   | -0.047      |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | 0.00122     |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    std                  | 0.00521     |\n",
      "|    value_loss           | 1.14e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0964  |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 3368     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0938     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011234217 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.05        |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.00188     |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    std                  | 0.00503     |\n",
      "|    value_loss           | 3.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.099   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 3398     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0882      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2237760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014766674 |\n",
      "|    clip_fraction        | 0.0676       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.09         |\n",
      "|    explained_variance   | -0.564       |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | 0.000141     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | 0.000627     |\n",
      "|    std                  | 0.00494      |\n",
      "|    value_loss           | 1.19e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0977  |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 3427     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0886    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2257920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09630027 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.12       |\n",
      "|    explained_variance   | -0.141     |\n",
      "|    learning_rate        | 0.00125    |\n",
      "|    loss                 | 0.0282     |\n",
      "|    n_updates            | 2230       |\n",
      "|    policy_gradient_loss | 0.0155     |\n",
      "|    std                  | 0.00488    |\n",
      "|    value_loss           | 1.33e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0977  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 3456     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0896     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008536834 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.12        |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.00504     |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.00868     |\n",
      "|    std                  | 0.00487     |\n",
      "|    value_loss           | 9.74e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0961  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 3487     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2298240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004410847 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.15        |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00088     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.000909    |\n",
      "|    std                  | 0.00485     |\n",
      "|    value_loss           | 1.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 3516     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0845     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008677194 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.16        |\n",
      "|    explained_variance   | -0.00185    |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.000524   |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.000568   |\n",
      "|    std                  | 0.0048      |\n",
      "|    value_loss           | 6.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3546     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0856      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2338560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034014715 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.19         |\n",
      "|    explained_variance   | 0.309        |\n",
      "|    learning_rate        | 0.00112      |\n",
      "|    loss                 | 0.00364      |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | 0.000588     |\n",
      "|    std                  | 0.00477      |\n",
      "|    value_loss           | 1.47e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0896  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 3575     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010402605 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.22        |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.00155     |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.00302     |\n",
      "|    std                  | 0.0047      |\n",
      "|    value_loss           | 1.5e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0884  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 3605     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2378880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033969958 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.24         |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.00105      |\n",
      "|    loss                 | 0.000896     |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | 0.000865     |\n",
      "|    std                  | 0.00469      |\n",
      "|    value_loss           | 1.19e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0886  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 3634     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2399040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046594334 |\n",
      "|    clip_fraction        | 0.273        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.28         |\n",
      "|    explained_variance   | -0.0625      |\n",
      "|    learning_rate        | 0.00102      |\n",
      "|    loss                 | 0.000976     |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | 0.0145       |\n",
      "|    std                  | 0.00461      |\n",
      "|    value_loss           | 1.79e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0844  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 3664     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004894998 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.29        |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | -0.000842   |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.00274     |\n",
      "|    std                  | 0.0046      |\n",
      "|    value_loss           | 8.22e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0878  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3697     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0985     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011203703 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.29        |\n",
      "|    explained_variance   | -0.248      |\n",
      "|    learning_rate        | 0.000951    |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.00534     |\n",
      "|    std                  | 0.00461     |\n",
      "|    value_loss           | 1.39e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 3727     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.078     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2459520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00406294 |\n",
      "|    clip_fraction        | 0.0621     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.3        |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.000918   |\n",
      "|    loss                 | 0.000647   |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | 0.000391   |\n",
      "|    std                  | 0.00458    |\n",
      "|    value_loss           | 2.82e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0901  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 3758     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0909     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005749099 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.32        |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.000884    |\n",
      "|    loss                 | -6.92e-05   |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.000586   |\n",
      "|    std                  | 0.0045      |\n",
      "|    value_loss           | 7.87e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0895  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3790     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0763      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2499840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012774515 |\n",
      "|    clip_fraction        | 0.0962       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.34         |\n",
      "|    explained_variance   | 0.557        |\n",
      "|    learning_rate        | 0.00085      |\n",
      "|    loss                 | -0.00106     |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | 0.00231      |\n",
      "|    std                  | 0.00454      |\n",
      "|    value_loss           | 1.25e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0902  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 3821     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.115       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027059359 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.34         |\n",
      "|    explained_variance   | 0.177        |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | 4.38e-06     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | 0.00136      |\n",
      "|    std                  | 0.00453      |\n",
      "|    value_loss           | 4.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0946  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3852     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0732      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2540160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028340062 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.37         |\n",
      "|    explained_variance   | 0.404        |\n",
      "|    learning_rate        | 0.000783     |\n",
      "|    loss                 | 0.00219      |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | 0.000274     |\n",
      "|    std                  | 0.00443      |\n",
      "|    value_loss           | 1.42e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0923  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 3883     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0849     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010429762 |\n",
      "|    clip_fraction        | 0.0827      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00075     |\n",
      "|    loss                 | 0.00343     |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.00188     |\n",
      "|    std                  | 0.0044      |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0918  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 3914     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013395251 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.00434    |\n",
      "|    learning_rate        | 0.000716    |\n",
      "|    loss                 | -0.000206   |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.00597     |\n",
      "|    std                  | 0.00433     |\n",
      "|    value_loss           | 7.35e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 3945     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0897      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2600640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027065098 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.43         |\n",
      "|    explained_variance   | 0.0502       |\n",
      "|    learning_rate        | 0.000682     |\n",
      "|    loss                 | 0.00027      |\n",
      "|    n_updates            | 2570         |\n",
      "|    policy_gradient_loss | 0.00047      |\n",
      "|    std                  | 0.00427      |\n",
      "|    value_loss           | 1.69e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0905  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 3976     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.13 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.135       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2620800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044247885 |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.46         |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.000649     |\n",
      "|    loss                 | -0.00163     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | 0.00146      |\n",
      "|    std                  | 0.00421      |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0922  |\n",
      "| time/              |          |\n",
      "|    fps             | 654      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 4006     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0869      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2640960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044720634 |\n",
      "|    clip_fraction        | 0.0582       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.47         |\n",
      "|    explained_variance   | 0.204        |\n",
      "|    learning_rate        | 0.000615     |\n",
      "|    loss                 | -0.00114     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | 0.00124      |\n",
      "|    std                  | 0.0042       |\n",
      "|    value_loss           | 6.02e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 4038     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0799      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013143873 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.49         |\n",
      "|    explained_variance   | 0.0544       |\n",
      "|    learning_rate        | 0.000582     |\n",
      "|    loss                 | -0.00295     |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | 0.00364      |\n",
      "|    std                  | 0.00415      |\n",
      "|    value_loss           | 5.51e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0886  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 4069     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0956      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2681280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052470225 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.5          |\n",
      "|    explained_variance   | -0.00445     |\n",
      "|    learning_rate        | 0.000548     |\n",
      "|    loss                 | 0.00752      |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | 3.74e-05     |\n",
      "|    std                  | 0.00413      |\n",
      "|    value_loss           | 7.51e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0877  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 4100     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0838     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013089458 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.51        |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.000514    |\n",
      "|    loss                 | 0.000259    |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | 0.00217     |\n",
      "|    std                  | 0.00411     |\n",
      "|    value_loss           | 9.34e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0897  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 4132     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004213055 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.53        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | 0.000418    |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | 0.00025     |\n",
      "|    std                  | 0.00409     |\n",
      "|    value_loss           | 2.57e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0912  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 4163     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0799     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007498899 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.53        |\n",
      "|    explained_variance   | 0.043       |\n",
      "|    learning_rate        | 0.000447    |\n",
      "|    loss                 | 0.00109     |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | 0.000324    |\n",
      "|    std                  | 0.00408     |\n",
      "|    value_loss           | 5.13e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0879  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 4194     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0929     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2761920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004932721 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.54        |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.000414    |\n",
      "|    loss                 | -0.00428    |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | 0.00033     |\n",
      "|    std                  | 0.00407     |\n",
      "|    value_loss           | 1.47e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.089   |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 4225     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.105       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2782080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022944272 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.55         |\n",
      "|    explained_variance   | 0.422        |\n",
      "|    learning_rate        | 0.00038      |\n",
      "|    loss                 | -0.00163     |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | -9.97e-06    |\n",
      "|    std                  | 0.00408      |\n",
      "|    value_loss           | 6.74e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0888  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 4256     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.106       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028081345 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.54         |\n",
      "|    explained_variance   | 0.257        |\n",
      "|    learning_rate        | 0.000346     |\n",
      "|    loss                 | 3.55e-05     |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | 0.000217     |\n",
      "|    std                  | 0.00409      |\n",
      "|    value_loss           | 2.45e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0891  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 4287     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0975      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2822400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013519395 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.55         |\n",
      "|    explained_variance   | 0.152        |\n",
      "|    learning_rate        | 0.000313     |\n",
      "|    loss                 | 0.000247     |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | 0.000266     |\n",
      "|    std                  | 0.00407      |\n",
      "|    value_loss           | 1.38e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0901  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4318     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0819    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2842560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00893872 |\n",
      "|    clip_fraction        | 0.0826     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.55       |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.000279   |\n",
      "|    loss                 | 0.00213    |\n",
      "|    n_updates            | 2810       |\n",
      "|    policy_gradient_loss | 0.00122    |\n",
      "|    std                  | 0.00407    |\n",
      "|    value_loss           | 2.65e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 4349     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.11        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2862720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036520127 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.56         |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.00108     |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | 0.000199     |\n",
      "|    std                  | 0.00408      |\n",
      "|    value_loss           | 4.88e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0928  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 4381     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0971      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015370437 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.56         |\n",
      "|    explained_variance   | 0.268        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -0.000671    |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | 0.000195     |\n",
      "|    std                  | 0.00409      |\n",
      "|    value_loss           | 2.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0932  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 4411     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.104       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2903040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033270032 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.56         |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.000178     |\n",
      "|    loss                 | -0.00135     |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    std                  | 0.00408      |\n",
      "|    value_loss           | 9.1e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0975  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 4442     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0976      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2923200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036352393 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.57         |\n",
      "|    explained_variance   | 0.386        |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.00243     |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.000398    |\n",
      "|    std                  | 0.00406      |\n",
      "|    value_loss           | 8.71e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0979  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 4473     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.083       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025051693 |\n",
      "|    clip_fraction        | 0.00102      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.58         |\n",
      "|    explained_variance   | 0.00992      |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | 0.00125      |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -1.56e-05    |\n",
      "|    std                  | 0.00404      |\n",
      "|    value_loss           | 9.6e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0972  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 4504     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0799      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027674707 |\n",
      "|    clip_fraction        | 0.0047       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.58         |\n",
      "|    explained_variance   | 0.087        |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | -0.000908    |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | -0.000255    |\n",
      "|    std                  | 0.00404      |\n",
      "|    value_loss           | 2.1e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0972  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 4535     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010993625 |\n",
      "|    clip_fraction        | 0.00291      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.58         |\n",
      "|    explained_variance   | 0.421        |\n",
      "|    learning_rate        | 4.4e-05      |\n",
      "|    loss                 | 1.48e-05     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000106    |\n",
      "|    std                  | 0.00403      |\n",
      "|    value_loss           | 1.17e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0955  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 4566     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.107       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3003840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020821828 |\n",
      "|    clip_fraction        | 0.000278     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.58         |\n",
      "|    explained_variance   | 0.359        |\n",
      "|    learning_rate        | 1.04e-05     |\n",
      "|    loss                 | -0.000447    |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    std                  | 0.00403      |\n",
      "|    value_loss           | 2.25e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0945  |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 4597     |\n",
      "|    total_timesteps | 3003840  |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Eval num_timesteps=4032, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0803    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4032       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00943256 |\n",
      "|    clip_fraction        | 0.0749     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.9       |\n",
      "|    explained_variance   | 0.00671    |\n",
      "|    learning_rate        | 0.005      |\n",
      "|    loss                 | -0.00191   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00297   |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 0.00613    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.846   |\n",
      "| time/              |          |\n",
      "|    fps             | 616      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4032     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8064, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0602     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8064        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024330972 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.00499     |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.901       |\n",
      "|    value_loss           | 4.09e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.732   |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 8064     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12096, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12096        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069940146 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.000167     |\n",
      "|    learning_rate        | 0.00498      |\n",
      "|    loss                 | 0.00415      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    std                  | 0.801        |\n",
      "|    value_loss           | 0.00175      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 585      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 12096    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16128, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021293052 |\n",
      "|    clip_fraction        | 0.068       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.09       |\n",
      "|    explained_variance   | 0.00589     |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0075     |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 8.58e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 16128    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20160, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0968     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009048783 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.00282     |\n",
      "|    learning_rate        | 0.00497     |\n",
      "|    loss                 | 0.00086     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    std                  | 0.596       |\n",
      "|    value_loss           | 0.00255     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24192, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0846    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24192      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01955185 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.0856     |\n",
      "|    learning_rate        | 0.00496    |\n",
      "|    loss                 | -0.0157    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.519      |\n",
      "|    value_loss           | 1.55e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.588   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 24192    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28224, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0464     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004763864 |\n",
      "|    clip_fraction        | 0.0408      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.00247     |\n",
      "|    learning_rate        | 0.00496     |\n",
      "|    loss                 | -0.00109    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    std                  | 0.482       |\n",
      "|    value_loss           | 0.000487    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.519   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 28224    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32256, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0401     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023425763 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 0.4         |\n",
      "|    value_loss           | 5.88e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 32256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36288, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0313     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027407393 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.0793      |\n",
      "|    learning_rate        | 0.00494     |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.333       |\n",
      "|    value_loss           | 4.8e-06     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.389   |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 36288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0547     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009628968 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.0198      |\n",
      "|    learning_rate        | 0.00494     |\n",
      "|    loss                 | -0.00193    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    std                  | 0.298       |\n",
      "|    value_loss           | 0.000256    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.349   |\n",
      "| time/              |          |\n",
      "|    fps             | 549      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44352, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0249     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44352       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028058365 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.112       |\n",
      "|    explained_variance   | 0.0634      |\n",
      "|    learning_rate        | 0.00493     |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.24        |\n",
      "|    value_loss           | 3.18e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.295   |\n",
      "| time/              |          |\n",
      "|    fps             | 549      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 44352    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48384, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0241     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020773366 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.515       |\n",
      "|    explained_variance   | 0.0238      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    std                  | 0.196       |\n",
      "|    value_loss           | 2.27e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 48384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52416, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030913899 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.906       |\n",
      "|    explained_variance   | 0.0972      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 1.49e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.206   |\n",
      "| time/              |          |\n",
      "|    fps             | 549      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 52416    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56448, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56448       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029128864 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.34        |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00491     |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 8.65e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.174   |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 56448    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029118683 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.76        |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0049      |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.104       |\n",
      "|    value_loss           | 6.17e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.147   |\n",
      "| time/              |          |\n",
      "|    fps             | 552      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64512, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009752283 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.09        |\n",
      "|    explained_variance   | 0.0338      |\n",
      "|    learning_rate        | 0.0049      |\n",
      "|    loss                 | 0.007       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    std                  | 0.0908      |\n",
      "|    value_loss           | 2.68e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.121   |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 64512    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68544, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68544       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022151653 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.44        |\n",
      "|    explained_variance   | -0.108      |\n",
      "|    learning_rate        | 0.00489     |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    std                  | 0.074       |\n",
      "|    value_loss           | 3.59e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0997  |\n",
      "| time/              |          |\n",
      "|    fps             | 552      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 68544    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72576, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0171     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017437967 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.81        |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    std                  | 0.0615      |\n",
      "|    value_loss           | 1.24e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.083   |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 72576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76608, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011458932 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.13        |\n",
      "|    explained_variance   | -0.159      |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.00355    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    std                  | 0.0524      |\n",
      "|    value_loss           | 2.61e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0696  |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 76608    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018228645 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.4         |\n",
      "|    explained_variance   | -0.333      |\n",
      "|    learning_rate        | 0.00487     |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.0456      |\n",
      "|    value_loss           | 1.5e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0594  |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84672, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0179     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010890467 |\n",
      "|    clip_fraction        | 0.0621      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.71        |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00486     |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00932    |\n",
      "|    std                  | 0.0392      |\n",
      "|    value_loss           | 1.65e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0507  |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 84672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88704, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0137      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88704        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027103724 |\n",
      "|    clip_fraction        | 0.131        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.91         |\n",
      "|    explained_variance   | 0.0352       |\n",
      "|    learning_rate        | 0.00486      |\n",
      "|    loss                 | -0.00329     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00259     |\n",
      "|    std                  | 0.0364       |\n",
      "|    value_loss           | 1.79e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0421  |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 88704    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92736, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92736       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008893678 |\n",
      "|    clip_fraction        | 0.0949      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.07        |\n",
      "|    explained_variance   | -0.312      |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    std                  | 0.0331      |\n",
      "|    value_loss           | 9.82e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0369  |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 92736    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96768, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0122      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074734595 |\n",
      "|    clip_fraction        | 0.084        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.22         |\n",
      "|    explained_variance   | -0.26        |\n",
      "|    learning_rate        | 0.00484      |\n",
      "|    loss                 | -0.00463     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000229    |\n",
      "|    std                  | 0.0307       |\n",
      "|    value_loss           | 6.64e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0332  |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 96768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065132836 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.39         |\n",
      "|    explained_variance   | 0.274        |\n",
      "|    learning_rate        | 0.00484      |\n",
      "|    loss                 | 0.00363      |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.0282       |\n",
      "|    value_loss           | 2.71e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0307  |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104832, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 104832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009095548 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.00167     |\n",
      "|    learning_rate        | 0.00483     |\n",
      "|    loss                 | -0.00624    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    std                  | 0.0264      |\n",
      "|    value_loss           | 1.51e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.029   |\n",
      "| time/              |          |\n",
      "|    fps             | 556      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 104832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108864, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0147    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 108864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02209499 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.65       |\n",
      "|    explained_variance   | 0.0167     |\n",
      "|    learning_rate        | 0.00482    |\n",
      "|    loss                 | 0.0407     |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | 0.0146     |\n",
      "|    std                  | 0.0251     |\n",
      "|    value_loss           | 4.7e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0271  |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 108864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112896, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015453685 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.81        |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.0026     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -9.42e-05   |\n",
      "|    std                  | 0.0231      |\n",
      "|    value_loss           | 1.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0251  |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 112896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116928, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0189      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 116928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061554234 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.94         |\n",
      "|    explained_variance   | 0.0725       |\n",
      "|    learning_rate        | 0.00481      |\n",
      "|    loss                 | -0.00537     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    std                  | 0.0216       |\n",
      "|    value_loss           | 6.9e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0242  |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 116928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00995    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016741417 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.04        |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0048      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00445     |\n",
      "|    std                  | 0.0206      |\n",
      "|    value_loss           | 1.06e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0231  |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124992, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 124992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011496339 |\n",
      "|    clip_fraction        | 0.0996      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.1         |\n",
      "|    explained_variance   | -0.0567     |\n",
      "|    learning_rate        | 0.0048      |\n",
      "|    loss                 | -0.000428   |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | 0.00131     |\n",
      "|    std                  | 0.02        |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.022   |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 124992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129024, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00984    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006491664 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.22        |\n",
      "|    explained_variance   | -0.00363    |\n",
      "|    learning_rate        | 0.00479     |\n",
      "|    loss                 | -0.00506    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    std                  | 0.0186      |\n",
      "|    value_loss           | 4.17e-08    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0204  |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133056, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.013       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 133056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056614233 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.36         |\n",
      "|    explained_variance   | -0.0465      |\n",
      "|    learning_rate        | 0.00478      |\n",
      "|    loss                 | -0.0103      |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    std                  | 0.0176       |\n",
      "|    value_loss           | 3.02e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0197  |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 133056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137088, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0142    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 137088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01457165 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.51       |\n",
      "|    explained_variance   | -0.0382    |\n",
      "|    learning_rate        | 0.00477    |\n",
      "|    loss                 | -0.00114   |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | 0.00545    |\n",
      "|    std                  | 0.0162     |\n",
      "|    value_loss           | 3.89e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0194  |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 137088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012617953 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.58        |\n",
      "|    explained_variance   | -0.0645     |\n",
      "|    learning_rate        | 0.00477     |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00696     |\n",
      "|    std                  | 0.0159      |\n",
      "|    value_loss           | 6.38e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0189  |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145152, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.012       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 145152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068944218 |\n",
      "|    clip_fraction        | 0.244        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.66         |\n",
      "|    explained_variance   | -0.0666      |\n",
      "|    learning_rate        | 0.00476      |\n",
      "|    loss                 | -0.000218    |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | 0.0129       |\n",
      "|    std                  | 0.0154       |\n",
      "|    value_loss           | 2.11e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0178  |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 145152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149184, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 149184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016362706 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.78        |\n",
      "|    explained_variance   | -0.0995     |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | 0.0318      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -3.9e-05    |\n",
      "|    std                  | 0.0144      |\n",
      "|    value_loss           | 6.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0173  |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 149184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153216, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00888    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 153216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010506982 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.85        |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.0049     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.00621     |\n",
      "|    std                  | 0.014       |\n",
      "|    value_loss           | 1.22e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0172  |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 153216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157248, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0132    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 157248     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03199304 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.9        |\n",
      "|    explained_variance   | -0.00188   |\n",
      "|    learning_rate        | 0.00474    |\n",
      "|    loss                 | 0.0408     |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | 0.0304     |\n",
      "|    std                  | 0.0136     |\n",
      "|    value_loss           | 3.25e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.017   |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 157248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0157      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 161280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048396382 |\n",
      "|    clip_fraction        | 0.163        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.96         |\n",
      "|    explained_variance   | -0.0748      |\n",
      "|    learning_rate        | 0.00473      |\n",
      "|    loss                 | -0.000128    |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00046     |\n",
      "|    std                  | 0.0131       |\n",
      "|    value_loss           | 3.35e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165312, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0134    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 165312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00630296 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | -0.112     |\n",
      "|    learning_rate        | 0.00473    |\n",
      "|    loss                 | 0.00604    |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | 0.00123    |\n",
      "|    std                  | 0.0128     |\n",
      "|    value_loss           | 5.29e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 165312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169344, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0155      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 169344       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026219245 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.08         |\n",
      "|    explained_variance   | -0.112       |\n",
      "|    learning_rate        | 0.00472      |\n",
      "|    loss                 | -0.00306     |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.000926     |\n",
      "|    std                  | 0.0124       |\n",
      "|    value_loss           | 4.74e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0168  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 169344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173376, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0191     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 173376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011639618 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.1         |\n",
      "|    explained_variance   | -0.0902     |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.000782   |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    std                  | 0.0123      |\n",
      "|    value_loss           | 1.81e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 309      |\n",
      "|    total_timesteps | 173376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177408, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 177408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113172205 |\n",
      "|    clip_fraction        | 0.263        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.11         |\n",
      "|    explained_variance   | -0.498       |\n",
      "|    learning_rate        | 0.00471      |\n",
      "|    loss                 | 0.0018       |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | 0.0168       |\n",
      "|    std                  | 0.0123       |\n",
      "|    value_loss           | 5.99e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 316      |\n",
      "|    total_timesteps | 177408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00876     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099029755 |\n",
      "|    clip_fraction        | 0.232        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.18         |\n",
      "|    explained_variance   | -0.0467      |\n",
      "|    learning_rate        | 0.0047       |\n",
      "|    loss                 | -0.0137      |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | 0.00985      |\n",
      "|    std                  | 0.0117       |\n",
      "|    value_loss           | 2.57e-08     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0163  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185472, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031752568 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.29        |\n",
      "|    explained_variance   | -0.0772     |\n",
      "|    learning_rate        | 0.00469     |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    std                  | 0.0113      |\n",
      "|    value_loss           | 6.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0162  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 185472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189504, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0106      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 189504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061537926 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.33         |\n",
      "|    explained_variance   | 0.0217       |\n",
      "|    learning_rate        | 0.00469      |\n",
      "|    loss                 | -0.00298     |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | 0.00191      |\n",
      "|    std                  | 0.011        |\n",
      "|    value_loss           | 4.92e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0161  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 337      |\n",
      "|    total_timesteps | 189504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193536, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 193536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013257044 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | -0.31       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00163     |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 1.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 193536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197568, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 197568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017796097 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.45        |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00467     |\n",
      "|    loss                 | 0.00618     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.0063      |\n",
      "|    std                  | 0.0105      |\n",
      "|    value_loss           | 1.91e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 197568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006837791 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.47        |\n",
      "|    explained_variance   | -1.02       |\n",
      "|    learning_rate        | 0.00467     |\n",
      "|    loss                 | -0.00207    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00516     |\n",
      "|    std                  | 0.0104      |\n",
      "|    value_loss           | 1.82e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 359      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205632, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 205632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021476664 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | -0.367      |\n",
      "|    learning_rate        | 0.00466     |\n",
      "|    loss                 | 0.023       |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | 0.00518     |\n",
      "|    std                  | 0.0102      |\n",
      "|    value_loss           | 2.39e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 205632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209664, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0188     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 209664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017518627 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.54        |\n",
      "|    explained_variance   | -1.89       |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | 0.0401      |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.00453     |\n",
      "|    std                  | 0.00996     |\n",
      "|    value_loss           | 8.32e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 209664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213696, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 213696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030004042 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.57         |\n",
      "|    explained_variance   | -0.724       |\n",
      "|    learning_rate        | 0.00465      |\n",
      "|    loss                 | 0.00327      |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | 0.0088       |\n",
      "|    std                  | 0.00989      |\n",
      "|    value_loss           | 1.09e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 213696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217728, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 217728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011946311 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.62        |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00464     |\n",
      "|    loss                 | 0.0048      |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.00839     |\n",
      "|    std                  | 0.00963     |\n",
      "|    value_loss           | 2.42e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 217728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0135     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028934715 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.68        |\n",
      "|    explained_variance   | -0.102      |\n",
      "|    learning_rate        | 0.00463     |\n",
      "|    loss                 | 0.0035      |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.00401     |\n",
      "|    std                  | 0.00929     |\n",
      "|    value_loss           | 3.13e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 395      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225792, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00991    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 225792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020988278 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.71        |\n",
      "|    explained_variance   | -0.787      |\n",
      "|    learning_rate        | 0.00463     |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | 0.00718     |\n",
      "|    std                  | 0.00919     |\n",
      "|    value_loss           | 1.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 402      |\n",
      "|    total_timesteps | 225792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229824, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 229824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015607152 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.74        |\n",
      "|    explained_variance   | -1.66       |\n",
      "|    learning_rate        | 0.00462     |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.0164      |\n",
      "|    std                  | 0.00905     |\n",
      "|    value_loss           | 1.42e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0159  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 409      |\n",
      "|    total_timesteps | 229824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=233856, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.018       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 233856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029562693 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.81         |\n",
      "|    explained_variance   | -2.4         |\n",
      "|    learning_rate        | 0.00461      |\n",
      "|    loss                 | -0.00531     |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | 0.0103       |\n",
      "|    std                  | 0.00874      |\n",
      "|    value_loss           | 7.37e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0155  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 416      |\n",
      "|    total_timesteps | 233856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237888, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 237888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015076711 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.86        |\n",
      "|    explained_variance   | -1.94       |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | 0.00174     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | 0.0394      |\n",
      "|    std                  | 0.00857     |\n",
      "|    value_loss           | 8.26e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 423      |\n",
      "|    total_timesteps | 237888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025254335 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.9         |\n",
      "|    explained_variance   | -2.21       |\n",
      "|    learning_rate        | 0.0046      |\n",
      "|    loss                 | 0.00146     |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.0153      |\n",
      "|    std                  | 0.0085      |\n",
      "|    value_loss           | 8.12e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 430      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245952, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0182     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 245952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008659871 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.9         |\n",
      "|    explained_variance   | -1.06       |\n",
      "|    learning_rate        | 0.00459     |\n",
      "|    loss                 | -0.00171    |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.00848     |\n",
      "|    value_loss           | 9.94e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 245952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249984, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0197    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 249984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03268984 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.92       |\n",
      "|    explained_variance   | -1.79      |\n",
      "|    learning_rate        | 0.00459    |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 1230       |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    std                  | 0.00839    |\n",
      "|    value_loss           | 8.56e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 444      |\n",
      "|    total_timesteps | 249984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254016, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0097     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 254016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039152842 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.95        |\n",
      "|    explained_variance   | -1.68       |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | 0.0594      |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.023       |\n",
      "|    std                  | 0.00826     |\n",
      "|    value_loss           | 9.21e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 254016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258048, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061502814 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.99        |\n",
      "|    explained_variance   | -0.613      |\n",
      "|    learning_rate        | 0.00457     |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.00809     |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 458      |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051054284 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.01         |\n",
      "|    explained_variance   | -2.21        |\n",
      "|    learning_rate        | 0.00457      |\n",
      "|    loss                 | 0.000123     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | 0.00495      |\n",
      "|    std                  | 0.00798      |\n",
      "|    value_loss           | 9.27e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 465      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266112, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 266112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012811162 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | -1.81       |\n",
      "|    learning_rate        | 0.00456     |\n",
      "|    loss                 | 0.00629     |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | 0.0094      |\n",
      "|    std                  | 0.00785     |\n",
      "|    value_loss           | 5.37e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 472      |\n",
      "|    total_timesteps | 266112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270144, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 270144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106455535 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.09         |\n",
      "|    explained_variance   | -1.37        |\n",
      "|    learning_rate        | 0.00455      |\n",
      "|    loss                 | 0.00214      |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | 0.0118       |\n",
      "|    std                  | 0.00757      |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 479      |\n",
      "|    total_timesteps | 270144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274176, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 274176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022245508 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.07        |\n",
      "|    explained_variance   | -2.54       |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | 0.023       |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | 0.00819     |\n",
      "|    std                  | 0.00772     |\n",
      "|    value_loss           | 7.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 486      |\n",
      "|    total_timesteps | 274176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278208, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0199     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 278208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043590214 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.11        |\n",
      "|    explained_variance   | -1.38       |\n",
      "|    learning_rate        | 0.00454     |\n",
      "|    loss                 | 0.061       |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | 0.0132      |\n",
      "|    std                  | 0.00748     |\n",
      "|    value_loss           | 7.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 493      |\n",
      "|    total_timesteps | 278208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009664848 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | -1.87       |\n",
      "|    learning_rate        | 0.00453     |\n",
      "|    loss                 | 0.00124     |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | 0.00878     |\n",
      "|    std                  | 0.00744     |\n",
      "|    value_loss           | 8.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 500      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286272, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00948    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 286272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018098578 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.17        |\n",
      "|    explained_variance   | -2.09       |\n",
      "|    learning_rate        | 0.00453     |\n",
      "|    loss                 | 0.00607     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.0216      |\n",
      "|    std                  | 0.00729     |\n",
      "|    value_loss           | 6.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 508      |\n",
      "|    total_timesteps | 286272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290304, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00899     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 290304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059365733 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.18         |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.00452      |\n",
      "|    loss                 | -0.00285     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | 0.0153       |\n",
      "|    std                  | 0.00723      |\n",
      "|    value_loss           | 3.31e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 290304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294336, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 294336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110013615 |\n",
      "|    clip_fraction        | 0.242        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.19         |\n",
      "|    explained_variance   | -0.197       |\n",
      "|    learning_rate        | 0.00451      |\n",
      "|    loss                 | 0.00801      |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | 0.0132       |\n",
      "|    std                  | 0.00718      |\n",
      "|    value_loss           | 1.44e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 522      |\n",
      "|    total_timesteps | 294336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298368, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0122    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 298368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03202745 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.21       |\n",
      "|    explained_variance   | -1.52      |\n",
      "|    learning_rate        | 0.00451    |\n",
      "|    loss                 | 0.00496    |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | 0.0206     |\n",
      "|    std                  | 0.0071     |\n",
      "|    value_loss           | 8.66e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 529      |\n",
      "|    total_timesteps | 298368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0148      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 302400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093060825 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.24         |\n",
      "|    explained_variance   | -1.31        |\n",
      "|    learning_rate        | 0.0045       |\n",
      "|    loss                 | -0.00106     |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | 0.0083       |\n",
      "|    std                  | 0.00704      |\n",
      "|    value_loss           | 6.97e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 536      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306432, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 306432     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03461474 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.27       |\n",
      "|    explained_variance   | -2.06      |\n",
      "|    learning_rate        | 0.00449    |\n",
      "|    loss                 | -0.000173  |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | 0.0194     |\n",
      "|    std                  | 0.00695    |\n",
      "|    value_loss           | 9.43e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 543      |\n",
      "|    total_timesteps | 306432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310464, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 310464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025282202 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.24        |\n",
      "|    explained_variance   | -1.33       |\n",
      "|    learning_rate        | 0.00449     |\n",
      "|    loss                 | 0.0326      |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | 0.0204      |\n",
      "|    std                  | 0.00702     |\n",
      "|    value_loss           | 7.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 550      |\n",
      "|    total_timesteps | 310464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314496, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 314496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035002258 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.27        |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.00448     |\n",
      "|    loss                 | 0.058       |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | 0.00653     |\n",
      "|    std                  | 0.00685     |\n",
      "|    value_loss           | 2.22e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 557      |\n",
      "|    total_timesteps | 314496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318528, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 318528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018448526 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.28        |\n",
      "|    explained_variance   | -1.31       |\n",
      "|    learning_rate        | 0.00447     |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | 0.0392      |\n",
      "|    std                  | 0.00684     |\n",
      "|    value_loss           | 6e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 564      |\n",
      "|    total_timesteps | 318528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024121067 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.35        |\n",
      "|    explained_variance   | -1.02       |\n",
      "|    learning_rate        | 0.00447     |\n",
      "|    loss                 | 0.0384      |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.00651     |\n",
      "|    value_loss           | 8e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 571      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326592, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 326592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014247404 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.42        |\n",
      "|    explained_variance   | -1.49       |\n",
      "|    learning_rate        | 0.00446     |\n",
      "|    loss                 | 0.00825     |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | 0.00501     |\n",
      "|    std                  | 0.00637     |\n",
      "|    value_loss           | 9.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 578      |\n",
      "|    total_timesteps | 326592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330624, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010249079 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.45        |\n",
      "|    explained_variance   | -1.39       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | 0.0039      |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.0162      |\n",
      "|    std                  | 0.0063      |\n",
      "|    value_loss           | 5.37e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 585      |\n",
      "|    total_timesteps | 330624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334656, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00968     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 334656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099679055 |\n",
      "|    clip_fraction        | 0.176        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.44         |\n",
      "|    explained_variance   | -0.847       |\n",
      "|    learning_rate        | 0.00445      |\n",
      "|    loss                 | -0.000467    |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | 0.00777      |\n",
      "|    std                  | 0.00623      |\n",
      "|    value_loss           | 6.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 593      |\n",
      "|    total_timesteps | 334656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338688, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 338688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013409927 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.48        |\n",
      "|    explained_variance   | -1.16       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.00265     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.0061      |\n",
      "|    value_loss           | 4.04e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 600      |\n",
      "|    total_timesteps | 338688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025125667 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.55        |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.00443     |\n",
      "|    loss                 | 0.0312      |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    std                  | 0.00595     |\n",
      "|    value_loss           | 1.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 607      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346752, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 346752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073083057 |\n",
      "|    clip_fraction        | 0.298        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.61         |\n",
      "|    explained_variance   | -0.925       |\n",
      "|    learning_rate        | 0.00443      |\n",
      "|    loss                 | -0.00202     |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | 0.0179       |\n",
      "|    std                  | 0.00582      |\n",
      "|    value_loss           | 5.07e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 615      |\n",
      "|    total_timesteps | 346752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350784, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0118    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 350784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04305744 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.62       |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.00442    |\n",
      "|    loss                 | 0.0561     |\n",
      "|    n_updates            | 1730       |\n",
      "|    policy_gradient_loss | 0.0182     |\n",
      "|    std                  | 0.00574    |\n",
      "|    value_loss           | 1.13e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 622      |\n",
      "|    total_timesteps | 350784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354816, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0118      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 354816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077834637 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.62         |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.00441      |\n",
      "|    loss                 | 0.00044      |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | 0.00734      |\n",
      "|    std                  | 0.00581      |\n",
      "|    value_loss           | 4.22e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 629      |\n",
      "|    total_timesteps | 354816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358848, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 358848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011917865 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.63        |\n",
      "|    explained_variance   | -2.15       |\n",
      "|    learning_rate        | 0.00441     |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | 0.0182      |\n",
      "|    std                  | 0.00578     |\n",
      "|    value_loss           | 5.33e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 636      |\n",
      "|    total_timesteps | 358848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0135   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 362880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0120675 |\n",
      "|    clip_fraction        | 0.281     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 7.6       |\n",
      "|    explained_variance   | -0.174    |\n",
      "|    learning_rate        | 0.0044    |\n",
      "|    loss                 | 0.00988   |\n",
      "|    n_updates            | 1790      |\n",
      "|    policy_gradient_loss | 0.0109    |\n",
      "|    std                  | 0.00598   |\n",
      "|    value_loss           | 3.74e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 643      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366912, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0133    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 366912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01971154 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | -1.74      |\n",
      "|    learning_rate        | 0.00439    |\n",
      "|    loss                 | 0.0326     |\n",
      "|    n_updates            | 1810       |\n",
      "|    policy_gradient_loss | 0.00941    |\n",
      "|    std                  | 0.00587    |\n",
      "|    value_loss           | 7.09e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 650      |\n",
      "|    total_timesteps | 366912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370944, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032377206 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.66        |\n",
      "|    explained_variance   | 0.0691      |\n",
      "|    learning_rate        | 0.00439     |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.00575     |\n",
      "|    value_loss           | 1.57e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 370944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374976, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0152     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 374976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012806471 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.71        |\n",
      "|    explained_variance   | -0.946      |\n",
      "|    learning_rate        | 0.00438     |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | 0.00737     |\n",
      "|    std                  | 0.00562     |\n",
      "|    value_loss           | 5.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 664      |\n",
      "|    total_timesteps | 374976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379008, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 379008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032664187 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.72        |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00437     |\n",
      "|    loss                 | 0.0307      |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.0175      |\n",
      "|    std                  | 0.00563     |\n",
      "|    value_loss           | 1.89e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 671      |\n",
      "|    total_timesteps | 379008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0146     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038005885 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.71        |\n",
      "|    explained_variance   | -0.707      |\n",
      "|    learning_rate        | 0.00436     |\n",
      "|    loss                 | 0.0197      |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.0243      |\n",
      "|    std                  | 0.00569     |\n",
      "|    value_loss           | 4.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 678      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387072, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00987   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 387072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05191376 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.7        |\n",
      "|    explained_variance   | -0.768     |\n",
      "|    learning_rate        | 0.00436    |\n",
      "|    loss                 | 0.0624     |\n",
      "|    n_updates            | 1910       |\n",
      "|    policy_gradient_loss | 0.0181     |\n",
      "|    std                  | 0.00563    |\n",
      "|    value_loss           | 3.31e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 685      |\n",
      "|    total_timesteps | 387072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391104, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0119    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 391104     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02825198 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.7        |\n",
      "|    explained_variance   | -1.33      |\n",
      "|    learning_rate        | 0.00435    |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 1930       |\n",
      "|    policy_gradient_loss | 0.0226     |\n",
      "|    std                  | 0.00566    |\n",
      "|    value_loss           | 5.18e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 693      |\n",
      "|    total_timesteps | 391104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395136, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 395136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051860638 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.72        |\n",
      "|    explained_variance   | -1.15       |\n",
      "|    learning_rate        | 0.00434     |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.0287      |\n",
      "|    std                  | 0.00563     |\n",
      "|    value_loss           | 4.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 700      |\n",
      "|    total_timesteps | 395136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399168, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0147    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 399168     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02443957 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.72       |\n",
      "|    explained_variance   | -0.697     |\n",
      "|    learning_rate        | 0.00434    |\n",
      "|    loss                 | -0.00379   |\n",
      "|    n_updates            | 1970       |\n",
      "|    policy_gradient_loss | 0.0314     |\n",
      "|    std                  | 0.00565    |\n",
      "|    value_loss           | 8.12e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 707      |\n",
      "|    total_timesteps | 399168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0156      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 403200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064078453 |\n",
      "|    clip_fraction        | 0.397        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.74         |\n",
      "|    explained_variance   | -0.729       |\n",
      "|    learning_rate        | 0.00433      |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | 0.0542       |\n",
      "|    std                  | 0.0056       |\n",
      "|    value_loss           | 6.49e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 714      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=407232, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 407232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008586265 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.77        |\n",
      "|    explained_variance   | -0.872      |\n",
      "|    learning_rate        | 0.00432     |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | 0.00688     |\n",
      "|    std                  | 0.00553     |\n",
      "|    value_loss           | 3.83e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 721      |\n",
      "|    total_timesteps | 407232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411264, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 411264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010963056 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.75        |\n",
      "|    explained_variance   | -1.48       |\n",
      "|    learning_rate        | 0.00432     |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.044       |\n",
      "|    std                  | 0.00553     |\n",
      "|    value_loss           | 4.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 728      |\n",
      "|    total_timesteps | 411264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415296, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0127    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 415296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04648626 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.74       |\n",
      "|    explained_variance   | -0.734     |\n",
      "|    learning_rate        | 0.00431    |\n",
      "|    loss                 | 0.0171     |\n",
      "|    n_updates            | 2050       |\n",
      "|    policy_gradient_loss | 0.0199     |\n",
      "|    std                  | 0.00553    |\n",
      "|    value_loss           | 3.69e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 735      |\n",
      "|    total_timesteps | 415296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=419328, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 419328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078000454 |\n",
      "|    clip_fraction        | 0.217        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.74         |\n",
      "|    explained_variance   | -0.816       |\n",
      "|    learning_rate        | 0.0043       |\n",
      "|    loss                 | 0.000139     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | 0.0109       |\n",
      "|    std                  | 0.00555      |\n",
      "|    value_loss           | 7.84e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 742      |\n",
      "|    total_timesteps | 419328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009846723 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | -0.526      |\n",
      "|    learning_rate        | 0.0043      |\n",
      "|    loss                 | 0.00481     |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.0075      |\n",
      "|    std                  | 0.00545     |\n",
      "|    value_loss           | 7.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 749      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=427392, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0158     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 427392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005085516 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | -1.21       |\n",
      "|    learning_rate        | 0.00429     |\n",
      "|    loss                 | -0.00125    |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    std                  | 0.00545     |\n",
      "|    value_loss           | 4.28e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 756      |\n",
      "|    total_timesteps | 427392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431424, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 431424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012029973 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.83        |\n",
      "|    explained_variance   | -0.00961    |\n",
      "|    learning_rate        | 0.00428     |\n",
      "|    loss                 | 0.00233     |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | 0.00755     |\n",
      "|    std                  | 0.00529     |\n",
      "|    value_loss           | 3.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 763      |\n",
      "|    total_timesteps | 431424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435456, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0127    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 435456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07590477 |\n",
      "|    clip_fraction        | 0.412      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.93       |\n",
      "|    explained_variance   | -0.5       |\n",
      "|    learning_rate        | 0.00428    |\n",
      "|    loss                 | 0.01       |\n",
      "|    n_updates            | 2150       |\n",
      "|    policy_gradient_loss | 0.0317     |\n",
      "|    std                  | 0.0051     |\n",
      "|    value_loss           | 3.68e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 771      |\n",
      "|    total_timesteps | 435456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439488, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 439488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016035281 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.91        |\n",
      "|    explained_variance   | -0.842      |\n",
      "|    learning_rate        | 0.00427     |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | 0.0341      |\n",
      "|    std                  | 0.0052      |\n",
      "|    value_loss           | 4.44e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 778      |\n",
      "|    total_timesteps | 439488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0115    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 443520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11652083 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.92       |\n",
      "|    explained_variance   | -0.641     |\n",
      "|    learning_rate        | 0.00426    |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 2190       |\n",
      "|    policy_gradient_loss | 0.0164     |\n",
      "|    std                  | 0.00523    |\n",
      "|    value_loss           | 1.36e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 785      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447552, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 447552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10865182 |\n",
      "|    clip_fraction        | 0.556      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.95       |\n",
      "|    explained_variance   | -0.474     |\n",
      "|    learning_rate        | 0.00426    |\n",
      "|    loss                 | 0.0597     |\n",
      "|    n_updates            | 2210       |\n",
      "|    policy_gradient_loss | 0.0613     |\n",
      "|    std                  | 0.00506    |\n",
      "|    value_loss           | 7.29e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 792      |\n",
      "|    total_timesteps | 447552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451584, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0173     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 451584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022620171 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.98        |\n",
      "|    explained_variance   | -0.447      |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.00666     |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    std                  | 0.00483     |\n",
      "|    value_loss           | 4.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 799      |\n",
      "|    total_timesteps | 451584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455616, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0153    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 455616     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03201082 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8          |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 0.00424    |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 2250       |\n",
      "|    policy_gradient_loss | 0.015      |\n",
      "|    std                  | 0.00492    |\n",
      "|    value_loss           | 7.96e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 806      |\n",
      "|    total_timesteps | 455616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459648, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 459648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010930156 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.97        |\n",
      "|    explained_variance   | -0.182      |\n",
      "|    learning_rate        | 0.00424     |\n",
      "|    loss                 | 0.000955    |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.0242      |\n",
      "|    std                  | 0.00497     |\n",
      "|    value_loss           | 9.64e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 814      |\n",
      "|    total_timesteps | 459648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042357817 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.97        |\n",
      "|    explained_variance   | 0.0281      |\n",
      "|    learning_rate        | 0.00423     |\n",
      "|    loss                 | 0.00711     |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.00502     |\n",
      "|    value_loss           | 3.86e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 821      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467712, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 467712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016132083 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8           |\n",
      "|    explained_variance   | -0.517      |\n",
      "|    learning_rate        | 0.00422     |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.00492     |\n",
      "|    value_loss           | 6.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 828      |\n",
      "|    total_timesteps | 467712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471744, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0111    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 471744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01581309 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.02       |\n",
      "|    explained_variance   | -0.967     |\n",
      "|    learning_rate        | 0.00422    |\n",
      "|    loss                 | 0.00306    |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | 0.018      |\n",
      "|    std                  | 0.00499    |\n",
      "|    value_loss           | 8.05e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 835      |\n",
      "|    total_timesteps | 471744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475776, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 475776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028716877 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.99        |\n",
      "|    explained_variance   | -1.35       |\n",
      "|    learning_rate        | 0.00421     |\n",
      "|    loss                 | 0.01        |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | 0.0224      |\n",
      "|    std                  | 0.00498     |\n",
      "|    value_loss           | 2.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 842      |\n",
      "|    total_timesteps | 475776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=479808, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0136    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 479808     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14920983 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.07       |\n",
      "|    explained_variance   | -1.29      |\n",
      "|    learning_rate        | 0.0042     |\n",
      "|    loss                 | 0.0697     |\n",
      "|    n_updates            | 2370       |\n",
      "|    policy_gradient_loss | 0.0463     |\n",
      "|    std                  | 0.00474    |\n",
      "|    value_loss           | 6.29e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 849      |\n",
      "|    total_timesteps | 479808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053766213 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.12        |\n",
      "|    explained_variance   | -1.03       |\n",
      "|    learning_rate        | 0.0042      |\n",
      "|    loss                 | 0.0399      |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    std                  | 0.00463     |\n",
      "|    value_loss           | 7.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 856      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487872, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 487872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017172156 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.18        |\n",
      "|    explained_variance   | 0.0664      |\n",
      "|    learning_rate        | 0.00419     |\n",
      "|    loss                 | -0.00146    |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.0177      |\n",
      "|    std                  | 0.0046      |\n",
      "|    value_loss           | 1.27e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 863      |\n",
      "|    total_timesteps | 487872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=491904, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0129    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 491904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02256687 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.25       |\n",
      "|    explained_variance   | -0.0336    |\n",
      "|    learning_rate        | 0.00418    |\n",
      "|    loss                 | -0.011     |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | 0.0108     |\n",
      "|    std                  | 0.0043     |\n",
      "|    value_loss           | 8.22e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 870      |\n",
      "|    total_timesteps | 491904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495936, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0137    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 495936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14810322 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | -1.02      |\n",
      "|    learning_rate        | 0.00418    |\n",
      "|    loss                 | 0.0836     |\n",
      "|    n_updates            | 2450       |\n",
      "|    policy_gradient_loss | 0.0873     |\n",
      "|    std                  | 0.00429    |\n",
      "|    value_loss           | 7.07e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 878      |\n",
      "|    total_timesteps | 495936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=499968, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 499968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022200847 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.511      |\n",
      "|    learning_rate        | 0.00417     |\n",
      "|    loss                 | -0.00337    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | 0.0164      |\n",
      "|    std                  | 0.00407     |\n",
      "|    value_loss           | 4.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 885      |\n",
      "|    total_timesteps | 499968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0188     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016414769 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 0.00416     |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | 0.0213      |\n",
      "|    std                  | 0.00412     |\n",
      "|    value_loss           | 9.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 892      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508032, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 508032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030657254 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.00416     |\n",
      "|    loss                 | 0.0166      |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | 0.0317      |\n",
      "|    std                  | 0.00414     |\n",
      "|    value_loss           | 2.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 899      |\n",
      "|    total_timesteps | 508032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512064, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0129    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 512064     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01841908 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.34       |\n",
      "|    explained_variance   | -0.744     |\n",
      "|    learning_rate        | 0.00415    |\n",
      "|    loss                 | 0.00276    |\n",
      "|    n_updates            | 2530       |\n",
      "|    policy_gradient_loss | 0.0255     |\n",
      "|    std                  | 0.00416    |\n",
      "|    value_loss           | 3.86e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 906      |\n",
      "|    total_timesteps | 512064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516096, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020504113 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.37        |\n",
      "|    explained_variance   | -1.19       |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | 0.00121     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    std                  | 0.00408     |\n",
      "|    value_loss           | 1.41e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 913      |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520128, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.00959  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 520128    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0378007 |\n",
      "|    clip_fraction        | 0.31      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 8.37      |\n",
      "|    explained_variance   | -0.231    |\n",
      "|    learning_rate        | 0.00414   |\n",
      "|    loss                 | 0.0266    |\n",
      "|    n_updates            | 2570      |\n",
      "|    policy_gradient_loss | 0.0101    |\n",
      "|    std                  | 0.00409   |\n",
      "|    value_loss           | 2.11e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 920      |\n",
      "|    total_timesteps | 520128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027106386 |\n",
      "|    clip_fraction        | 0.465       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.37        |\n",
      "|    explained_variance   | -1.11       |\n",
      "|    learning_rate        | 0.00413     |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.0435      |\n",
      "|    std                  | 0.00405     |\n",
      "|    value_loss           | 3.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 927      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528192, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0147    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 528192     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01736394 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.44       |\n",
      "|    explained_variance   | -0.917     |\n",
      "|    learning_rate        | 0.00412    |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 2610       |\n",
      "|    policy_gradient_loss | 0.0304     |\n",
      "|    std                  | 0.00401    |\n",
      "|    value_loss           | 5.13e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 934      |\n",
      "|    total_timesteps | 528192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532224, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 532224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077221796 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.44        |\n",
      "|    explained_variance   | -0.0446     |\n",
      "|    learning_rate        | 0.00412     |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.0256      |\n",
      "|    std                  | 0.00403     |\n",
      "|    value_loss           | 8.8e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 941      |\n",
      "|    total_timesteps | 532224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536256, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0157     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 536256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027095413 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.51        |\n",
      "|    explained_variance   | -1.58       |\n",
      "|    learning_rate        | 0.00411     |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | 0.0192      |\n",
      "|    std                  | 0.00385     |\n",
      "|    value_loss           | 7.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 948      |\n",
      "|    total_timesteps | 536256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540288, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0108    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 540288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09741471 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | -0.148     |\n",
      "|    learning_rate        | 0.0041     |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 2670       |\n",
      "|    policy_gradient_loss | 0.0326     |\n",
      "|    std                  | 0.0039     |\n",
      "|    value_loss           | 1.57e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 955      |\n",
      "|    total_timesteps | 540288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091274835 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.52        |\n",
      "|    explained_variance   | -0.307      |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | 0.0535      |\n",
      "|    std                  | 0.0038      |\n",
      "|    value_loss           | 8.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 962      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548352, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 548352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027038258 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | -0.77       |\n",
      "|    learning_rate        | 0.00409     |\n",
      "|    loss                 | 0.0177      |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | 0.0466      |\n",
      "|    std                  | 0.00392     |\n",
      "|    value_loss           | 2.64e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 969      |\n",
      "|    total_timesteps | 548352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552384, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 552384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017853042 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.47        |\n",
      "|    explained_variance   | -0.191      |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | 0.0319      |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | 0.0262      |\n",
      "|    std                  | 0.00389     |\n",
      "|    value_loss           | 3.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 976      |\n",
      "|    total_timesteps | 552384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556416, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0142    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 556416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16470493 |\n",
      "|    clip_fraction        | 0.403      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.47       |\n",
      "|    explained_variance   | -0.205     |\n",
      "|    learning_rate        | 0.00408    |\n",
      "|    loss                 | 0.0324     |\n",
      "|    n_updates            | 2750       |\n",
      "|    policy_gradient_loss | 0.0162     |\n",
      "|    std                  | 0.00395    |\n",
      "|    value_loss           | 1.49e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 983      |\n",
      "|    total_timesteps | 556416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560448, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0127      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103078205 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.43         |\n",
      "|    explained_variance   | -0.165       |\n",
      "|    learning_rate        | 0.00407      |\n",
      "|    loss                 | 0.0152       |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | 0.0331       |\n",
      "|    std                  | 0.00398      |\n",
      "|    value_loss           | 3.25e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 990      |\n",
      "|    total_timesteps | 560448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 564480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10108451 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.42       |\n",
      "|    explained_variance   | -0.071     |\n",
      "|    learning_rate        | 0.00406    |\n",
      "|    loss                 | 0.0393     |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | 0.0158     |\n",
      "|    std                  | 0.00393    |\n",
      "|    value_loss           | 1.18e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 997      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568512, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0108    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 568512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07220048 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.44       |\n",
      "|    explained_variance   | -0.381     |\n",
      "|    learning_rate        | 0.00406    |\n",
      "|    loss                 | 0.0883     |\n",
      "|    n_updates            | 2810       |\n",
      "|    policy_gradient_loss | 0.025      |\n",
      "|    std                  | 0.00383    |\n",
      "|    value_loss           | 1.7e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 1004     |\n",
      "|    total_timesteps | 568512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572544, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 572544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048083603 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.45        |\n",
      "|    explained_variance   | -0.628      |\n",
      "|    learning_rate        | 0.00405     |\n",
      "|    loss                 | 0.0647      |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | 0.042       |\n",
      "|    std                  | 0.00385     |\n",
      "|    value_loss           | 3.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 1011     |\n",
      "|    total_timesteps | 572544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576576, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0166     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 576576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019447438 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.46        |\n",
      "|    explained_variance   | -0.358      |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | 0.0125      |\n",
      "|    std                  | 0.00372     |\n",
      "|    value_loss           | 2.32e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 1019     |\n",
      "|    total_timesteps | 576576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580608, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032972254 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | -0.149      |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    std                  | 0.00372     |\n",
      "|    value_loss           | 2.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 1026     |\n",
      "|    total_timesteps | 580608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00914    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057475362 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.43        |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00403     |\n",
      "|    loss                 | -0.0057     |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | 0.00934     |\n",
      "|    std                  | 0.00388     |\n",
      "|    value_loss           | 5.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 1033     |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588672, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0126    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 588672     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17210865 |\n",
      "|    clip_fraction        | 0.411      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.49       |\n",
      "|    explained_variance   | 0.416      |\n",
      "|    learning_rate        | 0.00402    |\n",
      "|    loss                 | 0.068      |\n",
      "|    n_updates            | 2910       |\n",
      "|    policy_gradient_loss | 0.0334     |\n",
      "|    std                  | 0.00367    |\n",
      "|    value_loss           | 2.34e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 588672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592704, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0163     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 592704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021393828 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.52        |\n",
      "|    explained_variance   | -0.63       |\n",
      "|    learning_rate        | 0.00402     |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | 0.0398      |\n",
      "|    std                  | 0.00364     |\n",
      "|    value_loss           | 5.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 1047     |\n",
      "|    total_timesteps | 592704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596736, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00985    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 596736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033699896 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.49        |\n",
      "|    explained_variance   | -0.618      |\n",
      "|    learning_rate        | 0.00401     |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | 0.031       |\n",
      "|    std                  | 0.00374     |\n",
      "|    value_loss           | 4.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 1054     |\n",
      "|    total_timesteps | 596736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600768, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0162     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019869467 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.44        |\n",
      "|    explained_variance   | -0.706      |\n",
      "|    learning_rate        | 0.004       |\n",
      "|    loss                 | -0.00283    |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    std                  | 0.00385     |\n",
      "|    value_loss           | 4.64e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 1061     |\n",
      "|    total_timesteps | 600768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026073605 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.00554    |\n",
      "|    learning_rate        | 0.004       |\n",
      "|    loss                 | 0.00444     |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | 0.0167      |\n",
      "|    std                  | 0.00393     |\n",
      "|    value_loss           | 7.27e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 1069     |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608832, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 608832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026377164 |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.37        |\n",
      "|    explained_variance   | -0.382      |\n",
      "|    learning_rate        | 0.00399     |\n",
      "|    loss                 | 0.0211      |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | 0.0772      |\n",
      "|    std                  | 0.00403     |\n",
      "|    value_loss           | 2.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 1076     |\n",
      "|    total_timesteps | 608832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612864, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 612864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051869694 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.46        |\n",
      "|    explained_variance   | -1.4        |\n",
      "|    learning_rate        | 0.00398     |\n",
      "|    loss                 | 0.0619      |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | 0.0232      |\n",
      "|    std                  | 0.00391     |\n",
      "|    value_loss           | 3.17e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 1084     |\n",
      "|    total_timesteps | 612864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616896, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 616896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025120236 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.42        |\n",
      "|    explained_variance   | -0.165      |\n",
      "|    learning_rate        | 0.00398     |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    std                  | 0.00399     |\n",
      "|    value_loss           | 1.41e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 1091     |\n",
      "|    total_timesteps | 616896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620928, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057029817 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.43        |\n",
      "|    explained_variance   | -0.316      |\n",
      "|    learning_rate        | 0.00397     |\n",
      "|    loss                 | 0.0679      |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | 0.0404      |\n",
      "|    std                  | 0.00394     |\n",
      "|    value_loss           | 7.49e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 1098     |\n",
      "|    total_timesteps | 620928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00998    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007631016 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.43        |\n",
      "|    explained_variance   | -0.207      |\n",
      "|    learning_rate        | 0.00396     |\n",
      "|    loss                 | 0.00253     |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | 0.0174      |\n",
      "|    std                  | 0.00392     |\n",
      "|    value_loss           | 1.15e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 1104     |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628992, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0164     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 628992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030990101 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.442      |\n",
      "|    learning_rate        | 0.00396     |\n",
      "|    loss                 | 0.00813     |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    std                  | 0.00402     |\n",
      "|    value_loss           | 8.4e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 1111     |\n",
      "|    total_timesteps | 628992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633024, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0156    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 633024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03494492 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.4        |\n",
      "|    explained_variance   | -0.288     |\n",
      "|    learning_rate        | 0.00395    |\n",
      "|    loss                 | 0.0385     |\n",
      "|    n_updates            | 3130       |\n",
      "|    policy_gradient_loss | 0.0259     |\n",
      "|    std                  | 0.00398    |\n",
      "|    value_loss           | 5.48e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 1118     |\n",
      "|    total_timesteps | 633024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637056, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00918    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 637056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078551434 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.43        |\n",
      "|    explained_variance   | 0.0383      |\n",
      "|    learning_rate        | 0.00394     |\n",
      "|    loss                 | 0.0805      |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | 0.0333      |\n",
      "|    std                  | 0.00395     |\n",
      "|    value_loss           | 3.45e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 1124     |\n",
      "|    total_timesteps | 637056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641088, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 641088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054194413 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.42        |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00393     |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    std                  | 0.00401     |\n",
      "|    value_loss           | 1.57e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 1131     |\n",
      "|    total_timesteps | 641088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00975    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032572515 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.44        |\n",
      "|    explained_variance   | -0.38       |\n",
      "|    learning_rate        | 0.00393     |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | 0.0273      |\n",
      "|    std                  | 0.00392     |\n",
      "|    value_loss           | 5.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 1138     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649152, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0107    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 649152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04138034 |\n",
      "|    clip_fraction        | 0.479      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.46       |\n",
      "|    explained_variance   | -0.239     |\n",
      "|    learning_rate        | 0.00392    |\n",
      "|    loss                 | 0.046      |\n",
      "|    n_updates            | 3210       |\n",
      "|    policy_gradient_loss | 0.0474     |\n",
      "|    std                  | 0.00395    |\n",
      "|    value_loss           | 4.52e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 1145     |\n",
      "|    total_timesteps | 649152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653184, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.01      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 653184     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02993986 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.46       |\n",
      "|    explained_variance   | -0.409     |\n",
      "|    learning_rate        | 0.00391    |\n",
      "|    loss                 | 0.00658    |\n",
      "|    n_updates            | 3230       |\n",
      "|    policy_gradient_loss | 0.0279     |\n",
      "|    std                  | 0.00393    |\n",
      "|    value_loss           | 3.8e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 1151     |\n",
      "|    total_timesteps | 653184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657216, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00914   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 657216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02581322 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.55       |\n",
      "|    explained_variance   | -0.362     |\n",
      "|    learning_rate        | 0.00391    |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | 0.0249     |\n",
      "|    std                  | 0.00373    |\n",
      "|    value_loss           | 5.54e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 1158     |\n",
      "|    total_timesteps | 657216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661248, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 661248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026276764 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.56        |\n",
      "|    explained_variance   | -0.377      |\n",
      "|    learning_rate        | 0.0039      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | 0.0243      |\n",
      "|    std                  | 0.0038      |\n",
      "|    value_loss           | 1.33e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 1165     |\n",
      "|    total_timesteps | 661248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 665280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06990869 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.47       |\n",
      "|    explained_variance   | 0.0915     |\n",
      "|    learning_rate        | 0.00389    |\n",
      "|    loss                 | 0.00162    |\n",
      "|    n_updates            | 3290       |\n",
      "|    policy_gradient_loss | 0.0111     |\n",
      "|    std                  | 0.00403    |\n",
      "|    value_loss           | 1.63e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 1171     |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669312, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0078    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 669312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10314533 |\n",
      "|    clip_fraction        | 0.412      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.49       |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.00389    |\n",
      "|    loss                 | 0.0653     |\n",
      "|    n_updates            | 3310       |\n",
      "|    policy_gradient_loss | 0.027      |\n",
      "|    std                  | 0.0039     |\n",
      "|    value_loss           | 7.84e-06   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 1178     |\n",
      "|    total_timesteps | 669312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673344, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0123    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 673344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16577151 |\n",
      "|    clip_fraction        | 0.493      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.55       |\n",
      "|    explained_variance   | 0.191      |\n",
      "|    learning_rate        | 0.00388    |\n",
      "|    loss                 | 0.0801     |\n",
      "|    n_updates            | 3330       |\n",
      "|    policy_gradient_loss | 0.0349     |\n",
      "|    std                  | 0.00384    |\n",
      "|    value_loss           | 1.46e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 1185     |\n",
      "|    total_timesteps | 673344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=677376, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0133    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 677376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05593686 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.53       |\n",
      "|    explained_variance   | -0.261     |\n",
      "|    learning_rate        | 0.00387    |\n",
      "|    loss                 | 0.0302     |\n",
      "|    n_updates            | 3350       |\n",
      "|    policy_gradient_loss | 0.058      |\n",
      "|    std                  | 0.00378    |\n",
      "|    value_loss           | 7.3e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 1192     |\n",
      "|    total_timesteps | 677376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681408, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0148     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 681408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025498921 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.51        |\n",
      "|    explained_variance   | -0.348      |\n",
      "|    learning_rate        | 0.00387     |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    std                  | 0.00377     |\n",
      "|    value_loss           | 3.64e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 1199     |\n",
      "|    total_timesteps | 681408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0115     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 685440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033041287 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.49        |\n",
      "|    explained_variance   | -0.323      |\n",
      "|    learning_rate        | 0.00386     |\n",
      "|    loss                 | 0.0421      |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | 0.0448      |\n",
      "|    std                  | 0.00387     |\n",
      "|    value_loss           | 3.34e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 1205     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689472, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 689472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014514551 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.47        |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.00385     |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | 0.0161      |\n",
      "|    std                  | 0.00395     |\n",
      "|    value_loss           | 4.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 1212     |\n",
      "|    total_timesteps | 689472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693504, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 693504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023276534 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.5         |\n",
      "|    explained_variance   | -0.194      |\n",
      "|    learning_rate        | 0.00385     |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.00385     |\n",
      "|    value_loss           | 4.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 1219     |\n",
      "|    total_timesteps | 693504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697536, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0181     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 697536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043192975 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | 0.0376      |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | 0.0405      |\n",
      "|    std                  | 0.00406     |\n",
      "|    value_loss           | 1.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 1226     |\n",
      "|    total_timesteps | 697536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701568, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 701568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025443736 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.0965     |\n",
      "|    learning_rate        | 0.00383     |\n",
      "|    loss                 | 0.02        |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | 0.024       |\n",
      "|    std                  | 0.00417     |\n",
      "|    value_loss           | 6.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 1233     |\n",
      "|    total_timesteps | 701568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 705600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15309374 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.35       |\n",
      "|    explained_variance   | -0.0352    |\n",
      "|    learning_rate        | 0.00383    |\n",
      "|    loss                 | 0.0708     |\n",
      "|    n_updates            | 3490       |\n",
      "|    policy_gradient_loss | 0.0245     |\n",
      "|    std                  | 0.00422    |\n",
      "|    value_loss           | 4.71e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 1239     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709632, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0193    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 709632     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05577546 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.38       |\n",
      "|    explained_variance   | 0.146      |\n",
      "|    learning_rate        | 0.00382    |\n",
      "|    loss                 | 0.0153     |\n",
      "|    n_updates            | 3510       |\n",
      "|    policy_gradient_loss | 0.0243     |\n",
      "|    std                  | 0.0041     |\n",
      "|    value_loss           | 8.14e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 1246     |\n",
      "|    total_timesteps | 709632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713664, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0142    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 713664     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05248298 |\n",
      "|    clip_fraction        | 0.475      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.32       |\n",
      "|    explained_variance   | -0.12      |\n",
      "|    learning_rate        | 0.00381    |\n",
      "|    loss                 | 0.0784     |\n",
      "|    n_updates            | 3530       |\n",
      "|    policy_gradient_loss | 0.0385     |\n",
      "|    std                  | 0.00424    |\n",
      "|    value_loss           | 6.23e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 1253     |\n",
      "|    total_timesteps | 713664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717696, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0138     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 717696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043409623 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.29        |\n",
      "|    explained_variance   | -0.257      |\n",
      "|    learning_rate        | 0.00381     |\n",
      "|    loss                 | -0.00242    |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | 0.05        |\n",
      "|    std                  | 0.00421     |\n",
      "|    value_loss           | 3.06e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 1260     |\n",
      "|    total_timesteps | 717696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721728, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0117    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 721728     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15732154 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.37       |\n",
      "|    explained_variance   | -0.218     |\n",
      "|    learning_rate        | 0.0038     |\n",
      "|    loss                 | 0.0595     |\n",
      "|    n_updates            | 3570       |\n",
      "|    policy_gradient_loss | 0.0373     |\n",
      "|    std                  | 0.00416    |\n",
      "|    value_loss           | 2.06e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 1266     |\n",
      "|    total_timesteps | 721728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011852622 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.4         |\n",
      "|    explained_variance   | -0.152      |\n",
      "|    learning_rate        | 0.00379     |\n",
      "|    loss                 | 0.00377     |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | 0.0207      |\n",
      "|    std                  | 0.00403     |\n",
      "|    value_loss           | 4.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 1273     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729792, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 729792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035153247 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | -0.0776     |\n",
      "|    learning_rate        | 0.00379     |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.00424     |\n",
      "|    value_loss           | 8.09e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 1280     |\n",
      "|    total_timesteps | 729792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733824, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 733824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025675023 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.243      |\n",
      "|    learning_rate        | 0.00378     |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | 0.0225      |\n",
      "|    std                  | 0.00407     |\n",
      "|    value_loss           | 2.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 1287     |\n",
      "|    total_timesteps | 733824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=737856, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 737856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021124963 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.0649     |\n",
      "|    learning_rate        | 0.00377     |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | 0.0409      |\n",
      "|    std                  | 0.00408     |\n",
      "|    value_loss           | 3.85e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 1294     |\n",
      "|    total_timesteps | 737856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741888, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0111    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 741888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05121441 |\n",
      "|    clip_fraction        | 0.446      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.43       |\n",
      "|    explained_variance   | -0.254     |\n",
      "|    learning_rate        | 0.00377    |\n",
      "|    loss                 | 0.0453     |\n",
      "|    n_updates            | 3670       |\n",
      "|    policy_gradient_loss | 0.0412     |\n",
      "|    std                  | 0.004      |\n",
      "|    value_loss           | 2.83e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 1300     |\n",
      "|    total_timesteps | 741888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014623392 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.165      |\n",
      "|    learning_rate        | 0.00376     |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | 0.016       |\n",
      "|    std                  | 0.0041      |\n",
      "|    value_loss           | 3.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 1307     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749952, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.011     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 749952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06304331 |\n",
      "|    clip_fraction        | 0.432      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.41       |\n",
      "|    explained_variance   | -0.123     |\n",
      "|    learning_rate        | 0.00375    |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 3710       |\n",
      "|    policy_gradient_loss | 0.0368     |\n",
      "|    std                  | 0.00405    |\n",
      "|    value_loss           | 4.78e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 1314     |\n",
      "|    total_timesteps | 749952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753984, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0146    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 753984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02113166 |\n",
      "|    clip_fraction        | 0.431      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.4        |\n",
      "|    explained_variance   | 0.0501     |\n",
      "|    learning_rate        | 0.00375    |\n",
      "|    loss                 | 0.00102    |\n",
      "|    n_updates            | 3730       |\n",
      "|    policy_gradient_loss | 0.0236     |\n",
      "|    std                  | 0.00406    |\n",
      "|    value_loss           | 2.59e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 1321     |\n",
      "|    total_timesteps | 753984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758016, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0124    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 758016     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15900786 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.37       |\n",
      "|    explained_variance   | -0.217     |\n",
      "|    learning_rate        | 0.00374    |\n",
      "|    loss                 | 0.036      |\n",
      "|    n_updates            | 3750       |\n",
      "|    policy_gradient_loss | 0.0265     |\n",
      "|    std                  | 0.00418    |\n",
      "|    value_loss           | 4.54e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 1327     |\n",
      "|    total_timesteps | 758016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762048, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 762048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011457738 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | -0.223      |\n",
      "|    learning_rate        | 0.00373     |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | 0.0189      |\n",
      "|    std                  | 0.00425     |\n",
      "|    value_loss           | 1.96e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 1334     |\n",
      "|    total_timesteps | 762048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0184     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033635087 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | 0.0268      |\n",
      "|    learning_rate        | 0.00373     |\n",
      "|    loss                 | 0.00218     |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | 0.0147      |\n",
      "|    std                  | 0.00429     |\n",
      "|    value_loss           | 2.34e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 1341     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770112, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.014     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 770112     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03173939 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.4        |\n",
      "|    explained_variance   | 0.00349    |\n",
      "|    learning_rate        | 0.00372    |\n",
      "|    loss                 | 0.0565     |\n",
      "|    n_updates            | 3810       |\n",
      "|    policy_gradient_loss | 0.0251     |\n",
      "|    std                  | 0.00412    |\n",
      "|    value_loss           | 1.52e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 1347     |\n",
      "|    total_timesteps | 770112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774144, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103180006 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | -0.182      |\n",
      "|    learning_rate        | 0.00371     |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | 0.045       |\n",
      "|    std                  | 0.00418     |\n",
      "|    value_loss           | 2.27e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 1354     |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778176, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0166     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 778176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046104472 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | 0.017       |\n",
      "|    learning_rate        | 0.00371     |\n",
      "|    loss                 | 0.00636     |\n",
      "|    n_updates            | 3850        |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    std                  | 0.00425     |\n",
      "|    value_loss           | 1.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 1361     |\n",
      "|    total_timesteps | 778176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782208, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 782208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016405148 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.31        |\n",
      "|    explained_variance   | -0.202      |\n",
      "|    learning_rate        | 0.0037      |\n",
      "|    loss                 | 0.0195      |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.0043      |\n",
      "|    value_loss           | 2.8e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 1368     |\n",
      "|    total_timesteps | 782208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0198     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069087885 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.3         |\n",
      "|    explained_variance   | 0.059       |\n",
      "|    learning_rate        | 0.00369     |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | 0.0159      |\n",
      "|    std                  | 0.00444     |\n",
      "|    value_loss           | 5.01e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 1374     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790272, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.013     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 790272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01981146 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.29       |\n",
      "|    explained_variance   | -0.317     |\n",
      "|    learning_rate        | 0.00369    |\n",
      "|    loss                 | 0.0341     |\n",
      "|    n_updates            | 3910       |\n",
      "|    policy_gradient_loss | 0.0227     |\n",
      "|    std                  | 0.00457    |\n",
      "|    value_loss           | 2.97e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 1381     |\n",
      "|    total_timesteps | 790272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794304, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 794304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038635008 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | -0.0871     |\n",
      "|    learning_rate        | 0.00368     |\n",
      "|    loss                 | -0.00184    |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.00439     |\n",
      "|    value_loss           | 7.39e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 1388     |\n",
      "|    total_timesteps | 794304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798336, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 798336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008008812 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.24        |\n",
      "|    explained_variance   | 0.0149      |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | 0.00107     |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.00447     |\n",
      "|    value_loss           | 7.61e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 1395     |\n",
      "|    total_timesteps | 798336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802368, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0146     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 802368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023076754 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.24        |\n",
      "|    explained_variance   | -0.13       |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | -0.00289    |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | 0.0272      |\n",
      "|    std                  | 0.00446     |\n",
      "|    value_loss           | 1.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0151  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 1401     |\n",
      "|    total_timesteps | 802368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042025466 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.00366     |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | 0.0249      |\n",
      "|    std                  | 0.00427     |\n",
      "|    value_loss           | 3.51e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0151  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 1408     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810432, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 810432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020347888 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | -0.136      |\n",
      "|    learning_rate        | 0.00365     |\n",
      "|    loss                 | 1.11e-06    |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | 0.00954     |\n",
      "|    std                  | 0.00411     |\n",
      "|    value_loss           | 8.58e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 1415     |\n",
      "|    total_timesteps | 810432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814464, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0114     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 814464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061953053 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | -0.0182     |\n",
      "|    learning_rate        | 0.00365     |\n",
      "|    loss                 | 0.0299      |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | 0.0487      |\n",
      "|    std                  | 0.00412     |\n",
      "|    value_loss           | 2.79e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 1422     |\n",
      "|    total_timesteps | 814464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818496, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.016     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 818496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04671786 |\n",
      "|    clip_fraction        | 0.329      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.39       |\n",
      "|    explained_variance   | -0.187     |\n",
      "|    learning_rate        | 0.00364    |\n",
      "|    loss                 | 0.0715     |\n",
      "|    n_updates            | 4050       |\n",
      "|    policy_gradient_loss | 0.0224     |\n",
      "|    std                  | 0.00407    |\n",
      "|    value_loss           | 4.41e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 1429     |\n",
      "|    total_timesteps | 818496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822528, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0117    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 822528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12512124 |\n",
      "|    clip_fraction        | 0.494      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.35       |\n",
      "|    explained_variance   | -0.175     |\n",
      "|    learning_rate        | 0.00363    |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 4070       |\n",
      "|    policy_gradient_loss | 0.0479     |\n",
      "|    std                  | 0.00421    |\n",
      "|    value_loss           | 2.6e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 1435     |\n",
      "|    total_timesteps | 822528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037524115 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | -0.0107     |\n",
      "|    learning_rate        | 0.00363     |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    std                  | 0.00416     |\n",
      "|    value_loss           | 6.9e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 1442     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830592, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0165    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 830592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12812214 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.38       |\n",
      "|    explained_variance   | 0.0602     |\n",
      "|    learning_rate        | 0.00362    |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 4110       |\n",
      "|    policy_gradient_loss | 0.026      |\n",
      "|    std                  | 0.00417    |\n",
      "|    value_loss           | 1.64e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 1449     |\n",
      "|    total_timesteps | 830592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834624, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0244     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 834624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029898252 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.143      |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | 0.0236      |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | 0.0254      |\n",
      "|    std                  | 0.00414     |\n",
      "|    value_loss           | 2.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 1455     |\n",
      "|    total_timesteps | 834624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838656, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0141     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 838656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085402586 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.42        |\n",
      "|    explained_variance   | -0.0366     |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | 0.0231      |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | 0.0211      |\n",
      "|    std                  | 0.00411     |\n",
      "|    value_loss           | 1.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 1462     |\n",
      "|    total_timesteps | 838656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842688, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 842688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025927536 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | -0.0616     |\n",
      "|    learning_rate        | 0.0036      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | 0.0338      |\n",
      "|    std                  | 0.00425     |\n",
      "|    value_loss           | 5.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 1469     |\n",
      "|    total_timesteps | 842688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0205     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014045711 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | -0.188      |\n",
      "|    learning_rate        | 0.00359     |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | 0.00631     |\n",
      "|    std                  | 0.00422     |\n",
      "|    value_loss           | 2.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0159  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 1476     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850752, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0134    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 850752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04203203 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | -0.19      |\n",
      "|    learning_rate        | 0.00359    |\n",
      "|    loss                 | 0.076      |\n",
      "|    n_updates            | 4210       |\n",
      "|    policy_gradient_loss | 0.0306     |\n",
      "|    std                  | 0.00434    |\n",
      "|    value_loss           | 4.71e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 1482     |\n",
      "|    total_timesteps | 850752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854784, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0148     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 854784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038801335 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.32        |\n",
      "|    explained_variance   | -0.0177     |\n",
      "|    learning_rate        | 0.00358     |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | 0.0123      |\n",
      "|    std                  | 0.00425     |\n",
      "|    value_loss           | 2.31e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 1489     |\n",
      "|    total_timesteps | 854784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858816, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0228     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 858816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046419844 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.0514     |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 4250        |\n",
      "|    policy_gradient_loss | 0.0316      |\n",
      "|    std                  | 0.00417     |\n",
      "|    value_loss           | 4.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 1496     |\n",
      "|    total_timesteps | 858816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862848, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 862848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017048981 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | -0.176      |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00864     |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | 0.0268      |\n",
      "|    std                  | 0.00415     |\n",
      "|    value_loss           | 3.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 1502     |\n",
      "|    total_timesteps | 862848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0115    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 866880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09085015 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.34       |\n",
      "|    explained_variance   | 0.0211     |\n",
      "|    learning_rate        | 0.00356    |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 4290       |\n",
      "|    policy_gradient_loss | 0.0145     |\n",
      "|    std                  | 0.00428    |\n",
      "|    value_loss           | 8.38e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 1509     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870912, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 870912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019570129 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.32        |\n",
      "|    explained_variance   | -0.15       |\n",
      "|    learning_rate        | 0.00355     |\n",
      "|    loss                 | 0.00979     |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.00427     |\n",
      "|    value_loss           | 7.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 1516     |\n",
      "|    total_timesteps | 870912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874944, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0184     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 874944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033750854 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | -0.166      |\n",
      "|    learning_rate        | 0.00355     |\n",
      "|    loss                 | 0.00263     |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | 0.0169      |\n",
      "|    std                  | 0.00421     |\n",
      "|    value_loss           | 4.4e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 1523     |\n",
      "|    total_timesteps | 874944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878976, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 878976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015186785 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.31        |\n",
      "|    explained_variance   | -0.166      |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | 0.0113      |\n",
      "|    std                  | 0.00426     |\n",
      "|    value_loss           | 4.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 1529     |\n",
      "|    total_timesteps | 878976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883008, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0108    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 883008     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05002481 |\n",
      "|    clip_fraction        | 0.495      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.32       |\n",
      "|    explained_variance   | -0.0536    |\n",
      "|    learning_rate        | 0.00353    |\n",
      "|    loss                 | 0.00574    |\n",
      "|    n_updates            | 4370       |\n",
      "|    policy_gradient_loss | 0.0473     |\n",
      "|    std                  | 0.0043     |\n",
      "|    value_loss           | 7.68e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 1536     |\n",
      "|    total_timesteps | 883008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0121    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 887040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04443152 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.36       |\n",
      "|    explained_variance   | -0.0158    |\n",
      "|    learning_rate        | 0.00352    |\n",
      "|    loss                 | 0.014      |\n",
      "|    n_updates            | 4390       |\n",
      "|    policy_gradient_loss | 0.0242     |\n",
      "|    std                  | 0.00407    |\n",
      "|    value_loss           | 7.63e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 1543     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891072, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0151     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 891072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011644907 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.27        |\n",
      "|    explained_variance   | -0.0623     |\n",
      "|    learning_rate        | 0.00352     |\n",
      "|    loss                 | 0.00619     |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | 0.0159      |\n",
      "|    std                  | 0.00425     |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 1549     |\n",
      "|    total_timesteps | 891072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895104, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00939    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 895104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007916765 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.00351     |\n",
      "|    loss                 | 0.00764     |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    std                  | 0.0042      |\n",
      "|    value_loss           | 1.06e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 1556     |\n",
      "|    total_timesteps | 895104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899136, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0169     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 899136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018014792 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | -0.15       |\n",
      "|    learning_rate        | 0.0035      |\n",
      "|    loss                 | 0.00564     |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | 0.00897     |\n",
      "|    std                  | 0.00411     |\n",
      "|    value_loss           | 6.23e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 1563     |\n",
      "|    total_timesteps | 899136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903168, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0138     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 903168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008339776 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.31        |\n",
      "|    explained_variance   | -0.177      |\n",
      "|    learning_rate        | 0.0035      |\n",
      "|    loss                 | 0.00546     |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | 0.00906     |\n",
      "|    std                  | 0.00414     |\n",
      "|    value_loss           | 3.16e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 1570     |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 907200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017020265 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | -0.272      |\n",
      "|    learning_rate        | 0.00349     |\n",
      "|    loss                 | 0.00263     |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | 0.00816     |\n",
      "|    std                  | 0.00405     |\n",
      "|    value_loss           | 4.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 1576     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911232, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 911232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014123345 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.31        |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.00348     |\n",
      "|    loss                 | 0.0168      |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | 0.0185      |\n",
      "|    std                  | 0.00407     |\n",
      "|    value_loss           | 2.52e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 1583     |\n",
      "|    total_timesteps | 911232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915264, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 915264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018667202 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.34        |\n",
      "|    explained_variance   | -0.0793     |\n",
      "|    learning_rate        | 0.00348     |\n",
      "|    loss                 | 0.0227      |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | 0.00992     |\n",
      "|    std                  | 0.00399     |\n",
      "|    value_loss           | 3.33e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 1590     |\n",
      "|    total_timesteps | 915264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919296, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0101     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 919296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038063582 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | -0.29       |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 4550        |\n",
      "|    policy_gradient_loss | 0.0153      |\n",
      "|    std                  | 0.00407     |\n",
      "|    value_loss           | 1.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 1597     |\n",
      "|    total_timesteps | 919296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=923328, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0138    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 923328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09162401 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.33       |\n",
      "|    explained_variance   | -0.155     |\n",
      "|    learning_rate        | 0.00346    |\n",
      "|    loss                 | 0.0854     |\n",
      "|    n_updates            | 4570       |\n",
      "|    policy_gradient_loss | 0.0385     |\n",
      "|    std                  | 0.00406    |\n",
      "|    value_loss           | 2.75e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 1604     |\n",
      "|    total_timesteps | 923328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031050328 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | -0.163      |\n",
      "|    learning_rate        | 0.00346     |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | 0.0183      |\n",
      "|    std                  | 0.00391     |\n",
      "|    value_loss           | 6.4e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 1610     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931392, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 931392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007870552 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | 0.0112      |\n",
      "|    learning_rate        | 0.00345     |\n",
      "|    loss                 | -0.00295    |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | 0.00838     |\n",
      "|    std                  | 0.00409     |\n",
      "|    value_loss           | 3.54e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 1617     |\n",
      "|    total_timesteps | 931392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=935424, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0176    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 935424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03300806 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.39       |\n",
      "|    explained_variance   | -0.274     |\n",
      "|    learning_rate        | 0.00344    |\n",
      "|    loss                 | 0.00903    |\n",
      "|    n_updates            | 4630       |\n",
      "|    policy_gradient_loss | 0.0279     |\n",
      "|    std                  | 0.004      |\n",
      "|    value_loss           | 5.3e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 1624     |\n",
      "|    total_timesteps | 935424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939456, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 939456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032356232 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | 0.0307      |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | 0.0259      |\n",
      "|    std                  | 0.00394     |\n",
      "|    value_loss           | 7.22e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 1630     |\n",
      "|    total_timesteps | 939456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943488, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0104    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 943488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03477496 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.38       |\n",
      "|    explained_variance   | -0.067     |\n",
      "|    learning_rate        | 0.00343    |\n",
      "|    loss                 | 0.00479    |\n",
      "|    n_updates            | 4670       |\n",
      "|    policy_gradient_loss | 0.0266     |\n",
      "|    std                  | 0.00397    |\n",
      "|    value_loss           | 8.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 1637     |\n",
      "|    total_timesteps | 943488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0116    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 947520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01256595 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.36       |\n",
      "|    explained_variance   | 0.0699     |\n",
      "|    learning_rate        | 0.00342    |\n",
      "|    loss                 | 0.000332   |\n",
      "|    n_updates            | 4690       |\n",
      "|    policy_gradient_loss | 0.0133     |\n",
      "|    std                  | 0.00394    |\n",
      "|    value_loss           | 1.97e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 1644     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951552, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 951552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044678032 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.173      |\n",
      "|    learning_rate        | 0.00342     |\n",
      "|    loss                 | 0.099       |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | 0.0562      |\n",
      "|    std                  | 0.00396     |\n",
      "|    value_loss           | 4.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 1651     |\n",
      "|    total_timesteps | 951552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955584, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 955584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049481213 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | -0.207      |\n",
      "|    learning_rate        | 0.00341     |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    std                  | 0.00392     |\n",
      "|    value_loss           | 4.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 1658     |\n",
      "|    total_timesteps | 955584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959616, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0172    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 959616     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03660333 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.41       |\n",
      "|    explained_variance   | -0.139     |\n",
      "|    learning_rate        | 0.0034     |\n",
      "|    loss                 | -0.00261   |\n",
      "|    n_updates            | 4750       |\n",
      "|    policy_gradient_loss | 0.0179     |\n",
      "|    std                  | 0.00387    |\n",
      "|    value_loss           | 2.92e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 1664     |\n",
      "|    total_timesteps | 959616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963648, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 963648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027985943 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | -0.109      |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | 0.0297      |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | 0.0314      |\n",
      "|    std                  | 0.00396     |\n",
      "|    value_loss           | 3.28e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 1671     |\n",
      "|    total_timesteps | 963648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063595414 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.45        |\n",
      "|    explained_variance   | -0.281      |\n",
      "|    learning_rate        | 0.00339     |\n",
      "|    loss                 | 0.00662     |\n",
      "|    n_updates            | 4790        |\n",
      "|    policy_gradient_loss | 0.029       |\n",
      "|    std                  | 0.00385     |\n",
      "|    value_loss           | 2.44e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 1678     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971712, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 971712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020424858 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.47        |\n",
      "|    explained_variance   | 0.051       |\n",
      "|    learning_rate        | 0.00338     |\n",
      "|    loss                 | 0.00611     |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | 0.0162      |\n",
      "|    std                  | 0.00383     |\n",
      "|    value_loss           | 3.2e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 1685     |\n",
      "|    total_timesteps | 971712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975744, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 975744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014266586 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.46        |\n",
      "|    explained_variance   | -0.192      |\n",
      "|    learning_rate        | 0.00338     |\n",
      "|    loss                 | 0.0181      |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | 0.0206      |\n",
      "|    std                  | 0.00378     |\n",
      "|    value_loss           | 1.56e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 1691     |\n",
      "|    total_timesteps | 975744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979776, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 979776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010193765 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.46        |\n",
      "|    explained_variance   | -0.141      |\n",
      "|    learning_rate        | 0.00337     |\n",
      "|    loss                 | 0.00606     |\n",
      "|    n_updates            | 4850        |\n",
      "|    policy_gradient_loss | 0.0214      |\n",
      "|    std                  | 0.0038      |\n",
      "|    value_loss           | 1.37e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 1698     |\n",
      "|    total_timesteps | 979776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983808, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 983808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082357824 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.41        |\n",
      "|    explained_variance   | -0.181      |\n",
      "|    learning_rate        | 0.00336     |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | 0.0205      |\n",
      "|    std                  | 0.00393     |\n",
      "|    value_loss           | 6.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 1705     |\n",
      "|    total_timesteps | 983808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0139    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 987840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11967317 |\n",
      "|    clip_fraction        | 0.448      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.45       |\n",
      "|    explained_variance   | 0.00505    |\n",
      "|    learning_rate        | 0.00336    |\n",
      "|    loss                 | 0.0192     |\n",
      "|    n_updates            | 4890       |\n",
      "|    policy_gradient_loss | 0.0353     |\n",
      "|    std                  | 0.00381    |\n",
      "|    value_loss           | 4.39e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 490      |\n",
      "|    time_elapsed    | 1711     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991872, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0123    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 991872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09994949 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.49       |\n",
      "|    explained_variance   | 0.152      |\n",
      "|    learning_rate        | 0.00335    |\n",
      "|    loss                 | 0.0178     |\n",
      "|    n_updates            | 4910       |\n",
      "|    policy_gradient_loss | 0.0348     |\n",
      "|    std                  | 0.00368    |\n",
      "|    value_loss           | 9.11e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 492      |\n",
      "|    time_elapsed    | 1718     |\n",
      "|    total_timesteps | 991872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=995904, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 995904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156118 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.51        |\n",
      "|    explained_variance   | -0.0579     |\n",
      "|    learning_rate        | 0.00334     |\n",
      "|    loss                 | 0.00941     |\n",
      "|    n_updates            | 4930        |\n",
      "|    policy_gradient_loss | 0.0305      |\n",
      "|    std                  | 0.00366     |\n",
      "|    value_loss           | 6.12e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 494      |\n",
      "|    time_elapsed    | 1725     |\n",
      "|    total_timesteps | 995904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999936, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0174    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 999936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03493774 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.49       |\n",
      "|    explained_variance   | -0.202     |\n",
      "|    learning_rate        | 0.00334    |\n",
      "|    loss                 | 0.0668     |\n",
      "|    n_updates            | 4950       |\n",
      "|    policy_gradient_loss | 0.0185     |\n",
      "|    std                  | 0.00367    |\n",
      "|    value_loss           | 3.24e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 496      |\n",
      "|    time_elapsed    | 1732     |\n",
      "|    total_timesteps | 999936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1003968, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1003968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007438737 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.5         |\n",
      "|    explained_variance   | -0.113      |\n",
      "|    learning_rate        | 0.00333     |\n",
      "|    loss                 | 0.000688    |\n",
      "|    n_updates            | 4970        |\n",
      "|    policy_gradient_loss | 0.0168      |\n",
      "|    std                  | 0.00366     |\n",
      "|    value_loss           | 3.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 498      |\n",
      "|    time_elapsed    | 1738     |\n",
      "|    total_timesteps | 1003968  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028527366 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.55        |\n",
      "|    explained_variance   | 0.0686      |\n",
      "|    learning_rate        | 0.00332     |\n",
      "|    loss                 | 0.0292      |\n",
      "|    n_updates            | 4990        |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    std                  | 0.00357     |\n",
      "|    value_loss           | 2.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 1745     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1012032, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1012032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011877863 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.58        |\n",
      "|    explained_variance   | -0.151      |\n",
      "|    learning_rate        | 0.00332     |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 5010        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.0035      |\n",
      "|    value_loss           | 5.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 502      |\n",
      "|    time_elapsed    | 1752     |\n",
      "|    total_timesteps | 1012032  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1016064, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0107    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1016064    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03301972 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | -0.136     |\n",
      "|    learning_rate        | 0.00331    |\n",
      "|    loss                 | 0.0404     |\n",
      "|    n_updates            | 5030       |\n",
      "|    policy_gradient_loss | 0.0278     |\n",
      "|    std                  | 0.00336    |\n",
      "|    value_loss           | 1.33e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 504      |\n",
      "|    time_elapsed    | 1758     |\n",
      "|    total_timesteps | 1016064  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020096, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1020096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036321886 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | -0.198      |\n",
      "|    learning_rate        | 0.0033      |\n",
      "|    loss                 | 0.0361      |\n",
      "|    n_updates            | 5050        |\n",
      "|    policy_gradient_loss | 0.0458      |\n",
      "|    std                  | 0.00329     |\n",
      "|    value_loss           | 2.52e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 506      |\n",
      "|    time_elapsed    | 1765     |\n",
      "|    total_timesteps | 1020096  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1024128, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0116   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1024128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0356392 |\n",
      "|    clip_fraction        | 0.312     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 8.67      |\n",
      "|    explained_variance   | 0.16      |\n",
      "|    learning_rate        | 0.0033    |\n",
      "|    loss                 | 0.0238    |\n",
      "|    n_updates            | 5070      |\n",
      "|    policy_gradient_loss | 0.0114    |\n",
      "|    std                  | 0.00326   |\n",
      "|    value_loss           | 1.82e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 508      |\n",
      "|    time_elapsed    | 1772     |\n",
      "|    total_timesteps | 1024128  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.011     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1028160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03039433 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.7        |\n",
      "|    explained_variance   | -0.0303    |\n",
      "|    learning_rate        | 0.00329    |\n",
      "|    loss                 | 0.0594     |\n",
      "|    n_updates            | 5090       |\n",
      "|    policy_gradient_loss | 0.0314     |\n",
      "|    std                  | 0.00326    |\n",
      "|    value_loss           | 2.67e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 510      |\n",
      "|    time_elapsed    | 1779     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1032192, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033896115 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.72        |\n",
      "|    explained_variance   | -0.133      |\n",
      "|    learning_rate        | 0.00328     |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 5110        |\n",
      "|    policy_gradient_loss | 0.0264      |\n",
      "|    std                  | 0.00322     |\n",
      "|    value_loss           | 1.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 512      |\n",
      "|    time_elapsed    | 1785     |\n",
      "|    total_timesteps | 1032192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1036224, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0163     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1036224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015770614 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.73        |\n",
      "|    explained_variance   | -0.125      |\n",
      "|    learning_rate        | 0.00328     |\n",
      "|    loss                 | 0.00872     |\n",
      "|    n_updates            | 5130        |\n",
      "|    policy_gradient_loss | 0.0164      |\n",
      "|    std                  | 0.00324     |\n",
      "|    value_loss           | 3.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 514      |\n",
      "|    time_elapsed    | 1792     |\n",
      "|    total_timesteps | 1036224  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040256, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1040256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056972527 |\n",
      "|    clip_fraction        | 0.448        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.71         |\n",
      "|    explained_variance   | -0.0711      |\n",
      "|    learning_rate        | 0.00327      |\n",
      "|    loss                 | 0.00152      |\n",
      "|    n_updates            | 5150         |\n",
      "|    policy_gradient_loss | 0.0413       |\n",
      "|    std                  | 0.00326      |\n",
      "|    value_loss           | 5.04e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 516      |\n",
      "|    time_elapsed    | 1799     |\n",
      "|    total_timesteps | 1040256  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1044288, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1044288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015669584 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.72        |\n",
      "|    explained_variance   | -0.032      |\n",
      "|    learning_rate        | 0.00326     |\n",
      "|    loss                 | 0.00688     |\n",
      "|    n_updates            | 5170        |\n",
      "|    policy_gradient_loss | 0.0318      |\n",
      "|    std                  | 0.00331     |\n",
      "|    value_loss           | 3.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 518      |\n",
      "|    time_elapsed    | 1806     |\n",
      "|    total_timesteps | 1044288  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0113    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1048320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03425975 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.75       |\n",
      "|    explained_variance   | -0.19      |\n",
      "|    learning_rate        | 0.00326    |\n",
      "|    loss                 | 0.0146     |\n",
      "|    n_updates            | 5190       |\n",
      "|    policy_gradient_loss | 0.0224     |\n",
      "|    std                  | 0.00316    |\n",
      "|    value_loss           | 3.72e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 520      |\n",
      "|    time_elapsed    | 1812     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1052352, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1052352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011881382 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.81        |\n",
      "|    explained_variance   | -0.0773     |\n",
      "|    learning_rate        | 0.00325     |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 5210        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.00315     |\n",
      "|    value_loss           | 3.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 522      |\n",
      "|    time_elapsed    | 1819     |\n",
      "|    total_timesteps | 1052352  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1056384, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1056384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013020046 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.78        |\n",
      "|    explained_variance   | -0.0783     |\n",
      "|    learning_rate        | 0.00324     |\n",
      "|    loss                 | -0.00298    |\n",
      "|    n_updates            | 5230        |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    std                  | 0.00318     |\n",
      "|    value_loss           | 4.12e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 524      |\n",
      "|    time_elapsed    | 1826     |\n",
      "|    total_timesteps | 1056384  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060416, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0114     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1060416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006284314 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.77        |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.00324     |\n",
      "|    loss                 | -0.00411    |\n",
      "|    n_updates            | 5250        |\n",
      "|    policy_gradient_loss | 0.00924     |\n",
      "|    std                  | 0.00315     |\n",
      "|    value_loss           | 2.04e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 526      |\n",
      "|    time_elapsed    | 1833     |\n",
      "|    total_timesteps | 1060416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1064448, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.011     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1064448    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01961796 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.81       |\n",
      "|    explained_variance   | 0.219      |\n",
      "|    learning_rate        | 0.00323    |\n",
      "|    loss                 | 0.021      |\n",
      "|    n_updates            | 5270       |\n",
      "|    policy_gradient_loss | 0.0169     |\n",
      "|    std                  | 0.00307    |\n",
      "|    value_loss           | 6.76e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 1840     |\n",
      "|    total_timesteps | 1064448  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0111    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1068480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05936386 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.8        |\n",
      "|    explained_variance   | -0.146     |\n",
      "|    learning_rate        | 0.00322    |\n",
      "|    loss                 | 0.0423     |\n",
      "|    n_updates            | 5290       |\n",
      "|    policy_gradient_loss | 0.0333     |\n",
      "|    std                  | 0.00316    |\n",
      "|    value_loss           | 3.59e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 530      |\n",
      "|    time_elapsed    | 1846     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1072512, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1072512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032980975 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.76        |\n",
      "|    explained_variance   | 0.0208      |\n",
      "|    learning_rate        | 0.00322     |\n",
      "|    loss                 | 0.00528     |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | 0.0197      |\n",
      "|    std                  | 0.00319     |\n",
      "|    value_loss           | 5.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 532      |\n",
      "|    time_elapsed    | 1853     |\n",
      "|    total_timesteps | 1072512  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1076544, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0132    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1076544    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04250237 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.8        |\n",
      "|    explained_variance   | 0.0654     |\n",
      "|    learning_rate        | 0.00321    |\n",
      "|    loss                 | -0.00572   |\n",
      "|    n_updates            | 5330       |\n",
      "|    policy_gradient_loss | 0.014      |\n",
      "|    std                  | 0.00315    |\n",
      "|    value_loss           | 4.95e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 534      |\n",
      "|    time_elapsed    | 1861     |\n",
      "|    total_timesteps | 1076544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080576, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.017     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1080576    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06660322 |\n",
      "|    clip_fraction        | 0.419      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | -0.194     |\n",
      "|    learning_rate        | 0.0032     |\n",
      "|    loss                 | 0.0187     |\n",
      "|    n_updates            | 5350       |\n",
      "|    policy_gradient_loss | 0.035      |\n",
      "|    std                  | 0.00308    |\n",
      "|    value_loss           | 2.4e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 536      |\n",
      "|    time_elapsed    | 1867     |\n",
      "|    total_timesteps | 1080576  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1084608, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0126    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1084608    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01963481 |\n",
      "|    clip_fraction        | 0.26       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | 0.101      |\n",
      "|    learning_rate        | 0.0032     |\n",
      "|    loss                 | -0.000858  |\n",
      "|    n_updates            | 5370       |\n",
      "|    policy_gradient_loss | 0.0125     |\n",
      "|    std                  | 0.00307    |\n",
      "|    value_loss           | 1.48e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 538      |\n",
      "|    time_elapsed    | 1874     |\n",
      "|    total_timesteps | 1084608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0186     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024419151 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.79        |\n",
      "|    explained_variance   | -0.185      |\n",
      "|    learning_rate        | 0.00319     |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 5390        |\n",
      "|    policy_gradient_loss | 0.0153      |\n",
      "|    std                  | 0.0032      |\n",
      "|    value_loss           | 8.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 540      |\n",
      "|    time_elapsed    | 1882     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1092672, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1092672     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024198107 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.76        |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.00318     |\n",
      "|    loss                 | -6.18e-05   |\n",
      "|    n_updates            | 5410        |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    std                  | 0.00319     |\n",
      "|    value_loss           | 8.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 542      |\n",
      "|    time_elapsed    | 1889     |\n",
      "|    total_timesteps | 1092672  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1096704, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1096704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018323189 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.8         |\n",
      "|    explained_variance   | 0.0258      |\n",
      "|    learning_rate        | 0.00318     |\n",
      "|    loss                 | -0.00135    |\n",
      "|    n_updates            | 5430        |\n",
      "|    policy_gradient_loss | 0.00824     |\n",
      "|    std                  | 0.00319     |\n",
      "|    value_loss           | 1.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 544      |\n",
      "|    time_elapsed    | 1896     |\n",
      "|    total_timesteps | 1096704  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100736, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1100736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060605876 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.73        |\n",
      "|    explained_variance   | 0.0104      |\n",
      "|    learning_rate        | 0.00317     |\n",
      "|    loss                 | 0.00782     |\n",
      "|    n_updates            | 5450        |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    std                  | 0.00327     |\n",
      "|    value_loss           | 4.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 546      |\n",
      "|    time_elapsed    | 1903     |\n",
      "|    total_timesteps | 1100736  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1104768, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1104768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031771418 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.71        |\n",
      "|    explained_variance   | -0.0425     |\n",
      "|    learning_rate        | 0.00316     |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 5470        |\n",
      "|    policy_gradient_loss | 0.0196      |\n",
      "|    std                  | 0.00332     |\n",
      "|    value_loss           | 2.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 548      |\n",
      "|    time_elapsed    | 1910     |\n",
      "|    total_timesteps | 1104768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0121    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1108800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03742921 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.69       |\n",
      "|    explained_variance   | -0.238     |\n",
      "|    learning_rate        | 0.00316    |\n",
      "|    loss                 | 0.0459     |\n",
      "|    n_updates            | 5490       |\n",
      "|    policy_gradient_loss | 0.0245     |\n",
      "|    std                  | 0.00342    |\n",
      "|    value_loss           | 3.06e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 550      |\n",
      "|    time_elapsed    | 1916     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1112832, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.016     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1112832    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03203108 |\n",
      "|    clip_fraction        | 0.419      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.6        |\n",
      "|    explained_variance   | -0.138     |\n",
      "|    learning_rate        | 0.00315    |\n",
      "|    loss                 | 0.019      |\n",
      "|    n_updates            | 5510       |\n",
      "|    policy_gradient_loss | 0.0349     |\n",
      "|    std                  | 0.00353    |\n",
      "|    value_loss           | 5.14e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 552      |\n",
      "|    time_elapsed    | 1924     |\n",
      "|    total_timesteps | 1112832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1116864, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1116864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043366194 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.58        |\n",
      "|    explained_variance   | -0.173      |\n",
      "|    learning_rate        | 0.00314     |\n",
      "|    loss                 | -0.00202    |\n",
      "|    n_updates            | 5530        |\n",
      "|    policy_gradient_loss | 0.0457      |\n",
      "|    std                  | 0.00358     |\n",
      "|    value_loss           | 7.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 554      |\n",
      "|    time_elapsed    | 1930     |\n",
      "|    total_timesteps | 1116864  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120896, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021978078 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.56        |\n",
      "|    explained_variance   | -0.119      |\n",
      "|    learning_rate        | 0.00314     |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 5550        |\n",
      "|    policy_gradient_loss | 0.00818     |\n",
      "|    std                  | 0.00359     |\n",
      "|    value_loss           | 5.56e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 556      |\n",
      "|    time_elapsed    | 1937     |\n",
      "|    total_timesteps | 1120896  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1124928, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0113    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1124928    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04183925 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.62       |\n",
      "|    explained_variance   | -0.0925    |\n",
      "|    learning_rate        | 0.00313    |\n",
      "|    loss                 | 0.0574     |\n",
      "|    n_updates            | 5570       |\n",
      "|    policy_gradient_loss | 0.0274     |\n",
      "|    std                  | 0.00348    |\n",
      "|    value_loss           | 6.39e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 558      |\n",
      "|    time_elapsed    | 1944     |\n",
      "|    total_timesteps | 1124928  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044596624 |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.61        |\n",
      "|    explained_variance   | -0.0616     |\n",
      "|    learning_rate        | 0.00312     |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 5590        |\n",
      "|    policy_gradient_loss | 0.0396      |\n",
      "|    std                  | 0.00352     |\n",
      "|    value_loss           | 4.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 560      |\n",
      "|    time_elapsed    | 1950     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1132992, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1132992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07161407 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | 0.177      |\n",
      "|    learning_rate        | 0.00312    |\n",
      "|    loss                 | 0.0495     |\n",
      "|    n_updates            | 5610       |\n",
      "|    policy_gradient_loss | 0.0114     |\n",
      "|    std                  | 0.00351    |\n",
      "|    value_loss           | 3.23e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 562      |\n",
      "|    time_elapsed    | 1957     |\n",
      "|    total_timesteps | 1132992  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1137024, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1137024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015451847 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.55        |\n",
      "|    explained_variance   | -0.0252     |\n",
      "|    learning_rate        | 0.00311     |\n",
      "|    loss                 | 0.00605     |\n",
      "|    n_updates            | 5630        |\n",
      "|    policy_gradient_loss | 0.0313      |\n",
      "|    std                  | 0.00356     |\n",
      "|    value_loss           | 6.23e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 564      |\n",
      "|    time_elapsed    | 1964     |\n",
      "|    total_timesteps | 1137024  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1141056, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1141056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014506403 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.6         |\n",
      "|    explained_variance   | -0.0654     |\n",
      "|    learning_rate        | 0.0031      |\n",
      "|    loss                 | 0.0221      |\n",
      "|    n_updates            | 5650        |\n",
      "|    policy_gradient_loss | 0.0125      |\n",
      "|    std                  | 0.00347     |\n",
      "|    value_loss           | 6.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 566      |\n",
      "|    time_elapsed    | 1971     |\n",
      "|    total_timesteps | 1141056  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1145088, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00908    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1145088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037252124 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.64        |\n",
      "|    explained_variance   | -0.0994     |\n",
      "|    learning_rate        | 0.00309     |\n",
      "|    loss                 | 0.00242     |\n",
      "|    n_updates            | 5670        |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.00334     |\n",
      "|    value_loss           | 4.16e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 568      |\n",
      "|    time_elapsed    | 1978     |\n",
      "|    total_timesteps | 1145088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0138    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1149120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06347031 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.73       |\n",
      "|    explained_variance   | -0.0837    |\n",
      "|    learning_rate        | 0.00309    |\n",
      "|    loss                 | 0.00506    |\n",
      "|    n_updates            | 5690       |\n",
      "|    policy_gradient_loss | 0.00741    |\n",
      "|    std                  | 0.0032     |\n",
      "|    value_loss           | 3.73e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 570      |\n",
      "|    time_elapsed    | 1985     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1153152, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00949   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1153152    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05106549 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.76       |\n",
      "|    explained_variance   | -0.0554    |\n",
      "|    learning_rate        | 0.00308    |\n",
      "|    loss                 | 0.0521     |\n",
      "|    n_updates            | 5710       |\n",
      "|    policy_gradient_loss | 0.0189     |\n",
      "|    std                  | 0.00321    |\n",
      "|    value_loss           | 6.67e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 572      |\n",
      "|    time_elapsed    | 1992     |\n",
      "|    total_timesteps | 1153152  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1157184, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1157184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011253802 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.73        |\n",
      "|    explained_variance   | -0.0686     |\n",
      "|    learning_rate        | 0.00307     |\n",
      "|    loss                 | -0.00544    |\n",
      "|    n_updates            | 5730        |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    std                  | 0.00324     |\n",
      "|    value_loss           | 2.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 574      |\n",
      "|    time_elapsed    | 1998     |\n",
      "|    total_timesteps | 1157184  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1161216, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1161216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016552098 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | -0.0838     |\n",
      "|    learning_rate        | 0.00307     |\n",
      "|    loss                 | 0.00989     |\n",
      "|    n_updates            | 5750        |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    std                  | 0.00324     |\n",
      "|    value_loss           | 4.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 576      |\n",
      "|    time_elapsed    | 2005     |\n",
      "|    total_timesteps | 1161216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1165248, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1165248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035852212 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00306     |\n",
      "|    loss                 | -0.00357    |\n",
      "|    n_updates            | 5770        |\n",
      "|    policy_gradient_loss | 0.0185      |\n",
      "|    std                  | 0.00336     |\n",
      "|    value_loss           | 7.01e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 578      |\n",
      "|    time_elapsed    | 2012     |\n",
      "|    total_timesteps | 1165248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1169280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016150892 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | -0.0417     |\n",
      "|    learning_rate        | 0.00305     |\n",
      "|    loss                 | 0.00492     |\n",
      "|    n_updates            | 5790        |\n",
      "|    policy_gradient_loss | 0.00788     |\n",
      "|    std                  | 0.00337     |\n",
      "|    value_loss           | 3.24e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 580      |\n",
      "|    time_elapsed    | 2019     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1173312, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1173312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028548501 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | -0.0491     |\n",
      "|    learning_rate        | 0.00305     |\n",
      "|    loss                 | -0.000206   |\n",
      "|    n_updates            | 5810        |\n",
      "|    policy_gradient_loss | 0.00951     |\n",
      "|    std                  | 0.00346     |\n",
      "|    value_loss           | 4.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 582      |\n",
      "|    time_elapsed    | 2025     |\n",
      "|    total_timesteps | 1173312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1177344, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1177344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015219183 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.63        |\n",
      "|    explained_variance   | -0.0829     |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | -0.0024     |\n",
      "|    n_updates            | 5830        |\n",
      "|    policy_gradient_loss | 0.0273      |\n",
      "|    std                  | 0.00348     |\n",
      "|    value_loss           | 2.09e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 584      |\n",
      "|    time_elapsed    | 2032     |\n",
      "|    total_timesteps | 1177344  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1181376, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0128     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1181376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027951801 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | -0.0867     |\n",
      "|    learning_rate        | 0.00303     |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 5850        |\n",
      "|    policy_gradient_loss | 0.0243      |\n",
      "|    std                  | 0.0034      |\n",
      "|    value_loss           | 2.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 586      |\n",
      "|    time_elapsed    | 2039     |\n",
      "|    total_timesteps | 1181376  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1185408, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1185408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020620167 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.73        |\n",
      "|    explained_variance   | -0.0638     |\n",
      "|    learning_rate        | 0.00303     |\n",
      "|    loss                 | -0.00419    |\n",
      "|    n_updates            | 5870        |\n",
      "|    policy_gradient_loss | 0.00947     |\n",
      "|    std                  | 0.00326     |\n",
      "|    value_loss           | 5.17e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 588      |\n",
      "|    time_elapsed    | 2045     |\n",
      "|    total_timesteps | 1185408  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0167     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1189440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012343739 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.77        |\n",
      "|    explained_variance   | -0.0379     |\n",
      "|    learning_rate        | 0.00302     |\n",
      "|    loss                 | -0.00231    |\n",
      "|    n_updates            | 5890        |\n",
      "|    policy_gradient_loss | 0.0212      |\n",
      "|    std                  | 0.00326     |\n",
      "|    value_loss           | 4e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 590      |\n",
      "|    time_elapsed    | 2052     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1193472, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1193472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028552966 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.72        |\n",
      "|    explained_variance   | -0.0522     |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0198      |\n",
      "|    n_updates            | 5910        |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    std                  | 0.00337     |\n",
      "|    value_loss           | 2.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 592      |\n",
      "|    time_elapsed    | 2059     |\n",
      "|    total_timesteps | 1193472  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1197504, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0101     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1197504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033454653 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | -0.054      |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0891      |\n",
      "|    n_updates            | 5930        |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.0033      |\n",
      "|    value_loss           | 4.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 594      |\n",
      "|    time_elapsed    | 2066     |\n",
      "|    total_timesteps | 1197504  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1201536, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0101    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1201536    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08122295 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.72       |\n",
      "|    explained_variance   | -0.0245    |\n",
      "|    learning_rate        | 0.003      |\n",
      "|    loss                 | 0.122      |\n",
      "|    n_updates            | 5950       |\n",
      "|    policy_gradient_loss | 0.021      |\n",
      "|    std                  | 0.00335    |\n",
      "|    value_loss           | 1.01e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 596      |\n",
      "|    time_elapsed    | 2072     |\n",
      "|    total_timesteps | 1201536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1205568, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0184     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1205568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035235588 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.67        |\n",
      "|    explained_variance   | -0.088      |\n",
      "|    learning_rate        | 0.00299     |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 5970        |\n",
      "|    policy_gradient_loss | 0.0459      |\n",
      "|    std                  | 0.00343     |\n",
      "|    value_loss           | 2.96e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 598      |\n",
      "|    time_elapsed    | 2079     |\n",
      "|    total_timesteps | 1205568  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1209600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03584832 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.6        |\n",
      "|    explained_variance   | 0.198      |\n",
      "|    learning_rate        | 0.00299    |\n",
      "|    loss                 | 0.0207     |\n",
      "|    n_updates            | 5990       |\n",
      "|    policy_gradient_loss | 0.0207     |\n",
      "|    std                  | 0.00361    |\n",
      "|    value_loss           | 1.05e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 2086     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1213632, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1213632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028740998 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.57        |\n",
      "|    explained_variance   | -0.0583     |\n",
      "|    learning_rate        | 0.00298     |\n",
      "|    loss                 | 0.00946     |\n",
      "|    n_updates            | 6010        |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    std                  | 0.00368     |\n",
      "|    value_loss           | 9.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 602      |\n",
      "|    time_elapsed    | 2093     |\n",
      "|    total_timesteps | 1213632  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1217664, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1217664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027876489 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.61        |\n",
      "|    explained_variance   | -0.0299     |\n",
      "|    learning_rate        | 0.00297     |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 6030        |\n",
      "|    policy_gradient_loss | 0.0256      |\n",
      "|    std                  | 0.00352     |\n",
      "|    value_loss           | 2.53e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 604      |\n",
      "|    time_elapsed    | 2099     |\n",
      "|    total_timesteps | 1217664  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1221696, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0193    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1221696    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14344999 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.68       |\n",
      "|    explained_variance   | 0.0365     |\n",
      "|    learning_rate        | 0.00297    |\n",
      "|    loss                 | 0.00724    |\n",
      "|    n_updates            | 6050       |\n",
      "|    policy_gradient_loss | 0.0154     |\n",
      "|    std                  | 0.0034     |\n",
      "|    value_loss           | 7.52e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 606      |\n",
      "|    time_elapsed    | 2106     |\n",
      "|    total_timesteps | 1221696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1225728, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1225728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042933397 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | -0.0158     |\n",
      "|    learning_rate        | 0.00296     |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 6070        |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.00345     |\n",
      "|    value_loss           | 8.73e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 608      |\n",
      "|    time_elapsed    | 2113     |\n",
      "|    total_timesteps | 1225728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0108    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1229760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11773188 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.65       |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.00295    |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 6090       |\n",
      "|    policy_gradient_loss | 0.0173     |\n",
      "|    std                  | 0.00339    |\n",
      "|    value_loss           | 1.13e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 610      |\n",
      "|    time_elapsed    | 2119     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1233792, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1233792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031081479 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | -0.133      |\n",
      "|    learning_rate        | 0.00295     |\n",
      "|    loss                 | 0.0251      |\n",
      "|    n_updates            | 6110        |\n",
      "|    policy_gradient_loss | 0.0141      |\n",
      "|    std                  | 0.00339     |\n",
      "|    value_loss           | 1.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 612      |\n",
      "|    time_elapsed    | 2126     |\n",
      "|    total_timesteps | 1233792  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1237824, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.01        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1237824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034307893 |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.63         |\n",
      "|    explained_variance   | -0.0378      |\n",
      "|    learning_rate        | 0.00294      |\n",
      "|    loss                 | 0.00405      |\n",
      "|    n_updates            | 6130         |\n",
      "|    policy_gradient_loss | 0.00623      |\n",
      "|    std                  | 0.0034       |\n",
      "|    value_loss           | 2.31e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 614      |\n",
      "|    time_elapsed    | 2133     |\n",
      "|    total_timesteps | 1237824  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1241856, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1241856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043832067 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | 0.0957      |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 6150        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.00338     |\n",
      "|    value_loss           | 1.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 2140     |\n",
      "|    total_timesteps | 1241856  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1245888, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1245888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043345742 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.64        |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 6170        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.00339     |\n",
      "|    value_loss           | 3.96e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 618      |\n",
      "|    time_elapsed    | 2147     |\n",
      "|    total_timesteps | 1245888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024904655 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.6         |\n",
      "|    explained_variance   | -0.0927     |\n",
      "|    learning_rate        | 0.00292     |\n",
      "|    loss                 | 0.00848     |\n",
      "|    n_updates            | 6190        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.00341     |\n",
      "|    value_loss           | 4.39e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 620      |\n",
      "|    time_elapsed    | 2154     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1253952, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1253952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009318788 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.6         |\n",
      "|    explained_variance   | -0.0425     |\n",
      "|    learning_rate        | 0.00291     |\n",
      "|    loss                 | 0.00286     |\n",
      "|    n_updates            | 6210        |\n",
      "|    policy_gradient_loss | 0.00749     |\n",
      "|    std                  | 0.00344     |\n",
      "|    value_loss           | 3.06e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 622      |\n",
      "|    time_elapsed    | 2161     |\n",
      "|    total_timesteps | 1253952  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1257984, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0121    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1257984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02063557 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.59       |\n",
      "|    explained_variance   | -0.0191    |\n",
      "|    learning_rate        | 0.00291    |\n",
      "|    loss                 | 0.0121     |\n",
      "|    n_updates            | 6230       |\n",
      "|    policy_gradient_loss | 0.0451     |\n",
      "|    std                  | 0.00348    |\n",
      "|    value_loss           | 2.12e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 624      |\n",
      "|    time_elapsed    | 2168     |\n",
      "|    total_timesteps | 1257984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1262016, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0101      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1262016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060694693 |\n",
      "|    clip_fraction        | 0.315        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.57         |\n",
      "|    explained_variance   | -0.0308      |\n",
      "|    learning_rate        | 0.0029       |\n",
      "|    loss                 | 0.000528     |\n",
      "|    n_updates            | 6250         |\n",
      "|    policy_gradient_loss | 0.026        |\n",
      "|    std                  | 0.00347      |\n",
      "|    value_loss           | 4.92e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 626      |\n",
      "|    time_elapsed    | 2175     |\n",
      "|    total_timesteps | 1262016  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1266048, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0172     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1266048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025010781 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | -0.0367     |\n",
      "|    learning_rate        | 0.00289     |\n",
      "|    loss                 | -0.000368   |\n",
      "|    n_updates            | 6270        |\n",
      "|    policy_gradient_loss | 0.0083      |\n",
      "|    std                  | 0.00333     |\n",
      "|    value_loss           | 5.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 628      |\n",
      "|    time_elapsed    | 2182     |\n",
      "|    total_timesteps | 1266048  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0109    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1270080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08582915 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.68       |\n",
      "|    explained_variance   | -0.0621    |\n",
      "|    learning_rate        | 0.00289    |\n",
      "|    loss                 | 0.0229     |\n",
      "|    n_updates            | 6290       |\n",
      "|    policy_gradient_loss | 0.0221     |\n",
      "|    std                  | 0.00332    |\n",
      "|    value_loss           | 2.01e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 630      |\n",
      "|    time_elapsed    | 2189     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1274112, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1274112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027837794 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | -0.0298     |\n",
      "|    learning_rate        | 0.00288     |\n",
      "|    loss                 | 0.0206      |\n",
      "|    n_updates            | 6310        |\n",
      "|    policy_gradient_loss | 0.0246      |\n",
      "|    std                  | 0.00334     |\n",
      "|    value_loss           | 8.34e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 632      |\n",
      "|    time_elapsed    | 2196     |\n",
      "|    total_timesteps | 1274112  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1278144, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0175     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1278144     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011911139 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.68        |\n",
      "|    explained_variance   | -0.0566     |\n",
      "|    learning_rate        | 0.00287     |\n",
      "|    loss                 | 0.00217     |\n",
      "|    n_updates            | 6330        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00335     |\n",
      "|    value_loss           | 6.22e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 634      |\n",
      "|    time_elapsed    | 2202     |\n",
      "|    total_timesteps | 1278144  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1282176, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1282176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014007538 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | -0.0419     |\n",
      "|    learning_rate        | 0.00287     |\n",
      "|    loss                 | 0.0205      |\n",
      "|    n_updates            | 6350        |\n",
      "|    policy_gradient_loss | 0.0173      |\n",
      "|    std                  | 0.0033      |\n",
      "|    value_loss           | 7.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 636      |\n",
      "|    time_elapsed    | 2209     |\n",
      "|    total_timesteps | 1282176  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1286208, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1286208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042256698 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | 0.296       |\n",
      "|    learning_rate        | 0.00286     |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 6370        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.00334     |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 638      |\n",
      "|    time_elapsed    | 2216     |\n",
      "|    total_timesteps | 1286208  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0106     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026645353 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | -0.045      |\n",
      "|    learning_rate        | 0.00285     |\n",
      "|    loss                 | -0.000473   |\n",
      "|    n_updates            | 6390        |\n",
      "|    policy_gradient_loss | 0.0205      |\n",
      "|    std                  | 0.00334     |\n",
      "|    value_loss           | 3.79e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 640      |\n",
      "|    time_elapsed    | 2223     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1294272, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1294272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023786996 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.64        |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.00285     |\n",
      "|    loss                 | 0.00921     |\n",
      "|    n_updates            | 6410        |\n",
      "|    policy_gradient_loss | 0.0322      |\n",
      "|    std                  | 0.00335     |\n",
      "|    value_loss           | 2.88e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 642      |\n",
      "|    time_elapsed    | 2229     |\n",
      "|    total_timesteps | 1294272  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1298304, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0107     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1298304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026038209 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.64        |\n",
      "|    explained_variance   | 0.0749      |\n",
      "|    learning_rate        | 0.00284     |\n",
      "|    loss                 | 0.000797    |\n",
      "|    n_updates            | 6430        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.00335     |\n",
      "|    value_loss           | 6.36e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 644      |\n",
      "|    time_elapsed    | 2236     |\n",
      "|    total_timesteps | 1298304  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1302336, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1302336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059215702 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | -0.0472     |\n",
      "|    learning_rate        | 0.00283     |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 6450        |\n",
      "|    policy_gradient_loss | 0.0227      |\n",
      "|    std                  | 0.00332     |\n",
      "|    value_loss           | 4.39e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 646      |\n",
      "|    time_elapsed    | 2243     |\n",
      "|    total_timesteps | 1302336  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1306368, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.00889     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1306368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018369611 |\n",
      "|    clip_fraction        | 0.23         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.66         |\n",
      "|    explained_variance   | -0.0988      |\n",
      "|    learning_rate        | 0.00283      |\n",
      "|    loss                 | 0.00055      |\n",
      "|    n_updates            | 6470         |\n",
      "|    policy_gradient_loss | 0.0117       |\n",
      "|    std                  | 0.00335      |\n",
      "|    value_loss           | 6.35e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 648      |\n",
      "|    time_elapsed    | 2250     |\n",
      "|    total_timesteps | 1306368  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044324964 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.64        |\n",
      "|    explained_variance   | -0.0686     |\n",
      "|    learning_rate        | 0.00282     |\n",
      "|    loss                 | -0.00298    |\n",
      "|    n_updates            | 6490        |\n",
      "|    policy_gradient_loss | 0.0327      |\n",
      "|    std                  | 0.00336     |\n",
      "|    value_loss           | 1.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 650      |\n",
      "|    time_elapsed    | 2257     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1314432, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0105      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1314432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027357147 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.65         |\n",
      "|    explained_variance   | -0.0508      |\n",
      "|    learning_rate        | 0.00281      |\n",
      "|    loss                 | 7.41e-05     |\n",
      "|    n_updates            | 6510         |\n",
      "|    policy_gradient_loss | 0.00832      |\n",
      "|    std                  | 0.00333      |\n",
      "|    value_loss           | 4.17e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 652      |\n",
      "|    time_elapsed    | 2263     |\n",
      "|    total_timesteps | 1314432  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1318464, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00984    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1318464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016344443 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | -0.084      |\n",
      "|    learning_rate        | 0.00281     |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 6530        |\n",
      "|    policy_gradient_loss | 0.00886     |\n",
      "|    std                  | 0.00338     |\n",
      "|    value_loss           | 4.98e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 654      |\n",
      "|    time_elapsed    | 2270     |\n",
      "|    total_timesteps | 1318464  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1322496, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1322496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012235306 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.67        |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0028      |\n",
      "|    loss                 | 0.00652     |\n",
      "|    n_updates            | 6550        |\n",
      "|    policy_gradient_loss | 0.00955     |\n",
      "|    std                  | 0.00335     |\n",
      "|    value_loss           | 7.47e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 656      |\n",
      "|    time_elapsed    | 2277     |\n",
      "|    total_timesteps | 1322496  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1326528, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1326528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013893758 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.68        |\n",
      "|    explained_variance   | -0.162      |\n",
      "|    learning_rate        | 0.00279     |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 6570        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00331     |\n",
      "|    value_loss           | 3.56e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 658      |\n",
      "|    time_elapsed    | 2284     |\n",
      "|    total_timesteps | 1326528  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0196     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018178038 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | -0.0884     |\n",
      "|    learning_rate        | 0.00279     |\n",
      "|    loss                 | 0.000965    |\n",
      "|    n_updates            | 6590        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.00327     |\n",
      "|    value_loss           | 1.81e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 660      |\n",
      "|    time_elapsed    | 2291     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1334592, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00974    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1334592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007897853 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.71        |\n",
      "|    explained_variance   | -0.021      |\n",
      "|    learning_rate        | 0.00278     |\n",
      "|    loss                 | 0.00705     |\n",
      "|    n_updates            | 6610        |\n",
      "|    policy_gradient_loss | 0.0341      |\n",
      "|    std                  | 0.0033      |\n",
      "|    value_loss           | 6.23e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 662      |\n",
      "|    time_elapsed    | 2298     |\n",
      "|    total_timesteps | 1334592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1338624, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1338624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029668573 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.68        |\n",
      "|    explained_variance   | 0.00899     |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 6630        |\n",
      "|    policy_gradient_loss | 0.00867     |\n",
      "|    std                  | 0.00325     |\n",
      "|    value_loss           | 5.38e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 664      |\n",
      "|    time_elapsed    | 2304     |\n",
      "|    total_timesteps | 1338624  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1342656, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00972    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1342656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058539625 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.72        |\n",
      "|    explained_variance   | -0.053      |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 6650        |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    std                  | 0.00325     |\n",
      "|    value_loss           | 3.6e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 666      |\n",
      "|    time_elapsed    | 2311     |\n",
      "|    total_timesteps | 1342656  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1346688, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1346688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012250069 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.75        |\n",
      "|    explained_variance   | -0.0397     |\n",
      "|    learning_rate        | 0.00276     |\n",
      "|    loss                 | -0.00203    |\n",
      "|    n_updates            | 6670        |\n",
      "|    policy_gradient_loss | 0.0125      |\n",
      "|    std                  | 0.00317     |\n",
      "|    value_loss           | 2.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 668      |\n",
      "|    time_elapsed    | 2318     |\n",
      "|    total_timesteps | 1346688  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00997   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1350720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04488974 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.8        |\n",
      "|    explained_variance   | -0.0396    |\n",
      "|    learning_rate        | 0.00275    |\n",
      "|    loss                 | -0.00874   |\n",
      "|    n_updates            | 6690       |\n",
      "|    policy_gradient_loss | 0.0217     |\n",
      "|    std                  | 0.00315    |\n",
      "|    value_loss           | 4.33e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 670      |\n",
      "|    time_elapsed    | 2325     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1354752, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0192    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1354752    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08547717 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.79       |\n",
      "|    explained_variance   | 0.191      |\n",
      "|    learning_rate        | 0.00275    |\n",
      "|    loss                 | 0.0542     |\n",
      "|    n_updates            | 6710       |\n",
      "|    policy_gradient_loss | 0.0464     |\n",
      "|    std                  | 0.00317    |\n",
      "|    value_loss           | 1.52e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 672      |\n",
      "|    time_elapsed    | 2332     |\n",
      "|    total_timesteps | 1354752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1358784, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1358784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042371977 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.83        |\n",
      "|    explained_variance   | -0.0222     |\n",
      "|    learning_rate        | 0.00274     |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 6730        |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    std                  | 0.00312     |\n",
      "|    value_loss           | 1.01e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 674      |\n",
      "|    time_elapsed    | 2339     |\n",
      "|    total_timesteps | 1358784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1362816, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1362816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010405143 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.83        |\n",
      "|    explained_variance   | -0.0227     |\n",
      "|    learning_rate        | 0.00273     |\n",
      "|    loss                 | 0.000205    |\n",
      "|    n_updates            | 6750        |\n",
      "|    policy_gradient_loss | 0.00994     |\n",
      "|    std                  | 0.0031      |\n",
      "|    value_loss           | 3.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 676      |\n",
      "|    time_elapsed    | 2345     |\n",
      "|    total_timesteps | 1362816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1366848, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0147     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1366848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043575887 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.81        |\n",
      "|    explained_variance   | -0.036      |\n",
      "|    learning_rate        | 0.00273     |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 6770        |\n",
      "|    policy_gradient_loss | 0.0199      |\n",
      "|    std                  | 0.00312     |\n",
      "|    value_loss           | 6.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 678      |\n",
      "|    time_elapsed    | 2352     |\n",
      "|    total_timesteps | 1366848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1370880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05716352 |\n",
      "|    clip_fraction        | 0.427      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.82       |\n",
      "|    explained_variance   | -0.0652    |\n",
      "|    learning_rate        | 0.00272    |\n",
      "|    loss                 | 0.0865     |\n",
      "|    n_updates            | 6790       |\n",
      "|    policy_gradient_loss | 0.0356     |\n",
      "|    std                  | 0.00309    |\n",
      "|    value_loss           | 2.47e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 680      |\n",
      "|    time_elapsed    | 2359     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1374912, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0158     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1374912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029390475 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.86        |\n",
      "|    explained_variance   | -0.0326     |\n",
      "|    learning_rate        | 0.00271     |\n",
      "|    loss                 | 0.0299      |\n",
      "|    n_updates            | 6810        |\n",
      "|    policy_gradient_loss | 0.00859     |\n",
      "|    std                  | 0.00298     |\n",
      "|    value_loss           | 5.13e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 682      |\n",
      "|    time_elapsed    | 2366     |\n",
      "|    total_timesteps | 1374912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1378944, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0172    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1378944    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02998185 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.92       |\n",
      "|    explained_variance   | -0.0267    |\n",
      "|    learning_rate        | 0.00271    |\n",
      "|    loss                 | 0.0537     |\n",
      "|    n_updates            | 6830       |\n",
      "|    policy_gradient_loss | 0.00797    |\n",
      "|    std                  | 0.00292    |\n",
      "|    value_loss           | 4.07e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 684      |\n",
      "|    time_elapsed    | 2372     |\n",
      "|    total_timesteps | 1378944  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1382976, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.011    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1382976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0961989 |\n",
      "|    clip_fraction        | 0.355     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 8.93      |\n",
      "|    explained_variance   | 0.0711    |\n",
      "|    learning_rate        | 0.0027    |\n",
      "|    loss                 | 0.000593  |\n",
      "|    n_updates            | 6850      |\n",
      "|    policy_gradient_loss | 0.0204    |\n",
      "|    std                  | 0.00288   |\n",
      "|    value_loss           | 3.25e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 686      |\n",
      "|    time_elapsed    | 2379     |\n",
      "|    total_timesteps | 1382976  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1387008, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0135     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1387008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029464327 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.97        |\n",
      "|    explained_variance   | -0.0629     |\n",
      "|    learning_rate        | 0.00269     |\n",
      "|    loss                 | 0.0644      |\n",
      "|    n_updates            | 6870        |\n",
      "|    policy_gradient_loss | 0.0219      |\n",
      "|    std                  | 0.00285     |\n",
      "|    value_loss           | 8.36e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 688      |\n",
      "|    time_elapsed    | 2386     |\n",
      "|    total_timesteps | 1387008  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030889025 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.95        |\n",
      "|    explained_variance   | -0.0393     |\n",
      "|    learning_rate        | 0.00268     |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 6890        |\n",
      "|    policy_gradient_loss | 0.0249      |\n",
      "|    std                  | 0.00288     |\n",
      "|    value_loss           | 4.26e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 690      |\n",
      "|    time_elapsed    | 2392     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1395072, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1395072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086453175 |\n",
      "|    clip_fraction        | 0.296        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.95         |\n",
      "|    explained_variance   | -0.0299      |\n",
      "|    learning_rate        | 0.00268      |\n",
      "|    loss                 | -0.00104     |\n",
      "|    n_updates            | 6910         |\n",
      "|    policy_gradient_loss | 0.0155       |\n",
      "|    std                  | 0.00289      |\n",
      "|    value_loss           | 2.73e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 692      |\n",
      "|    time_elapsed    | 2399     |\n",
      "|    total_timesteps | 1395072  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1399104, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0132      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1399104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142485015 |\n",
      "|    clip_fraction        | 0.5          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.96         |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.00267      |\n",
      "|    loss                 | 0.0118       |\n",
      "|    n_updates            | 6930         |\n",
      "|    policy_gradient_loss | 0.0328       |\n",
      "|    std                  | 0.00289      |\n",
      "|    value_loss           | 1.18e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 694      |\n",
      "|    time_elapsed    | 2406     |\n",
      "|    total_timesteps | 1399104  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1403136, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1403136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025615932 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.96        |\n",
      "|    explained_variance   | -0.0838     |\n",
      "|    learning_rate        | 0.00266     |\n",
      "|    loss                 | 0.0227      |\n",
      "|    n_updates            | 6950        |\n",
      "|    policy_gradient_loss | 0.035       |\n",
      "|    std                  | 0.00287     |\n",
      "|    value_loss           | 3.51e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 696      |\n",
      "|    time_elapsed    | 2413     |\n",
      "|    total_timesteps | 1403136  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1407168, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1407168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009671784 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.93        |\n",
      "|    explained_variance   | 0.0971      |\n",
      "|    learning_rate        | 0.00266     |\n",
      "|    loss                 | -0.00135    |\n",
      "|    n_updates            | 6970        |\n",
      "|    policy_gradient_loss | 0.00546     |\n",
      "|    std                  | 0.00294     |\n",
      "|    value_loss           | 1.45e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 698      |\n",
      "|    time_elapsed    | 2419     |\n",
      "|    total_timesteps | 1407168  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026459815 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.91        |\n",
      "|    explained_variance   | -0.0465     |\n",
      "|    learning_rate        | 0.00265     |\n",
      "|    loss                 | 0.0225      |\n",
      "|    n_updates            | 6990        |\n",
      "|    policy_gradient_loss | 0.0177      |\n",
      "|    std                  | 0.00295     |\n",
      "|    value_loss           | 5.93e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 2426     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1415232, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1415232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043672264 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.92        |\n",
      "|    explained_variance   | -0.0425     |\n",
      "|    learning_rate        | 0.00264     |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 7010        |\n",
      "|    policy_gradient_loss | 0.0216      |\n",
      "|    std                  | 0.00292     |\n",
      "|    value_loss           | 5.53e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 702      |\n",
      "|    time_elapsed    | 2433     |\n",
      "|    total_timesteps | 1415232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1419264, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1419264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006536287 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.93        |\n",
      "|    explained_variance   | -0.053      |\n",
      "|    learning_rate        | 0.00264     |\n",
      "|    loss                 | 0.00264     |\n",
      "|    n_updates            | 7030        |\n",
      "|    policy_gradient_loss | 0.018       |\n",
      "|    std                  | 0.00296     |\n",
      "|    value_loss           | 4.28e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 704      |\n",
      "|    time_elapsed    | 2440     |\n",
      "|    total_timesteps | 1419264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1423296, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1423296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024931367 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.91        |\n",
      "|    explained_variance   | 0.031       |\n",
      "|    learning_rate        | 0.00263     |\n",
      "|    loss                 | -0.00783    |\n",
      "|    n_updates            | 7050        |\n",
      "|    policy_gradient_loss | 0.0247      |\n",
      "|    std                  | 0.00299     |\n",
      "|    value_loss           | 7.13e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 706      |\n",
      "|    time_elapsed    | 2447     |\n",
      "|    total_timesteps | 1423296  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1427328, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1427328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009035479 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.92        |\n",
      "|    explained_variance   | -0.0353     |\n",
      "|    learning_rate        | 0.00262     |\n",
      "|    loss                 | -0.000394   |\n",
      "|    n_updates            | 7070        |\n",
      "|    policy_gradient_loss | 0.015       |\n",
      "|    std                  | 0.00297     |\n",
      "|    value_loss           | 3.48e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 708      |\n",
      "|    time_elapsed    | 2454     |\n",
      "|    total_timesteps | 1427328  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0171    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1431360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05335089 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.91       |\n",
      "|    explained_variance   | -0.0296    |\n",
      "|    learning_rate        | 0.00262    |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 7090       |\n",
      "|    policy_gradient_loss | 0.0353     |\n",
      "|    std                  | 0.00299    |\n",
      "|    value_loss           | 3.78e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 710      |\n",
      "|    time_elapsed    | 2461     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1435392, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1435392    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07690999 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.9        |\n",
      "|    explained_variance   | -0.0122    |\n",
      "|    learning_rate        | 0.00261    |\n",
      "|    loss                 | 0.0528     |\n",
      "|    n_updates            | 7110       |\n",
      "|    policy_gradient_loss | 0.0176     |\n",
      "|    std                  | 0.00298    |\n",
      "|    value_loss           | 4.25e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 712      |\n",
      "|    time_elapsed    | 2468     |\n",
      "|    total_timesteps | 1435392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1439424, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0167     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1439424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021138173 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.95        |\n",
      "|    explained_variance   | -0.0439     |\n",
      "|    learning_rate        | 0.0026      |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 7130        |\n",
      "|    policy_gradient_loss | 0.0202      |\n",
      "|    std                  | 0.00288     |\n",
      "|    value_loss           | 3.32e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0115  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 714      |\n",
      "|    time_elapsed    | 2475     |\n",
      "|    total_timesteps | 1439424  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1443456, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0108    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1443456    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04147079 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | -0.0517    |\n",
      "|    learning_rate        | 0.0026     |\n",
      "|    loss                 | 0.0373     |\n",
      "|    n_updates            | 7150       |\n",
      "|    policy_gradient_loss | 0.0201     |\n",
      "|    std                  | 0.00287    |\n",
      "|    value_loss           | 2.89e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 716      |\n",
      "|    time_elapsed    | 2482     |\n",
      "|    total_timesteps | 1443456  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1447488, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1447488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029565126 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.99        |\n",
      "|    explained_variance   | -0.0199     |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 7170        |\n",
      "|    policy_gradient_loss | 0.0207      |\n",
      "|    std                  | 0.00284     |\n",
      "|    value_loss           | 3.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 718      |\n",
      "|    time_elapsed    | 2489     |\n",
      "|    total_timesteps | 1447488  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00941    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051557183 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.02        |\n",
      "|    explained_variance   | 0.0477      |\n",
      "|    learning_rate        | 0.00258     |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 7190        |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    std                  | 0.0028      |\n",
      "|    value_loss           | 6.29e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 720      |\n",
      "|    time_elapsed    | 2495     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1455552, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1455552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031946324 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.02        |\n",
      "|    explained_variance   | -0.0145     |\n",
      "|    learning_rate        | 0.00258     |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 7210        |\n",
      "|    policy_gradient_loss | 0.0254      |\n",
      "|    std                  | 0.00283     |\n",
      "|    value_loss           | 1.29e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0113  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 722      |\n",
      "|    time_elapsed    | 2503     |\n",
      "|    total_timesteps | 1455552  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1459584, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1459584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022098852 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.04        |\n",
      "|    explained_variance   | -0.00179    |\n",
      "|    learning_rate        | 0.00257     |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 7230        |\n",
      "|    policy_gradient_loss | 0.0328      |\n",
      "|    std                  | 0.0028      |\n",
      "|    value_loss           | 3.51e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 724      |\n",
      "|    time_elapsed    | 2509     |\n",
      "|    total_timesteps | 1459584  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1463616, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1463616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010061879 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.08        |\n",
      "|    explained_variance   | -0.0178     |\n",
      "|    learning_rate        | 0.00256     |\n",
      "|    loss                 | -0.00243    |\n",
      "|    n_updates            | 7250        |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    std                  | 0.00274     |\n",
      "|    value_loss           | 4.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 726      |\n",
      "|    time_elapsed    | 2516     |\n",
      "|    total_timesteps | 1463616  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1467648, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00961   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1467648    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05797272 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.11       |\n",
      "|    explained_variance   | 0.111      |\n",
      "|    learning_rate        | 0.00256    |\n",
      "|    loss                 | 0.00597    |\n",
      "|    n_updates            | 7270       |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    std                  | 0.00271    |\n",
      "|    value_loss           | 1.51e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 728      |\n",
      "|    time_elapsed    | 2523     |\n",
      "|    total_timesteps | 1467648  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1471680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02598574 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.11       |\n",
      "|    explained_variance   | -0.0459    |\n",
      "|    learning_rate        | 0.00255    |\n",
      "|    loss                 | -0.00555   |\n",
      "|    n_updates            | 7290       |\n",
      "|    policy_gradient_loss | 0.0233     |\n",
      "|    std                  | 0.0027     |\n",
      "|    value_loss           | 4.22e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 730      |\n",
      "|    time_elapsed    | 2530     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1475712, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0111    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1475712    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03589791 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.12       |\n",
      "|    explained_variance   | 0.00637    |\n",
      "|    learning_rate        | 0.00254    |\n",
      "|    loss                 | 0.0018     |\n",
      "|    n_updates            | 7310       |\n",
      "|    policy_gradient_loss | 0.0256     |\n",
      "|    std                  | 0.00268    |\n",
      "|    value_loss           | 1.69e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 732      |\n",
      "|    time_elapsed    | 2536     |\n",
      "|    total_timesteps | 1475712  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1479744, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1479744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030499395 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.14        |\n",
      "|    explained_variance   | 0.0202      |\n",
      "|    learning_rate        | 0.00254     |\n",
      "|    loss                 | 1.34e-05    |\n",
      "|    n_updates            | 7330        |\n",
      "|    policy_gradient_loss | 0.0264      |\n",
      "|    std                  | 0.00266     |\n",
      "|    value_loss           | 3.91e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 734      |\n",
      "|    time_elapsed    | 2543     |\n",
      "|    total_timesteps | 1479744  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1483776, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0199     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1483776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055727966 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.13        |\n",
      "|    explained_variance   | -0.00787    |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 7350        |\n",
      "|    policy_gradient_loss | 0.0298      |\n",
      "|    std                  | 0.00268     |\n",
      "|    value_loss           | 5.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 736      |\n",
      "|    time_elapsed    | 2550     |\n",
      "|    total_timesteps | 1483776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1487808, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0162     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1487808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049161144 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.15        |\n",
      "|    explained_variance   | -0.0134     |\n",
      "|    learning_rate        | 0.00252     |\n",
      "|    loss                 | 0.0686      |\n",
      "|    n_updates            | 7370        |\n",
      "|    policy_gradient_loss | 0.0302      |\n",
      "|    std                  | 0.00266     |\n",
      "|    value_loss           | 3.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 738      |\n",
      "|    time_elapsed    | 2557     |\n",
      "|    total_timesteps | 1487808  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010726319 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.14        |\n",
      "|    explained_variance   | -0.0146     |\n",
      "|    learning_rate        | 0.00252     |\n",
      "|    loss                 | 0.00141     |\n",
      "|    n_updates            | 7390        |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    std                  | 0.00263     |\n",
      "|    value_loss           | 5.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 740      |\n",
      "|    time_elapsed    | 2564     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1495872, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1495872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008750451 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.16        |\n",
      "|    explained_variance   | -0.0179     |\n",
      "|    learning_rate        | 0.00251     |\n",
      "|    loss                 | -0.00221    |\n",
      "|    n_updates            | 7410        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.00259     |\n",
      "|    value_loss           | 5.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 742      |\n",
      "|    time_elapsed    | 2571     |\n",
      "|    total_timesteps | 1495872  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1499904, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0107      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1499904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044873147 |\n",
      "|    clip_fraction        | 0.23         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.19         |\n",
      "|    explained_variance   | 0.145        |\n",
      "|    learning_rate        | 0.0025       |\n",
      "|    loss                 | -0.00113     |\n",
      "|    n_updates            | 7430         |\n",
      "|    policy_gradient_loss | 0.00927      |\n",
      "|    std                  | 0.00256      |\n",
      "|    value_loss           | 1.34e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 744      |\n",
      "|    time_elapsed    | 2577     |\n",
      "|    total_timesteps | 1499904  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1503936, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1503936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024521872 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.24        |\n",
      "|    explained_variance   | -0.0281     |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 7450        |\n",
      "|    policy_gradient_loss | 0.0398      |\n",
      "|    std                  | 0.00248     |\n",
      "|    value_loss           | 5.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 746      |\n",
      "|    time_elapsed    | 2584     |\n",
      "|    total_timesteps | 1503936  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1507968, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1507968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006713643 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.25        |\n",
      "|    explained_variance   | -0.0421     |\n",
      "|    learning_rate        | 0.00249     |\n",
      "|    loss                 | 0.00605     |\n",
      "|    n_updates            | 7470        |\n",
      "|    policy_gradient_loss | 0.0148      |\n",
      "|    std                  | 0.00247     |\n",
      "|    value_loss           | 3.58e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 748      |\n",
      "|    time_elapsed    | 2591     |\n",
      "|    total_timesteps | 1507968  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008611244 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.22        |\n",
      "|    explained_variance   | -0.00344    |\n",
      "|    learning_rate        | 0.00248     |\n",
      "|    loss                 | -0.00116    |\n",
      "|    n_updates            | 7490        |\n",
      "|    policy_gradient_loss | 0.00483     |\n",
      "|    std                  | 0.00255     |\n",
      "|    value_loss           | 7.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 750      |\n",
      "|    time_elapsed    | 2598     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1516032, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0138     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1516032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035603642 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.17        |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.00248     |\n",
      "|    loss                 | -0.00322    |\n",
      "|    n_updates            | 7510        |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.00253     |\n",
      "|    value_loss           | 7.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 752      |\n",
      "|    time_elapsed    | 2605     |\n",
      "|    total_timesteps | 1516032  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520064, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0118    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1520064    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09348807 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | -0.059     |\n",
      "|    learning_rate        | 0.00247    |\n",
      "|    loss                 | 0.0468     |\n",
      "|    n_updates            | 7530       |\n",
      "|    policy_gradient_loss | 0.0129     |\n",
      "|    std                  | 0.00251    |\n",
      "|    value_loss           | 2.04e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 754      |\n",
      "|    time_elapsed    | 2612     |\n",
      "|    total_timesteps | 1520064  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1524096, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0142      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1524096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061542755 |\n",
      "|    clip_fraction        | 0.315        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.21         |\n",
      "|    explained_variance   | -0.0486      |\n",
      "|    learning_rate        | 0.00246      |\n",
      "|    loss                 | 0.00214      |\n",
      "|    n_updates            | 7550         |\n",
      "|    policy_gradient_loss | 0.0209       |\n",
      "|    std                  | 0.0025       |\n",
      "|    value_loss           | 1.76e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 756      |\n",
      "|    time_elapsed    | 2619     |\n",
      "|    total_timesteps | 1524096  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1528128, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1528128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017859077 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.23        |\n",
      "|    explained_variance   | -0.0534     |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | 0.00559     |\n",
      "|    n_updates            | 7570        |\n",
      "|    policy_gradient_loss | 0.0226      |\n",
      "|    std                  | 0.00247     |\n",
      "|    value_loss           | 8.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 758      |\n",
      "|    time_elapsed    | 2625     |\n",
      "|    total_timesteps | 1528128  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048798583 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.25        |\n",
      "|    explained_variance   | 0.00187     |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 7590        |\n",
      "|    policy_gradient_loss | 0.015       |\n",
      "|    std                  | 0.00247     |\n",
      "|    value_loss           | 1.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 760      |\n",
      "|    time_elapsed    | 2632     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1536192, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1536192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025345245 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.24        |\n",
      "|    explained_variance   | -0.0312     |\n",
      "|    learning_rate        | 0.00244     |\n",
      "|    loss                 | 0.00459     |\n",
      "|    n_updates            | 7610        |\n",
      "|    policy_gradient_loss | 0.019       |\n",
      "|    std                  | 0.00248     |\n",
      "|    value_loss           | 4.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 762      |\n",
      "|    time_elapsed    | 2639     |\n",
      "|    total_timesteps | 1536192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540224, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0104    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1540224    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02808746 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.24       |\n",
      "|    explained_variance   | -0.0183    |\n",
      "|    learning_rate        | 0.00244    |\n",
      "|    loss                 | 0.0526     |\n",
      "|    n_updates            | 7630       |\n",
      "|    policy_gradient_loss | 0.0185     |\n",
      "|    std                  | 0.00252    |\n",
      "|    value_loss           | 1.54e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 764      |\n",
      "|    time_elapsed    | 2646     |\n",
      "|    total_timesteps | 1540224  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1544256, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.024     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1544256    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10002537 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.25       |\n",
      "|    explained_variance   | -0.0397    |\n",
      "|    learning_rate        | 0.00243    |\n",
      "|    loss                 | 0.0747     |\n",
      "|    n_updates            | 7650       |\n",
      "|    policy_gradient_loss | 0.0205     |\n",
      "|    std                  | 0.00252    |\n",
      "|    value_loss           | 1.15e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 766      |\n",
      "|    time_elapsed    | 2652     |\n",
      "|    total_timesteps | 1544256  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1548288, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1548288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011555076 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.24        |\n",
      "|    explained_variance   | -0.0575     |\n",
      "|    learning_rate        | 0.00242     |\n",
      "|    loss                 | 0.00234     |\n",
      "|    n_updates            | 7670        |\n",
      "|    policy_gradient_loss | 0.0134      |\n",
      "|    std                  | 0.00252     |\n",
      "|    value_loss           | 5.24e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 768      |\n",
      "|    time_elapsed    | 2659     |\n",
      "|    total_timesteps | 1548288  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012356138 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.23        |\n",
      "|    explained_variance   | 0.000513    |\n",
      "|    learning_rate        | 0.00242     |\n",
      "|    loss                 | 0.000446    |\n",
      "|    n_updates            | 7690        |\n",
      "|    policy_gradient_loss | 0.0167      |\n",
      "|    std                  | 0.00253     |\n",
      "|    value_loss           | 5.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 770      |\n",
      "|    time_elapsed    | 2666     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1556352, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1556352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009411508 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.26        |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.00241     |\n",
      "|    loss                 | -0.00326    |\n",
      "|    n_updates            | 7710        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.00253     |\n",
      "|    value_loss           | 1.68e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 772      |\n",
      "|    time_elapsed    | 2673     |\n",
      "|    total_timesteps | 1556352  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560384, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0112     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1560384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007785598 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.29        |\n",
      "|    explained_variance   | -0.0345     |\n",
      "|    learning_rate        | 0.0024      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 7730        |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 0.00244     |\n",
      "|    value_loss           | 9.24e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 774      |\n",
      "|    time_elapsed    | 2680     |\n",
      "|    total_timesteps | 1560384  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1564416, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0179    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1564416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04907456 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.31       |\n",
      "|    explained_variance   | 0.0631     |\n",
      "|    learning_rate        | 0.0024     |\n",
      "|    loss                 | -0.00493   |\n",
      "|    n_updates            | 7750       |\n",
      "|    policy_gradient_loss | 0.0199     |\n",
      "|    std                  | 0.00244    |\n",
      "|    value_loss           | 1.71e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 776      |\n",
      "|    time_elapsed    | 2687     |\n",
      "|    total_timesteps | 1564416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1568448, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0212     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1568448     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032773376 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.29        |\n",
      "|    explained_variance   | -0.0318     |\n",
      "|    learning_rate        | 0.00239     |\n",
      "|    loss                 | 0.0383      |\n",
      "|    n_updates            | 7770        |\n",
      "|    policy_gradient_loss | 0.0297      |\n",
      "|    std                  | 0.00248     |\n",
      "|    value_loss           | 6.35e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 778      |\n",
      "|    time_elapsed    | 2694     |\n",
      "|    total_timesteps | 1568448  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0114    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1572480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04069471 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.32       |\n",
      "|    explained_variance   | 0.0359     |\n",
      "|    learning_rate        | 0.00238    |\n",
      "|    loss                 | 0.0064     |\n",
      "|    n_updates            | 7790       |\n",
      "|    policy_gradient_loss | 0.0127     |\n",
      "|    std                  | 0.00244    |\n",
      "|    value_loss           | 6.14e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 780      |\n",
      "|    time_elapsed    | 2700     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1576512, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1576512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012980427 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.27        |\n",
      "|    explained_variance   | -0.0359     |\n",
      "|    learning_rate        | 0.00238     |\n",
      "|    loss                 | 0.00435     |\n",
      "|    n_updates            | 7810        |\n",
      "|    policy_gradient_loss | 0.00418     |\n",
      "|    std                  | 0.00252     |\n",
      "|    value_loss           | 6.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 782      |\n",
      "|    time_elapsed    | 2707     |\n",
      "|    total_timesteps | 1576512  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580544, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1580544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033010203 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.26        |\n",
      "|    explained_variance   | -0.12       |\n",
      "|    learning_rate        | 0.00237     |\n",
      "|    loss                 | 0.0732      |\n",
      "|    n_updates            | 7830        |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    std                  | 0.0025      |\n",
      "|    value_loss           | 8.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 784      |\n",
      "|    time_elapsed    | 2714     |\n",
      "|    total_timesteps | 1580544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1584576, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0182     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1584576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049208753 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.26        |\n",
      "|    explained_variance   | -0.0213     |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 7850        |\n",
      "|    policy_gradient_loss | 0.0366      |\n",
      "|    std                  | 0.00255     |\n",
      "|    value_loss           | 2.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 786      |\n",
      "|    time_elapsed    | 2721     |\n",
      "|    total_timesteps | 1584576  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1588608, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1588608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059148088 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.24        |\n",
      "|    explained_variance   | -0.062      |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | 0.0849      |\n",
      "|    n_updates            | 7870        |\n",
      "|    policy_gradient_loss | 0.0241      |\n",
      "|    std                  | 0.00256     |\n",
      "|    value_loss           | 5.82e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 788      |\n",
      "|    time_elapsed    | 2728     |\n",
      "|    total_timesteps | 1588608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026884858 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.27        |\n",
      "|    explained_variance   | 0.0114      |\n",
      "|    learning_rate        | 0.00235     |\n",
      "|    loss                 | -0.00272    |\n",
      "|    n_updates            | 7890        |\n",
      "|    policy_gradient_loss | 0.00485     |\n",
      "|    std                  | 0.00251     |\n",
      "|    value_loss           | 6.92e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 790      |\n",
      "|    time_elapsed    | 2735     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1596672, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0206    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1596672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08282839 |\n",
      "|    clip_fraction        | 0.525      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.173      |\n",
      "|    learning_rate        | 0.00234    |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 7910       |\n",
      "|    policy_gradient_loss | 0.0318     |\n",
      "|    std                  | 0.0026     |\n",
      "|    value_loss           | 5.55e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 792      |\n",
      "|    time_elapsed    | 2742     |\n",
      "|    total_timesteps | 1596672  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600704, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0156     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072198085 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.16        |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00234     |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 7930        |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    std                  | 0.00265     |\n",
      "|    value_loss           | 5.45e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 794      |\n",
      "|    time_elapsed    | 2748     |\n",
      "|    total_timesteps | 1600704  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1604736, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1604736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057897415 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.17        |\n",
      "|    explained_variance   | -0.049      |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | -0.00157    |\n",
      "|    n_updates            | 7950        |\n",
      "|    policy_gradient_loss | 0.0392      |\n",
      "|    std                  | 0.00264     |\n",
      "|    value_loss           | 7.24e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 796      |\n",
      "|    time_elapsed    | 2755     |\n",
      "|    total_timesteps | 1604736  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1608768, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1608768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013208751 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.19        |\n",
      "|    explained_variance   | -0.0399     |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 0.0121      |\n",
      "|    n_updates            | 7970        |\n",
      "|    policy_gradient_loss | 0.014       |\n",
      "|    std                  | 0.00263     |\n",
      "|    value_loss           | 3.5e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 798      |\n",
      "|    time_elapsed    | 2762     |\n",
      "|    total_timesteps | 1608768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022806074 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.18        |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 0.0399      |\n",
      "|    n_updates            | 7990        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.00268     |\n",
      "|    value_loss           | 1.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 2769     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1616832, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1616832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026573304 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.18        |\n",
      "|    explained_variance   | -0.034      |\n",
      "|    learning_rate        | 0.00231     |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 8010        |\n",
      "|    policy_gradient_loss | 0.0257      |\n",
      "|    std                  | 0.00265     |\n",
      "|    value_loss           | 1.01e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 802      |\n",
      "|    time_elapsed    | 2776     |\n",
      "|    total_timesteps | 1616832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620864, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1620864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029678587 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.22        |\n",
      "|    explained_variance   | -0.0569     |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 8030        |\n",
      "|    policy_gradient_loss | 0.0168      |\n",
      "|    std                  | 0.00259     |\n",
      "|    value_loss           | 3.38e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 804      |\n",
      "|    time_elapsed    | 2782     |\n",
      "|    total_timesteps | 1620864  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1624896, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0143    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1624896    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12373993 |\n",
      "|    clip_fraction        | 0.506      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | 0.0156     |\n",
      "|    learning_rate        | 0.0023     |\n",
      "|    loss                 | 0.00685    |\n",
      "|    n_updates            | 8050       |\n",
      "|    policy_gradient_loss | 0.0302     |\n",
      "|    std                  | 0.00267    |\n",
      "|    value_loss           | 1.77e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 806      |\n",
      "|    time_elapsed    | 2789     |\n",
      "|    total_timesteps | 1624896  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1628928, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0153     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1628928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023099171 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.12        |\n",
      "|    explained_variance   | -0.0592     |\n",
      "|    learning_rate        | 0.00229     |\n",
      "|    loss                 | 0.00872     |\n",
      "|    n_updates            | 8070        |\n",
      "|    policy_gradient_loss | 0.0272      |\n",
      "|    std                  | 0.00276     |\n",
      "|    value_loss           | 2.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 808      |\n",
      "|    time_elapsed    | 2796     |\n",
      "|    total_timesteps | 1628928  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018718861 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.12        |\n",
      "|    explained_variance   | 0.0698      |\n",
      "|    learning_rate        | 0.00228     |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 8090        |\n",
      "|    policy_gradient_loss | 0.00929     |\n",
      "|    std                  | 0.00272     |\n",
      "|    value_loss           | 8.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 810      |\n",
      "|    time_elapsed    | 2803     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1636992, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0134     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1636992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017906822 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.07        |\n",
      "|    explained_variance   | -0.22       |\n",
      "|    learning_rate        | 0.00228     |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 8110        |\n",
      "|    policy_gradient_loss | 0.00786     |\n",
      "|    std                  | 0.00277     |\n",
      "|    value_loss           | 4.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 812      |\n",
      "|    time_elapsed    | 2810     |\n",
      "|    total_timesteps | 1636992  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1641024, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0199     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1641024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895166 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.08        |\n",
      "|    explained_variance   | -0.0318     |\n",
      "|    learning_rate        | 0.00227     |\n",
      "|    loss                 | -0.000623   |\n",
      "|    n_updates            | 8130        |\n",
      "|    policy_gradient_loss | 0.00954     |\n",
      "|    std                  | 0.00277     |\n",
      "|    value_loss           | 9.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 814      |\n",
      "|    time_elapsed    | 2817     |\n",
      "|    total_timesteps | 1641024  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1645056, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1645056    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07539915 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.08       |\n",
      "|    explained_variance   | 0.236      |\n",
      "|    learning_rate        | 0.00226    |\n",
      "|    loss                 | 0.0512     |\n",
      "|    n_updates            | 8150       |\n",
      "|    policy_gradient_loss | 0.0124     |\n",
      "|    std                  | 0.00282    |\n",
      "|    value_loss           | 3.98e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 816      |\n",
      "|    time_elapsed    | 2824     |\n",
      "|    total_timesteps | 1645056  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1649088, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1649088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016035048 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.11        |\n",
      "|    explained_variance   | 0.0102      |\n",
      "|    learning_rate        | 0.00225     |\n",
      "|    loss                 | 0.00291     |\n",
      "|    n_updates            | 8170        |\n",
      "|    policy_gradient_loss | 0.0134      |\n",
      "|    std                  | 0.00279     |\n",
      "|    value_loss           | 4.48e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 818      |\n",
      "|    time_elapsed    | 2830     |\n",
      "|    total_timesteps | 1649088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010652454 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.12        |\n",
      "|    explained_variance   | -0.033      |\n",
      "|    learning_rate        | 0.00225     |\n",
      "|    loss                 | 0.0239      |\n",
      "|    n_updates            | 8190        |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    std                  | 0.00274     |\n",
      "|    value_loss           | 6.53e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 820      |\n",
      "|    time_elapsed    | 2837     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1657152, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.01       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1657152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016263118 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.16        |\n",
      "|    explained_variance   | -0.0812     |\n",
      "|    learning_rate        | 0.00224     |\n",
      "|    loss                 | 0.00475     |\n",
      "|    n_updates            | 8210        |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.00267     |\n",
      "|    value_loss           | 1.38e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 822      |\n",
      "|    time_elapsed    | 2844     |\n",
      "|    total_timesteps | 1657152  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1661184, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.017      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1661184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028525217 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.2         |\n",
      "|    explained_variance   | -0.0775     |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | 0.0558      |\n",
      "|    n_updates            | 8230        |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    std                  | 0.00265     |\n",
      "|    value_loss           | 5.63e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 824      |\n",
      "|    time_elapsed    | 2851     |\n",
      "|    total_timesteps | 1661184  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1665216, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.00971   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1665216    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10507289 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | -0.0404    |\n",
      "|    learning_rate        | 0.00223    |\n",
      "|    loss                 | 0.0893     |\n",
      "|    n_updates            | 8250       |\n",
      "|    policy_gradient_loss | 0.0606     |\n",
      "|    std                  | 0.00265    |\n",
      "|    value_loss           | 4.88e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 826      |\n",
      "|    time_elapsed    | 2859     |\n",
      "|    total_timesteps | 1665216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1669248, episode_reward=-0.03 +/- 0.03\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0317    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1669248    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03012835 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.00222    |\n",
      "|    loss                 | 0.0068     |\n",
      "|    n_updates            | 8270       |\n",
      "|    policy_gradient_loss | 0.00824    |\n",
      "|    std                  | 0.00259    |\n",
      "|    value_loss           | 1.65e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 828      |\n",
      "|    time_elapsed    | 2867     |\n",
      "|    total_timesteps | 1669248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005126721 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.19        |\n",
      "|    explained_variance   | -0.0468     |\n",
      "|    learning_rate        | 0.00221     |\n",
      "|    loss                 | -0.00403    |\n",
      "|    n_updates            | 8290        |\n",
      "|    policy_gradient_loss | 0.0197      |\n",
      "|    std                  | 0.00261     |\n",
      "|    value_loss           | 8.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 830      |\n",
      "|    time_elapsed    | 2874     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1677312, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1677312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010958053 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.23        |\n",
      "|    explained_variance   | -0.1        |\n",
      "|    learning_rate        | 0.00221     |\n",
      "|    loss                 | -0.0078     |\n",
      "|    n_updates            | 8310        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00255     |\n",
      "|    value_loss           | 1.64e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 832      |\n",
      "|    time_elapsed    | 2881     |\n",
      "|    total_timesteps | 1677312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1681344, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0143    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1681344    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10068737 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.24       |\n",
      "|    explained_variance   | -0.12      |\n",
      "|    learning_rate        | 0.0022     |\n",
      "|    loss                 | 0.00752    |\n",
      "|    n_updates            | 8330       |\n",
      "|    policy_gradient_loss | 0.0235     |\n",
      "|    std                  | 0.00255    |\n",
      "|    value_loss           | 8.17e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 834      |\n",
      "|    time_elapsed    | 2888     |\n",
      "|    total_timesteps | 1681344  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1685376, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0106     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1685376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014848436 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.26        |\n",
      "|    explained_variance   | -0.0799     |\n",
      "|    learning_rate        | 0.00219     |\n",
      "|    loss                 | 0.000794    |\n",
      "|    n_updates            | 8350        |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    std                  | 0.00254     |\n",
      "|    value_loss           | 4.16e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 836      |\n",
      "|    time_elapsed    | 2896     |\n",
      "|    total_timesteps | 1685376  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1689408, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0125    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1689408    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04452131 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.27       |\n",
      "|    explained_variance   | -0.0405    |\n",
      "|    learning_rate        | 0.00219    |\n",
      "|    loss                 | 0.0805     |\n",
      "|    n_updates            | 8370       |\n",
      "|    policy_gradient_loss | 0.0106     |\n",
      "|    std                  | 0.00251    |\n",
      "|    value_loss           | 7.73e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 838      |\n",
      "|    time_elapsed    | 2903     |\n",
      "|    total_timesteps | 1689408  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.014      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056885973 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.28        |\n",
      "|    explained_variance   | 0.0429      |\n",
      "|    learning_rate        | 0.00218     |\n",
      "|    loss                 | 0.0704      |\n",
      "|    n_updates            | 8390        |\n",
      "|    policy_gradient_loss | 0.0361      |\n",
      "|    std                  | 0.00253     |\n",
      "|    value_loss           | 1.27e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 840      |\n",
      "|    time_elapsed    | 2910     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1697472, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00984    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1697472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016224071 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.32        |\n",
      "|    explained_variance   | -0.0592     |\n",
      "|    learning_rate        | 0.00217     |\n",
      "|    loss                 | -0.000424   |\n",
      "|    n_updates            | 8410        |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    std                  | 0.00245     |\n",
      "|    value_loss           | 8.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 842      |\n",
      "|    time_elapsed    | 2917     |\n",
      "|    total_timesteps | 1697472  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1701504, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0114    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1701504    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02886558 |\n",
      "|    clip_fraction        | 0.399      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.31       |\n",
      "|    explained_variance   | -0.069     |\n",
      "|    learning_rate        | 0.00217    |\n",
      "|    loss                 | 0.0294     |\n",
      "|    n_updates            | 8430       |\n",
      "|    policy_gradient_loss | 0.024      |\n",
      "|    std                  | 0.00245    |\n",
      "|    value_loss           | 3.73e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 844      |\n",
      "|    time_elapsed    | 2924     |\n",
      "|    total_timesteps | 1701504  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1705536, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1705536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024946691 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.34        |\n",
      "|    explained_variance   | -0.0585     |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | -0.000368   |\n",
      "|    n_updates            | 8450        |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.00245     |\n",
      "|    value_loss           | 4.93e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 846      |\n",
      "|    time_elapsed    | 2931     |\n",
      "|    total_timesteps | 1705536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1709568, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0135    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1709568    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03198011 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.37       |\n",
      "|    explained_variance   | -0.0964    |\n",
      "|    learning_rate        | 0.00215    |\n",
      "|    loss                 | 0.0389     |\n",
      "|    n_updates            | 8470       |\n",
      "|    policy_gradient_loss | 0.0238     |\n",
      "|    std                  | 0.00242    |\n",
      "|    value_loss           | 1.13e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0114  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 848      |\n",
      "|    time_elapsed    | 2939     |\n",
      "|    total_timesteps | 1709568  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0205    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1713600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08392223 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.41       |\n",
      "|    explained_variance   | 0.201      |\n",
      "|    learning_rate        | 0.00215    |\n",
      "|    loss                 | 0.0229     |\n",
      "|    n_updates            | 8490       |\n",
      "|    policy_gradient_loss | 0.00963    |\n",
      "|    std                  | 0.00238    |\n",
      "|    value_loss           | 2.12e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 850      |\n",
      "|    time_elapsed    | 2946     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1717632, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0119    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1717632    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03252615 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.43       |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.00214    |\n",
      "|    loss                 | 0.00854    |\n",
      "|    n_updates            | 8510       |\n",
      "|    policy_gradient_loss | 0.0049     |\n",
      "|    std                  | 0.00235    |\n",
      "|    value_loss           | 1.11e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 852      |\n",
      "|    time_elapsed    | 2953     |\n",
      "|    total_timesteps | 1717632  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1721664, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.018     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1721664    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04230559 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | -0.107     |\n",
      "|    learning_rate        | 0.00213    |\n",
      "|    loss                 | 0.0291     |\n",
      "|    n_updates            | 8530       |\n",
      "|    policy_gradient_loss | 0.0331     |\n",
      "|    std                  | 0.0024     |\n",
      "|    value_loss           | 5.85e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 854      |\n",
      "|    time_elapsed    | 2960     |\n",
      "|    total_timesteps | 1721664  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1725696, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1725696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019820832 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.32        |\n",
      "|    explained_variance   | -0.0595     |\n",
      "|    learning_rate        | 0.00213     |\n",
      "|    loss                 | -0.000219   |\n",
      "|    n_updates            | 8550        |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    std                  | 0.00248     |\n",
      "|    value_loss           | 4.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 856      |\n",
      "|    time_elapsed    | 2967     |\n",
      "|    total_timesteps | 1725696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1729728, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0119    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1729728    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03541462 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.33       |\n",
      "|    explained_variance   | 0.0332     |\n",
      "|    learning_rate        | 0.00212    |\n",
      "|    loss                 | -0.00399   |\n",
      "|    n_updates            | 8570       |\n",
      "|    policy_gradient_loss | 0.012      |\n",
      "|    std                  | 0.00248    |\n",
      "|    value_loss           | 1.4e-07    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 858      |\n",
      "|    time_elapsed    | 2975     |\n",
      "|    total_timesteps | 1729728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1733760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015127083 |\n",
      "|    clip_fraction        | 0.473       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.36        |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.00211     |\n",
      "|    loss                 | 0.00792     |\n",
      "|    n_updates            | 8590        |\n",
      "|    policy_gradient_loss | 0.0329      |\n",
      "|    std                  | 0.00241     |\n",
      "|    value_loss           | 2.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 860      |\n",
      "|    time_elapsed    | 2982     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1737792, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1737792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081405565 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.38        |\n",
      "|    explained_variance   | -0.0937     |\n",
      "|    learning_rate        | 0.00211     |\n",
      "|    loss                 | 0.0261      |\n",
      "|    n_updates            | 8610        |\n",
      "|    policy_gradient_loss | 0.0236      |\n",
      "|    std                  | 0.0024      |\n",
      "|    value_loss           | 4.09e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 862      |\n",
      "|    time_elapsed    | 2989     |\n",
      "|    total_timesteps | 1737792  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1741824, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1741824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107850105 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.36        |\n",
      "|    explained_variance   | -0.0686     |\n",
      "|    learning_rate        | 0.0021      |\n",
      "|    loss                 | 0.16        |\n",
      "|    n_updates            | 8630        |\n",
      "|    policy_gradient_loss | 0.0283      |\n",
      "|    std                  | 0.00244     |\n",
      "|    value_loss           | 3.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 864      |\n",
      "|    time_elapsed    | 2996     |\n",
      "|    total_timesteps | 1741824  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1745856, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0135     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1745856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007592007 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.38        |\n",
      "|    explained_variance   | -0.0372     |\n",
      "|    learning_rate        | 0.00209     |\n",
      "|    loss                 | 0.00863     |\n",
      "|    n_updates            | 8650        |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.0024      |\n",
      "|    value_loss           | 4.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 866      |\n",
      "|    time_elapsed    | 3003     |\n",
      "|    total_timesteps | 1745856  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1749888, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1749888    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06680006 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.39       |\n",
      "|    explained_variance   | -0.0362    |\n",
      "|    learning_rate        | 0.00209    |\n",
      "|    loss                 | 0.0639     |\n",
      "|    n_updates            | 8670       |\n",
      "|    policy_gradient_loss | 0.012      |\n",
      "|    std                  | 0.00239    |\n",
      "|    value_loss           | 1.26e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 868      |\n",
      "|    time_elapsed    | 3011     |\n",
      "|    total_timesteps | 1749888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0106    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1753920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01947359 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.38       |\n",
      "|    explained_variance   | 0.0957     |\n",
      "|    learning_rate        | 0.00208    |\n",
      "|    loss                 | 0.0104     |\n",
      "|    n_updates            | 8690       |\n",
      "|    policy_gradient_loss | 0.00709    |\n",
      "|    std                  | 0.00242    |\n",
      "|    value_loss           | 2.24e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 870      |\n",
      "|    time_elapsed    | 3018     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1757952, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1757952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044336773 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.00207     |\n",
      "|    loss                 | 0.00966     |\n",
      "|    n_updates            | 8710        |\n",
      "|    policy_gradient_loss | 0.0265      |\n",
      "|    std                  | 0.00243     |\n",
      "|    value_loss           | 1.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 872      |\n",
      "|    time_elapsed    | 3025     |\n",
      "|    total_timesteps | 1757952  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1761984, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1761984      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0137696415 |\n",
      "|    clip_fraction        | 0.265        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.39         |\n",
      "|    explained_variance   | 0.314        |\n",
      "|    learning_rate        | 0.00207      |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 8730         |\n",
      "|    policy_gradient_loss | 0.00931      |\n",
      "|    std                  | 0.00243      |\n",
      "|    value_loss           | 3.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 874      |\n",
      "|    time_elapsed    | 3032     |\n",
      "|    total_timesteps | 1761984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1766016, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0113    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1766016    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07124004 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | -0.019     |\n",
      "|    learning_rate        | 0.00206    |\n",
      "|    loss                 | 0.00683    |\n",
      "|    n_updates            | 8750       |\n",
      "|    policy_gradient_loss | 0.0107     |\n",
      "|    std                  | 0.00237    |\n",
      "|    value_loss           | 4.82e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 876      |\n",
      "|    time_elapsed    | 3039     |\n",
      "|    total_timesteps | 1766016  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770048, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0132    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1770048    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04757631 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.39       |\n",
      "|    explained_variance   | -0.071     |\n",
      "|    learning_rate        | 0.00205    |\n",
      "|    loss                 | 0.0483     |\n",
      "|    n_updates            | 8770       |\n",
      "|    policy_gradient_loss | 0.0128     |\n",
      "|    std                  | 0.0024     |\n",
      "|    value_loss           | 4.61e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 878      |\n",
      "|    time_elapsed    | 3047     |\n",
      "|    total_timesteps | 1770048  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024782324 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.43        |\n",
      "|    explained_variance   | 0.0635      |\n",
      "|    learning_rate        | 0.00205     |\n",
      "|    loss                 | 0.0348      |\n",
      "|    n_updates            | 8790        |\n",
      "|    policy_gradient_loss | 0.0222      |\n",
      "|    std                  | 0.00236     |\n",
      "|    value_loss           | 9.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 880      |\n",
      "|    time_elapsed    | 3054     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1778112, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1778112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030272124 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.44        |\n",
      "|    explained_variance   | 0.0602      |\n",
      "|    learning_rate        | 0.00204     |\n",
      "|    loss                 | 0.0239      |\n",
      "|    n_updates            | 8810        |\n",
      "|    policy_gradient_loss | 0.0282      |\n",
      "|    std                  | 0.00237     |\n",
      "|    value_loss           | 3.72e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 882      |\n",
      "|    time_elapsed    | 3062     |\n",
      "|    total_timesteps | 1778112  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1782144, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0186     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1782144     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012976749 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.44        |\n",
      "|    explained_variance   | 0.00976     |\n",
      "|    learning_rate        | 0.00203     |\n",
      "|    loss                 | 0.0072      |\n",
      "|    n_updates            | 8830        |\n",
      "|    policy_gradient_loss | 0.0194      |\n",
      "|    std                  | 0.00236     |\n",
      "|    value_loss           | 3.53e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 884      |\n",
      "|    time_elapsed    | 3069     |\n",
      "|    total_timesteps | 1782144  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1786176, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1786176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009050949 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.44        |\n",
      "|    explained_variance   | -0.063      |\n",
      "|    learning_rate        | 0.00203     |\n",
      "|    loss                 | 0.00348     |\n",
      "|    n_updates            | 8850        |\n",
      "|    policy_gradient_loss | 0.0287      |\n",
      "|    std                  | 0.00234     |\n",
      "|    value_loss           | 2.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 886      |\n",
      "|    time_elapsed    | 3076     |\n",
      "|    total_timesteps | 1786176  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790208, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0143   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1790208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0811958 |\n",
      "|    clip_fraction        | 0.345     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.48      |\n",
      "|    explained_variance   | -0.045    |\n",
      "|    learning_rate        | 0.00202   |\n",
      "|    loss                 | 0.0852    |\n",
      "|    n_updates            | 8870      |\n",
      "|    policy_gradient_loss | 0.027     |\n",
      "|    std                  | 0.00232   |\n",
      "|    value_loss           | 4.56e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 888      |\n",
      "|    time_elapsed    | 3083     |\n",
      "|    total_timesteps | 1790208  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1794240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014873115 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.47        |\n",
      "|    explained_variance   | -0.0409     |\n",
      "|    learning_rate        | 0.00201     |\n",
      "|    loss                 | -0.000725   |\n",
      "|    n_updates            | 8890        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.00233     |\n",
      "|    value_loss           | 1.2e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.012   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 890      |\n",
      "|    time_elapsed    | 3090     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1798272, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1798272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099460356 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | 0.045       |\n",
      "|    learning_rate        | 0.00201     |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 8910        |\n",
      "|    policy_gradient_loss | 0.0299      |\n",
      "|    std                  | 0.00238     |\n",
      "|    value_loss           | 2.25e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 892      |\n",
      "|    time_elapsed    | 3098     |\n",
      "|    total_timesteps | 1798272  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1802304, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1802304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022337697 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.45        |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.002       |\n",
      "|    loss                 | -0.00567    |\n",
      "|    n_updates            | 8930        |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    std                  | 0.00233     |\n",
      "|    value_loss           | 1.57e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 894      |\n",
      "|    time_elapsed    | 3105     |\n",
      "|    total_timesteps | 1802304  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1806336, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1806336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059493475 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.45        |\n",
      "|    explained_variance   | -0.0464     |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | 0.0653      |\n",
      "|    n_updates            | 8950        |\n",
      "|    policy_gradient_loss | 0.0379      |\n",
      "|    std                  | 0.00234     |\n",
      "|    value_loss           | 1.47e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 896      |\n",
      "|    time_elapsed    | 3112     |\n",
      "|    total_timesteps | 1806336  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810368, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0143    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1810368    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01830248 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.193      |\n",
      "|    learning_rate        | 0.00199    |\n",
      "|    loss                 | 0.000136   |\n",
      "|    n_updates            | 8970       |\n",
      "|    policy_gradient_loss | 0.00831    |\n",
      "|    std                  | 0.00223    |\n",
      "|    value_loss           | 2e-07      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 898      |\n",
      "|    time_elapsed    | 3120     |\n",
      "|    total_timesteps | 1810368  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0139   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1814400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0239169 |\n",
      "|    clip_fraction        | 0.53      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.56      |\n",
      "|    explained_variance   | 0.232     |\n",
      "|    learning_rate        | 0.00198   |\n",
      "|    loss                 | 0.00388   |\n",
      "|    n_updates            | 8990      |\n",
      "|    policy_gradient_loss | 0.0469    |\n",
      "|    std                  | 0.00223   |\n",
      "|    value_loss           | 1.19e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 900      |\n",
      "|    time_elapsed    | 3127     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1818432, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1818432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022213165 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | -0.0454     |\n",
      "|    learning_rate        | 0.00197     |\n",
      "|    loss                 | 0.00293     |\n",
      "|    n_updates            | 9010        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 4.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 902      |\n",
      "|    time_elapsed    | 3135     |\n",
      "|    total_timesteps | 1818432  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1822464, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1822464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019478124 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.00197     |\n",
      "|    loss                 | 0.0187      |\n",
      "|    n_updates            | 9030        |\n",
      "|    policy_gradient_loss | 0.0245      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 7.41e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 904      |\n",
      "|    time_elapsed    | 3142     |\n",
      "|    total_timesteps | 1822464  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1826496, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1826496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057746485 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | 0.0259      |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | 0.0862      |\n",
      "|    n_updates            | 9050        |\n",
      "|    policy_gradient_loss | 0.0235      |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 7.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 906      |\n",
      "|    time_elapsed    | 3149     |\n",
      "|    total_timesteps | 1826496  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830528, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1830528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059066255 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.0385     |\n",
      "|    learning_rate        | 0.00195     |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 9070        |\n",
      "|    policy_gradient_loss | 0.0213      |\n",
      "|    std                  | 0.00226     |\n",
      "|    value_loss           | 5.02e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 908      |\n",
      "|    time_elapsed    | 3157     |\n",
      "|    total_timesteps | 1830528  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0088   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1834560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4228875 |\n",
      "|    clip_fraction        | 0.486     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.59      |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.00195   |\n",
      "|    loss                 | 0.0353    |\n",
      "|    n_updates            | 9090      |\n",
      "|    policy_gradient_loss | 0.0625    |\n",
      "|    std                  | 0.00225   |\n",
      "|    value_loss           | 8.9e-08   |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 910      |\n",
      "|    time_elapsed    | 3164     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1838592, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0111     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1838592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017586004 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.00194     |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 9110        |\n",
      "|    policy_gradient_loss | 0.0177      |\n",
      "|    std                  | 0.00224     |\n",
      "|    value_loss           | 5.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 912      |\n",
      "|    time_elapsed    | 3171     |\n",
      "|    total_timesteps | 1838592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1842624, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1842624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095458224 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.0582     |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 9130        |\n",
      "|    policy_gradient_loss | 0.0407      |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 5.92e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 914      |\n",
      "|    time_elapsed    | 3178     |\n",
      "|    total_timesteps | 1842624  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1846656, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0181     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1846656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015274055 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.0628     |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | -0.00868    |\n",
      "|    n_updates            | 9150        |\n",
      "|    policy_gradient_loss | 0.0221      |\n",
      "|    std                  | 0.00226     |\n",
      "|    value_loss           | 5.74e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 916      |\n",
      "|    time_elapsed    | 3186     |\n",
      "|    total_timesteps | 1846656  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850688, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1850688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075778715 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | -0.00972    |\n",
      "|    learning_rate        | 0.00192     |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 9170        |\n",
      "|    policy_gradient_loss | 0.0178      |\n",
      "|    std                  | 0.00224     |\n",
      "|    value_loss           | 1.21e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 918      |\n",
      "|    time_elapsed    | 3193     |\n",
      "|    total_timesteps | 1850688  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0116    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1854720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06369697 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.58       |\n",
      "|    explained_variance   | 0.124      |\n",
      "|    learning_rate        | 0.00191    |\n",
      "|    loss                 | 0.0384     |\n",
      "|    n_updates            | 9190       |\n",
      "|    policy_gradient_loss | 0.0181     |\n",
      "|    std                  | 0.00222    |\n",
      "|    value_loss           | 9.18e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 920      |\n",
      "|    time_elapsed    | 3200     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1858752, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0101     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1858752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012256508 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0819     |\n",
      "|    learning_rate        | 0.00191     |\n",
      "|    loss                 | 0.00216     |\n",
      "|    n_updates            | 9210        |\n",
      "|    policy_gradient_loss | 0.0226      |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 1.5e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 922      |\n",
      "|    time_elapsed    | 3207     |\n",
      "|    total_timesteps | 1858752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1862784, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0113    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1862784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07069495 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.55       |\n",
      "|    explained_variance   | 0.0634     |\n",
      "|    learning_rate        | 0.0019     |\n",
      "|    loss                 | 0.00358    |\n",
      "|    n_updates            | 9230       |\n",
      "|    policy_gradient_loss | 0.0162     |\n",
      "|    std                  | 0.00227    |\n",
      "|    value_loss           | 7.89e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 924      |\n",
      "|    time_elapsed    | 3215     |\n",
      "|    total_timesteps | 1862784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1866816, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1866816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028048977 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 9250        |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.0023      |\n",
      "|    value_loss           | 1.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 926      |\n",
      "|    time_elapsed    | 3222     |\n",
      "|    total_timesteps | 1866816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870848, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0106     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1870848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036868427 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.54        |\n",
      "|    explained_variance   | -0.083      |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | -0.00373    |\n",
      "|    n_updates            | 9270        |\n",
      "|    policy_gradient_loss | 0.0234      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 4.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 928      |\n",
      "|    time_elapsed    | 3229     |\n",
      "|    total_timesteps | 1870848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014994354 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.55        |\n",
      "|    explained_variance   | -0.0561     |\n",
      "|    learning_rate        | 0.00188     |\n",
      "|    loss                 | 0.000201    |\n",
      "|    n_updates            | 9290        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00229     |\n",
      "|    value_loss           | 1.96e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 930      |\n",
      "|    time_elapsed    | 3237     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1878912, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0118    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1878912    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09673646 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | -0.0344    |\n",
      "|    learning_rate        | 0.00187    |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 9310       |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    std                  | 0.00231    |\n",
      "|    value_loss           | 2.5e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0119  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 932      |\n",
      "|    time_elapsed    | 3244     |\n",
      "|    total_timesteps | 1878912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1882944, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.011     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1882944    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01084642 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | 0.0459     |\n",
      "|    learning_rate        | 0.00187    |\n",
      "|    loss                 | 0.00521    |\n",
      "|    n_updates            | 9330       |\n",
      "|    policy_gradient_loss | 0.0212     |\n",
      "|    std                  | 0.00231    |\n",
      "|    value_loss           | 5.41e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 934      |\n",
      "|    time_elapsed    | 3251     |\n",
      "|    total_timesteps | 1882944  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1886976, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0197    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1886976    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07986026 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | -0.0218    |\n",
      "|    learning_rate        | 0.00186    |\n",
      "|    loss                 | 0.00118    |\n",
      "|    n_updates            | 9350       |\n",
      "|    policy_gradient_loss | 0.0278     |\n",
      "|    std                  | 0.00231    |\n",
      "|    value_loss           | 2.51e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0118  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 936      |\n",
      "|    time_elapsed    | 3258     |\n",
      "|    total_timesteps | 1886976  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1891008, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1891008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023777232 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | -0.0645     |\n",
      "|    learning_rate        | 0.00185     |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 9370        |\n",
      "|    policy_gradient_loss | 0.0175      |\n",
      "|    std                  | 0.00226     |\n",
      "|    value_loss           | 3.64e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 938      |\n",
      "|    time_elapsed    | 3266     |\n",
      "|    total_timesteps | 1891008  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0132    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1895040    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06801052 |\n",
      "|    clip_fraction        | 0.323      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.6        |\n",
      "|    explained_variance   | -0.081     |\n",
      "|    learning_rate        | 0.00184    |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 9390       |\n",
      "|    policy_gradient_loss | 0.023      |\n",
      "|    std                  | 0.00223    |\n",
      "|    value_loss           | 2.82e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 940      |\n",
      "|    time_elapsed    | 3273     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1899072, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0112    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1899072    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04071791 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.63       |\n",
      "|    explained_variance   | 0.0303     |\n",
      "|    learning_rate        | 0.00184    |\n",
      "|    loss                 | 0.0139     |\n",
      "|    n_updates            | 9410       |\n",
      "|    policy_gradient_loss | 0.0177     |\n",
      "|    std                  | 0.0022     |\n",
      "|    value_loss           | 2.29e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 942      |\n",
      "|    time_elapsed    | 3280     |\n",
      "|    total_timesteps | 1899072  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1903104, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0188     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1903104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010886236 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.61        |\n",
      "|    explained_variance   | -0.0273     |\n",
      "|    learning_rate        | 0.00183     |\n",
      "|    loss                 | 0.00433     |\n",
      "|    n_updates            | 9430        |\n",
      "|    policy_gradient_loss | 0.0233      |\n",
      "|    std                  | 0.00223     |\n",
      "|    value_loss           | 5.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 944      |\n",
      "|    time_elapsed    | 3288     |\n",
      "|    total_timesteps | 1903104  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1907136, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1907136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016184043 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | -0.0834     |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 9450        |\n",
      "|    policy_gradient_loss | 0.0225      |\n",
      "|    std                  | 0.00227     |\n",
      "|    value_loss           | 3.46e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 946      |\n",
      "|    time_elapsed    | 3295     |\n",
      "|    total_timesteps | 1907136  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1911168, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0186     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1911168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063319355 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0701     |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 9470        |\n",
      "|    policy_gradient_loss | 0.0186      |\n",
      "|    std                  | 0.00232     |\n",
      "|    value_loss           | 2.99e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 948      |\n",
      "|    time_elapsed    | 3303     |\n",
      "|    total_timesteps | 1911168  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018481184 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | -0.0494     |\n",
      "|    learning_rate        | 0.00181     |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 9490        |\n",
      "|    policy_gradient_loss | 0.0403      |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 4.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 950      |\n",
      "|    time_elapsed    | 3310     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1919232, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.00955  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1919232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0436352 |\n",
      "|    clip_fraction        | 0.25      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.63      |\n",
      "|    explained_variance   | 0.137     |\n",
      "|    learning_rate        | 0.0018    |\n",
      "|    loss                 | 0.00593   |\n",
      "|    n_updates            | 9510      |\n",
      "|    policy_gradient_loss | 0.0132    |\n",
      "|    std                  | 0.00223   |\n",
      "|    value_loss           | 1.55e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 952      |\n",
      "|    time_elapsed    | 3317     |\n",
      "|    total_timesteps | 1919232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1923264, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1923264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005730284 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | 0.0247      |\n",
      "|    learning_rate        | 0.0018      |\n",
      "|    loss                 | 0.00106     |\n",
      "|    n_updates            | 9530        |\n",
      "|    policy_gradient_loss | 0.0134      |\n",
      "|    std                  | 0.00222     |\n",
      "|    value_loss           | 5.64e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 954      |\n",
      "|    time_elapsed    | 3324     |\n",
      "|    total_timesteps | 1923264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1927296, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1927296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014682309 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.61        |\n",
      "|    explained_variance   | -0.0569     |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | -0.000429   |\n",
      "|    n_updates            | 9550        |\n",
      "|    policy_gradient_loss | 0.00906     |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 2.14e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 956      |\n",
      "|    time_elapsed    | 3332     |\n",
      "|    total_timesteps | 1927296  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1931328, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1931328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043256883 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | -0.0207     |\n",
      "|    learning_rate        | 0.00178     |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 9570        |\n",
      "|    policy_gradient_loss | 0.0241      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 3e-06       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 958      |\n",
      "|    time_elapsed    | 3339     |\n",
      "|    total_timesteps | 1931328  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020943273 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.128      |\n",
      "|    learning_rate        | 0.00178     |\n",
      "|    loss                 | -0.00354    |\n",
      "|    n_updates            | 9590        |\n",
      "|    policy_gradient_loss | 0.0126      |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 6.23e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 960      |\n",
      "|    time_elapsed    | 3346     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1939392, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1939392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031135393 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.00177     |\n",
      "|    loss                 | 0.00339     |\n",
      "|    n_updates            | 9610        |\n",
      "|    policy_gradient_loss | 0.0228      |\n",
      "|    std                  | 0.00232     |\n",
      "|    value_loss           | 1.81e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 962      |\n",
      "|    time_elapsed    | 3353     |\n",
      "|    total_timesteps | 1939392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1943424, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0129   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1943424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0452113 |\n",
      "|    clip_fraction        | 0.316     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.55      |\n",
      "|    explained_variance   | 0.273     |\n",
      "|    learning_rate        | 0.00176   |\n",
      "|    loss                 | 0.0546    |\n",
      "|    n_updates            | 9630      |\n",
      "|    policy_gradient_loss | 0.0156    |\n",
      "|    std                  | 0.00232   |\n",
      "|    value_loss           | 5.53e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 964      |\n",
      "|    time_elapsed    | 3360     |\n",
      "|    total_timesteps | 1943424  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1947456, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1947456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014285807 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.6         |\n",
      "|    explained_variance   | -0.0823     |\n",
      "|    learning_rate        | 0.00176     |\n",
      "|    loss                 | 0.00389     |\n",
      "|    n_updates            | 9650        |\n",
      "|    policy_gradient_loss | 0.0255      |\n",
      "|    std                  | 0.00227     |\n",
      "|    value_loss           | 4.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 966      |\n",
      "|    time_elapsed    | 3368     |\n",
      "|    total_timesteps | 1947456  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1951488, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0141    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1951488    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09083931 |\n",
      "|    clip_fraction        | 0.329      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.57       |\n",
      "|    explained_variance   | -0.0576    |\n",
      "|    learning_rate        | 0.00175    |\n",
      "|    loss                 | 0.0789     |\n",
      "|    n_updates            | 9670       |\n",
      "|    policy_gradient_loss | 0.0298     |\n",
      "|    std                  | 0.00233    |\n",
      "|    value_loss           | 4.84e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 968      |\n",
      "|    time_elapsed    | 3375     |\n",
      "|    total_timesteps | 1951488  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0113     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012152992 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0698     |\n",
      "|    learning_rate        | 0.00174     |\n",
      "|    loss                 | 0.00912     |\n",
      "|    n_updates            | 9690        |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 3.6e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 970      |\n",
      "|    time_elapsed    | 3382     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1959552, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1959552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020388933 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00174     |\n",
      "|    loss                 | 0.00278     |\n",
      "|    n_updates            | 9710        |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 4.81e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 972      |\n",
      "|    time_elapsed    | 3389     |\n",
      "|    total_timesteps | 1959552  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1963584, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1963584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009791677 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | -0.108      |\n",
      "|    learning_rate        | 0.00173     |\n",
      "|    loss                 | 0.00851     |\n",
      "|    n_updates            | 9730        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 4.08e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 974      |\n",
      "|    time_elapsed    | 3397     |\n",
      "|    total_timesteps | 1963584  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1967616, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.014      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1967616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008045173 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | -0.0147     |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | 0.00421     |\n",
      "|    n_updates            | 9750        |\n",
      "|    policy_gradient_loss | 0.0204      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 2.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 976      |\n",
      "|    time_elapsed    | 3404     |\n",
      "|    total_timesteps | 1967616  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1971648, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.016      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1971648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059235193 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | 0.0623      |\n",
      "|    n_updates            | 9770        |\n",
      "|    policy_gradient_loss | 0.0175      |\n",
      "|    std                  | 0.00229     |\n",
      "|    value_loss           | 4.2e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 978      |\n",
      "|    time_elapsed    | 3411     |\n",
      "|    total_timesteps | 1971648  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0135    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1975680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09233345 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | -0.0921    |\n",
      "|    learning_rate        | 0.00171    |\n",
      "|    loss                 | 0.0326     |\n",
      "|    n_updates            | 9790       |\n",
      "|    policy_gradient_loss | 0.0174     |\n",
      "|    std                  | 0.00232    |\n",
      "|    value_loss           | 3.79e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 980      |\n",
      "|    time_elapsed    | 3419     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1979712, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0169    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1979712    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06409581 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.57       |\n",
      "|    explained_variance   | -0.103     |\n",
      "|    learning_rate        | 0.0017     |\n",
      "|    loss                 | 0.0979     |\n",
      "|    n_updates            | 9810       |\n",
      "|    policy_gradient_loss | 0.0291     |\n",
      "|    std                  | 0.00227    |\n",
      "|    value_loss           | 2.89e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 982      |\n",
      "|    time_elapsed    | 3426     |\n",
      "|    total_timesteps | 1979712  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1983744, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.012     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1983744    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07003053 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.61       |\n",
      "|    explained_variance   | -0.106     |\n",
      "|    learning_rate        | 0.0017     |\n",
      "|    loss                 | 0.0101     |\n",
      "|    n_updates            | 9830       |\n",
      "|    policy_gradient_loss | 0.0137     |\n",
      "|    std                  | 0.00221    |\n",
      "|    value_loss           | 2.6e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 984      |\n",
      "|    time_elapsed    | 3433     |\n",
      "|    total_timesteps | 1983744  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1987776, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0134     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1987776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008077437 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.0487     |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | 0.000544    |\n",
      "|    n_updates            | 9850        |\n",
      "|    policy_gradient_loss | 0.0169      |\n",
      "|    std                  | 0.00218     |\n",
      "|    value_loss           | 6.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0131  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 986      |\n",
      "|    time_elapsed    | 3441     |\n",
      "|    total_timesteps | 1987776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1991808, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1991808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030654766 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.00168     |\n",
      "|    loss                 | 0.00967     |\n",
      "|    n_updates            | 9870        |\n",
      "|    policy_gradient_loss | 0.00778     |\n",
      "|    std                  | 0.00217     |\n",
      "|    value_loss           | 2.43e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 988      |\n",
      "|    time_elapsed    | 3448     |\n",
      "|    total_timesteps | 1991808  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0108     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015922714 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.00168     |\n",
      "|    loss                 | 0.00837     |\n",
      "|    n_updates            | 9890        |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    std                  | 0.00224     |\n",
      "|    value_loss           | 1.74e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 990      |\n",
      "|    time_elapsed    | 3455     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1999872, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0128     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1999872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023992624 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.00167     |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 9910        |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    std                  | 0.00226     |\n",
      "|    value_loss           | 1.7e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 992      |\n",
      "|    time_elapsed    | 3462     |\n",
      "|    total_timesteps | 1999872  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2003904, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0133    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2003904    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07056061 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.58       |\n",
      "|    explained_variance   | -0.0729    |\n",
      "|    learning_rate        | 0.00166    |\n",
      "|    loss                 | 0.00115    |\n",
      "|    n_updates            | 9930       |\n",
      "|    policy_gradient_loss | 0.0265     |\n",
      "|    std                  | 0.00223    |\n",
      "|    value_loss           | 3.45e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 994      |\n",
      "|    time_elapsed    | 3469     |\n",
      "|    total_timesteps | 2003904  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2007936, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0232     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2007936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080144644 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0559     |\n",
      "|    learning_rate        | 0.00166     |\n",
      "|    loss                 | 0.0044      |\n",
      "|    n_updates            | 9950        |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    std                  | 0.00227     |\n",
      "|    value_loss           | 3.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 996      |\n",
      "|    time_elapsed    | 3476     |\n",
      "|    total_timesteps | 2007936  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2011968, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2011968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025704876 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.55        |\n",
      "|    explained_variance   | -0.0601     |\n",
      "|    learning_rate        | 0.00165     |\n",
      "|    loss                 | 0.0021      |\n",
      "|    n_updates            | 9970        |\n",
      "|    policy_gradient_loss | 0.00972     |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 6.29e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 998      |\n",
      "|    time_elapsed    | 3482     |\n",
      "|    total_timesteps | 2011968  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0151     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014115579 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.55        |\n",
      "|    explained_variance   | 0.00191     |\n",
      "|    learning_rate        | 0.00164     |\n",
      "|    loss                 | -0.00122    |\n",
      "|    n_updates            | 9990        |\n",
      "|    policy_gradient_loss | 0.00408     |\n",
      "|    std                  | 0.0023      |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 3489     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020032, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0151     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2020032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018063497 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.49        |\n",
      "|    explained_variance   | -0.00174    |\n",
      "|    learning_rate        | 0.00164     |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 10010       |\n",
      "|    policy_gradient_loss | 0.00881     |\n",
      "|    std                  | 0.00237     |\n",
      "|    value_loss           | 9.26e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1002     |\n",
      "|    time_elapsed    | 3496     |\n",
      "|    total_timesteps | 2020032  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2024064, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0145    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2024064    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06650223 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | 0.0146     |\n",
      "|    learning_rate        | 0.00163    |\n",
      "|    loss                 | 0.00291    |\n",
      "|    n_updates            | 10030      |\n",
      "|    policy_gradient_loss | 0.011      |\n",
      "|    std                  | 0.00237    |\n",
      "|    value_loss           | 3.06e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1004     |\n",
      "|    time_elapsed    | 3503     |\n",
      "|    total_timesteps | 2024064  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2028096, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.013     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2028096    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03203382 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.48       |\n",
      "|    explained_variance   | -0.0568    |\n",
      "|    learning_rate        | 0.00162    |\n",
      "|    loss                 | -0.00186   |\n",
      "|    n_updates            | 10050      |\n",
      "|    policy_gradient_loss | 0.014      |\n",
      "|    std                  | 0.00236    |\n",
      "|    value_loss           | 3.74e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1006     |\n",
      "|    time_elapsed    | 3510     |\n",
      "|    total_timesteps | 2028096  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2032128, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2032128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016687106 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.49        |\n",
      "|    explained_variance   | -0.061      |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | -0.0048     |\n",
      "|    n_updates            | 10070       |\n",
      "|    policy_gradient_loss | 0.0264      |\n",
      "|    std                  | 0.00236     |\n",
      "|    value_loss           | 3.86e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1008     |\n",
      "|    time_elapsed    | 3517     |\n",
      "|    total_timesteps | 2032128  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011267226 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.49        |\n",
      "|    explained_variance   | -0.0702     |\n",
      "|    learning_rate        | 0.00161     |\n",
      "|    loss                 | 0.00527     |\n",
      "|    n_updates            | 10090       |\n",
      "|    policy_gradient_loss | 0.0192      |\n",
      "|    std                  | 0.00235     |\n",
      "|    value_loss           | 2.19e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1010     |\n",
      "|    time_elapsed    | 3524     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040192, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2040192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029758656 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.51        |\n",
      "|    explained_variance   | 0.0526      |\n",
      "|    learning_rate        | 0.0016      |\n",
      "|    loss                 | -0.00187    |\n",
      "|    n_updates            | 10110       |\n",
      "|    policy_gradient_loss | 0.00799     |\n",
      "|    std                  | 0.0023      |\n",
      "|    value_loss           | 1.76e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1012     |\n",
      "|    time_elapsed    | 3531     |\n",
      "|    total_timesteps | 2040192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2044224, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2044224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014030344 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.55        |\n",
      "|    explained_variance   | -0.0674     |\n",
      "|    learning_rate        | 0.0016      |\n",
      "|    loss                 | -0.00444    |\n",
      "|    n_updates            | 10130       |\n",
      "|    policy_gradient_loss | 0.00692     |\n",
      "|    std                  | 0.00226     |\n",
      "|    value_loss           | 2.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1014     |\n",
      "|    time_elapsed    | 3537     |\n",
      "|    total_timesteps | 2044224  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2048256, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0129    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2048256    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10704226 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | -0.0296    |\n",
      "|    learning_rate        | 0.00159    |\n",
      "|    loss                 | 0.025      |\n",
      "|    n_updates            | 10150      |\n",
      "|    policy_gradient_loss | 0.0316     |\n",
      "|    std                  | 0.00225    |\n",
      "|    value_loss           | 5.06e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1016     |\n",
      "|    time_elapsed    | 3544     |\n",
      "|    total_timesteps | 2048256  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2052288, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2052288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021116346 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0839     |\n",
      "|    learning_rate        | 0.00158     |\n",
      "|    loss                 | 0.000935    |\n",
      "|    n_updates            | 10170       |\n",
      "|    policy_gradient_loss | 0.0181      |\n",
      "|    std                  | 0.00224     |\n",
      "|    value_loss           | 2.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1018     |\n",
      "|    time_elapsed    | 3551     |\n",
      "|    total_timesteps | 2052288  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0126    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2056320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02299257 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.00158    |\n",
      "|    loss                 | 0.0132     |\n",
      "|    n_updates            | 10190      |\n",
      "|    policy_gradient_loss | 0.00725    |\n",
      "|    std                  | 0.00228    |\n",
      "|    value_loss           | 3.51e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1020     |\n",
      "|    time_elapsed    | 3558     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060352, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2060352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023150533 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.43        |\n",
      "|    explained_variance   | -0.12       |\n",
      "|    learning_rate        | 0.00157     |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 10210       |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    std                  | 0.00236     |\n",
      "|    value_loss           | 2.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1022     |\n",
      "|    time_elapsed    | 3565     |\n",
      "|    total_timesteps | 2060352  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2064384, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.014       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2064384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144709945 |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.42         |\n",
      "|    explained_variance   | -0.138       |\n",
      "|    learning_rate        | 0.00156      |\n",
      "|    loss                 | 0.01         |\n",
      "|    n_updates            | 10230        |\n",
      "|    policy_gradient_loss | 0.0137       |\n",
      "|    std                  | 0.00235      |\n",
      "|    value_loss           | 2.67e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1024     |\n",
      "|    time_elapsed    | 3572     |\n",
      "|    total_timesteps | 2064384  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2068416, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0184   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2068416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0285757 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.41      |\n",
      "|    explained_variance   | 0.128     |\n",
      "|    learning_rate        | 0.00156   |\n",
      "|    loss                 | 0.0129    |\n",
      "|    n_updates            | 10250     |\n",
      "|    policy_gradient_loss | 0.00581   |\n",
      "|    std                  | 0.00234   |\n",
      "|    value_loss           | 6.19e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1026     |\n",
      "|    time_elapsed    | 3579     |\n",
      "|    total_timesteps | 2068416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2072448, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0163     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2072448     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014511053 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.36        |\n",
      "|    explained_variance   | 0.0637      |\n",
      "|    learning_rate        | 0.00155     |\n",
      "|    loss                 | 0.00573     |\n",
      "|    n_updates            | 10270       |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    std                  | 0.00242     |\n",
      "|    value_loss           | 3.15e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1028     |\n",
      "|    time_elapsed    | 3585     |\n",
      "|    total_timesteps | 2072448  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011867366 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.36        |\n",
      "|    explained_variance   | 0.0132      |\n",
      "|    learning_rate        | 0.00154     |\n",
      "|    loss                 | 0.000737    |\n",
      "|    n_updates            | 10290       |\n",
      "|    policy_gradient_loss | 0.00456     |\n",
      "|    std                  | 0.0024      |\n",
      "|    value_loss           | 9.8e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1030     |\n",
      "|    time_elapsed    | 3593     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080512, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2080512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024544515 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.44        |\n",
      "|    explained_variance   | -0.0741     |\n",
      "|    learning_rate        | 0.00154     |\n",
      "|    loss                 | 0.00832     |\n",
      "|    n_updates            | 10310       |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 2.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1032     |\n",
      "|    time_elapsed    | 3599     |\n",
      "|    total_timesteps | 2080512  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2084544, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.014       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2084544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072623645 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.43         |\n",
      "|    explained_variance   | -0.0633      |\n",
      "|    learning_rate        | 0.00153      |\n",
      "|    loss                 | -0.00493     |\n",
      "|    n_updates            | 10330        |\n",
      "|    policy_gradient_loss | 0.00748      |\n",
      "|    std                  | 0.00232      |\n",
      "|    value_loss           | 3.57e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1034     |\n",
      "|    time_elapsed    | 3606     |\n",
      "|    total_timesteps | 2084544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2088576, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0114     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2088576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004007475 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.46        |\n",
      "|    explained_variance   | -0.0392     |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | 0.0125      |\n",
      "|    n_updates            | 10350       |\n",
      "|    policy_gradient_loss | 0.00949     |\n",
      "|    std                  | 0.00229     |\n",
      "|    value_loss           | 6.54e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1036     |\n",
      "|    time_elapsed    | 3613     |\n",
      "|    total_timesteps | 2088576  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2092608, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0134    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2092608    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02040397 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.47       |\n",
      "|    explained_variance   | 0.141      |\n",
      "|    learning_rate        | 0.00152    |\n",
      "|    loss                 | 0.0245     |\n",
      "|    n_updates            | 10370      |\n",
      "|    policy_gradient_loss | 0.00458    |\n",
      "|    std                  | 0.00227    |\n",
      "|    value_loss           | 1.73e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1038     |\n",
      "|    time_elapsed    | 3620     |\n",
      "|    total_timesteps | 2092608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037131574 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.5         |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.00151     |\n",
      "|    loss                 | 0.000496    |\n",
      "|    n_updates            | 10390       |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 4.67e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1040     |\n",
      "|    time_elapsed    | 3627     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100672, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0201     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2100672     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045395214 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.0086     |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | 0.00864     |\n",
      "|    n_updates            | 10410       |\n",
      "|    policy_gradient_loss | 0.0255      |\n",
      "|    std                  | 0.00222     |\n",
      "|    value_loss           | 1.67e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0134  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1042     |\n",
      "|    time_elapsed    | 3634     |\n",
      "|    total_timesteps | 2100672  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2104704, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2104704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030102909 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.0859     |\n",
      "|    learning_rate        | 0.0015      |\n",
      "|    loss                 | 0.00864     |\n",
      "|    n_updates            | 10430       |\n",
      "|    policy_gradient_loss | 0.0186      |\n",
      "|    std                  | 0.00222     |\n",
      "|    value_loss           | 6.87e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1044     |\n",
      "|    time_elapsed    | 3640     |\n",
      "|    total_timesteps | 2104704  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2108736, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0126    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2108736    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03269639 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.53       |\n",
      "|    explained_variance   | 0.119      |\n",
      "|    learning_rate        | 0.00149    |\n",
      "|    loss                 | 0.0363     |\n",
      "|    n_updates            | 10450      |\n",
      "|    policy_gradient_loss | 0.0297     |\n",
      "|    std                  | 0.00222    |\n",
      "|    value_loss           | 2.53e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1046     |\n",
      "|    time_elapsed    | 3648     |\n",
      "|    total_timesteps | 2108736  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2112768, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0125    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2112768    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10867906 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.52       |\n",
      "|    explained_variance   | -0.0814    |\n",
      "|    learning_rate        | 0.00148    |\n",
      "|    loss                 | 0.0208     |\n",
      "|    n_updates            | 10470      |\n",
      "|    policy_gradient_loss | 0.0261     |\n",
      "|    std                  | 0.00223    |\n",
      "|    value_loss           | 2.85e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1048     |\n",
      "|    time_elapsed    | 3654     |\n",
      "|    total_timesteps | 2112768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0175   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2116800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0716373 |\n",
      "|    clip_fraction        | 0.23      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.53      |\n",
      "|    explained_variance   | -0.0454   |\n",
      "|    learning_rate        | 0.00148   |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 10490     |\n",
      "|    policy_gradient_loss | 0.0143    |\n",
      "|    std                  | 0.00219   |\n",
      "|    value_loss           | 3.61e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1050     |\n",
      "|    time_elapsed    | 3661     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120832, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0116    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2120832    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01234633 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.55       |\n",
      "|    explained_variance   | -0.0481    |\n",
      "|    learning_rate        | 0.00147    |\n",
      "|    loss                 | -0.00396   |\n",
      "|    n_updates            | 10510      |\n",
      "|    policy_gradient_loss | 0.015      |\n",
      "|    std                  | 0.00218    |\n",
      "|    value_loss           | 3.34e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1052     |\n",
      "|    time_elapsed    | 3668     |\n",
      "|    total_timesteps | 2120832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2124864, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 251      |\n",
      "|    mean_reward          | -0.0132  |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 2124864  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.072293 |\n",
      "|    clip_fraction        | 0.372    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 9.55     |\n",
      "|    explained_variance   | 0.0241   |\n",
      "|    learning_rate        | 0.00146  |\n",
      "|    loss                 | 0.0294   |\n",
      "|    n_updates            | 10530    |\n",
      "|    policy_gradient_loss | 0.0167   |\n",
      "|    std                  | 0.00218  |\n",
      "|    value_loss           | 1.02e-07 |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1054     |\n",
      "|    time_elapsed    | 3675     |\n",
      "|    total_timesteps | 2124864  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2128896, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0202     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2128896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013967739 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0372     |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | 0.00665     |\n",
      "|    n_updates            | 10550       |\n",
      "|    policy_gradient_loss | 0.00516     |\n",
      "|    std                  | 0.00217     |\n",
      "|    value_loss           | 3.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1056     |\n",
      "|    time_elapsed    | 3682     |\n",
      "|    total_timesteps | 2128896  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2132928, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0141     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2132928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061437458 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.54        |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00145     |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 10570       |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.00218     |\n",
      "|    value_loss           | 4.96e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1058     |\n",
      "|    time_elapsed    | 3689     |\n",
      "|    total_timesteps | 2132928  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023217171 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.54        |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.00144     |\n",
      "|    loss                 | 0.0163      |\n",
      "|    n_updates            | 10590       |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    std                  | 0.0022      |\n",
      "|    value_loss           | 5.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1060     |\n",
      "|    time_elapsed    | 3695     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140992, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0136    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2140992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09153379 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | -0.025     |\n",
      "|    learning_rate        | 0.00144    |\n",
      "|    loss                 | 0.032      |\n",
      "|    n_updates            | 10610      |\n",
      "|    policy_gradient_loss | 0.0191     |\n",
      "|    std                  | 0.00218    |\n",
      "|    value_loss           | 3.44e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1062     |\n",
      "|    time_elapsed    | 3702     |\n",
      "|    total_timesteps | 2140992  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2145024, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0156     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2145024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007348704 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0519     |\n",
      "|    learning_rate        | 0.00143     |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 10630       |\n",
      "|    policy_gradient_loss | 0.00501     |\n",
      "|    std                  | 0.00215     |\n",
      "|    value_loss           | 2.48e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1064     |\n",
      "|    time_elapsed    | 3709     |\n",
      "|    total_timesteps | 2145024  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2149056, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0158     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2149056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011948831 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.0304     |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | 0.00276     |\n",
      "|    n_updates            | 10650       |\n",
      "|    policy_gradient_loss | 0.00989     |\n",
      "|    std                  | 0.00215     |\n",
      "|    value_loss           | 1.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0155  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1066     |\n",
      "|    time_elapsed    | 3716     |\n",
      "|    total_timesteps | 2149056  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2153088, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0204    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2153088    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06523029 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | -0.0202    |\n",
      "|    learning_rate        | 0.00141    |\n",
      "|    loss                 | 0.0342     |\n",
      "|    n_updates            | 10670      |\n",
      "|    policy_gradient_loss | 0.0174     |\n",
      "|    std                  | 0.00218    |\n",
      "|    value_loss           | 6.02e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1068     |\n",
      "|    time_elapsed    | 3723     |\n",
      "|    total_timesteps | 2153088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0141     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011694929 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.54        |\n",
      "|    explained_variance   | -0.0674     |\n",
      "|    learning_rate        | 0.00141     |\n",
      "|    loss                 | -0.00319    |\n",
      "|    n_updates            | 10690       |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    std                  | 0.00219     |\n",
      "|    value_loss           | 3.12e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0155  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1070     |\n",
      "|    time_elapsed    | 3730     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2161152, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2161152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019187594 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.51        |\n",
      "|    explained_variance   | 0.095       |\n",
      "|    learning_rate        | 0.0014      |\n",
      "|    loss                 | -0.00342    |\n",
      "|    n_updates            | 10710       |\n",
      "|    policy_gradient_loss | 0.00594     |\n",
      "|    std                  | 0.00224     |\n",
      "|    value_loss           | 1.31e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0161  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1072     |\n",
      "|    time_elapsed    | 3736     |\n",
      "|    total_timesteps | 2161152  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2165184, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0191     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2165184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027943479 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.47        |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.00885     |\n",
      "|    n_updates            | 10730       |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 2.36e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1074     |\n",
      "|    time_elapsed    | 3743     |\n",
      "|    total_timesteps | 2165184  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2169216, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0144    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2169216    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04957874 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.46       |\n",
      "|    explained_variance   | 0.00508    |\n",
      "|    learning_rate        | 0.00139    |\n",
      "|    loss                 | -0.00398   |\n",
      "|    n_updates            | 10750      |\n",
      "|    policy_gradient_loss | 0.00523    |\n",
      "|    std                  | 0.00229    |\n",
      "|    value_loss           | 2.22e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1076     |\n",
      "|    time_elapsed    | 3750     |\n",
      "|    total_timesteps | 2169216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2173248, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0154     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2173248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024011984 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.46        |\n",
      "|    explained_variance   | -0.0668     |\n",
      "|    learning_rate        | 0.00138     |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 10770       |\n",
      "|    policy_gradient_loss | 0.0134      |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 5.1e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1078     |\n",
      "|    time_elapsed    | 3757     |\n",
      "|    total_timesteps | 2173248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0141     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021684552 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.46        |\n",
      "|    explained_variance   | -0.0154     |\n",
      "|    learning_rate        | 0.00137     |\n",
      "|    loss                 | -0.00362    |\n",
      "|    n_updates            | 10790       |\n",
      "|    policy_gradient_loss | 0.00942     |\n",
      "|    std                  | 0.00231     |\n",
      "|    value_loss           | 2.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1080     |\n",
      "|    time_elapsed    | 3764     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2181312, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0219    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2181312    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09304231 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.43       |\n",
      "|    explained_variance   | 0.0737     |\n",
      "|    learning_rate        | 0.00137    |\n",
      "|    loss                 | 0.0471     |\n",
      "|    n_updates            | 10810      |\n",
      "|    policy_gradient_loss | 0.00728    |\n",
      "|    std                  | 0.00234    |\n",
      "|    value_loss           | 4.04e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0165  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1082     |\n",
      "|    time_elapsed    | 3771     |\n",
      "|    total_timesteps | 2181312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2185344, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2185344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013499547 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.44        |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.00136     |\n",
      "|    loss                 | 0.0065      |\n",
      "|    n_updates            | 10830       |\n",
      "|    policy_gradient_loss | 0.00894     |\n",
      "|    std                  | 0.00234     |\n",
      "|    value_loss           | 1.11e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0165  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1084     |\n",
      "|    time_elapsed    | 3778     |\n",
      "|    total_timesteps | 2185344  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2189376, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0138    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2189376    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01836652 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.42       |\n",
      "|    explained_variance   | 0.0308     |\n",
      "|    learning_rate        | 0.00135    |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 10850      |\n",
      "|    policy_gradient_loss | 0.00949    |\n",
      "|    std                  | 0.00235    |\n",
      "|    value_loss           | 3.65e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0159  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1086     |\n",
      "|    time_elapsed    | 3785     |\n",
      "|    total_timesteps | 2189376  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2193408, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2193408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046109933 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.42        |\n",
      "|    explained_variance   | 0.0646      |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | 0.026       |\n",
      "|    n_updates            | 10870       |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    std                  | 0.00234     |\n",
      "|    value_loss           | 1.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1088     |\n",
      "|    time_elapsed    | 3791     |\n",
      "|    total_timesteps | 2193408  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0148    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2197440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01330601 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | -0.0319    |\n",
      "|    learning_rate        | 0.00134    |\n",
      "|    loss                 | 0.0138     |\n",
      "|    n_updates            | 10890      |\n",
      "|    policy_gradient_loss | 0.00609    |\n",
      "|    std                  | 0.00236    |\n",
      "|    value_loss           | 3.48e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1090     |\n",
      "|    time_elapsed    | 3798     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2201472, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0192    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2201472    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00774979 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.39       |\n",
      "|    explained_variance   | -0.105     |\n",
      "|    learning_rate        | 0.00133    |\n",
      "|    loss                 | -0.00957   |\n",
      "|    n_updates            | 10910      |\n",
      "|    policy_gradient_loss | 0.000572   |\n",
      "|    std                  | 0.00237    |\n",
      "|    value_loss           | 2.06e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1092     |\n",
      "|    time_elapsed    | 3805     |\n",
      "|    total_timesteps | 2201472  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2205504, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0157     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2205504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013690641 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | -0.0838     |\n",
      "|    learning_rate        | 0.00133     |\n",
      "|    loss                 | 0.00505     |\n",
      "|    n_updates            | 10930       |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    std                  | 0.00239     |\n",
      "|    value_loss           | 3.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1094     |\n",
      "|    time_elapsed    | 3813     |\n",
      "|    total_timesteps | 2205504  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2209536, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2209536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006723378 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | -0.0041     |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00926    |\n",
      "|    n_updates            | 10950       |\n",
      "|    policy_gradient_loss | 0.00859     |\n",
      "|    std                  | 0.0024      |\n",
      "|    value_loss           | 3.53e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1096     |\n",
      "|    time_elapsed    | 3821     |\n",
      "|    total_timesteps | 2209536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2213568, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0184     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2213568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024397716 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.4         |\n",
      "|    explained_variance   | -0.0478     |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | -0.00195    |\n",
      "|    n_updates            | 10970       |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    std                  | 0.00238     |\n",
      "|    value_loss           | 3.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1098     |\n",
      "|    time_elapsed    | 3829     |\n",
      "|    total_timesteps | 2213568  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014854681 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.38        |\n",
      "|    explained_variance   | -0.0936     |\n",
      "|    learning_rate        | 0.00131     |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 10990       |\n",
      "|    policy_gradient_loss | 0.00825     |\n",
      "|    std                  | 0.00238     |\n",
      "|    value_loss           | 4.35e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1100     |\n",
      "|    time_elapsed    | 3835     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2221632, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0151     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2221632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027267277 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | -0.0593     |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | -0.00508    |\n",
      "|    n_updates            | 11010       |\n",
      "|    policy_gradient_loss | 0.00816     |\n",
      "|    std                  | 0.00239     |\n",
      "|    value_loss           | 4.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1102     |\n",
      "|    time_elapsed    | 3842     |\n",
      "|    total_timesteps | 2221632  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2225664, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0106    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2225664    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06842448 |\n",
      "|    clip_fraction        | 0.388      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | -0.117     |\n",
      "|    learning_rate        | 0.00129    |\n",
      "|    loss                 | 0.0143     |\n",
      "|    n_updates            | 11030      |\n",
      "|    policy_gradient_loss | 0.0325     |\n",
      "|    std                  | 0.00239    |\n",
      "|    value_loss           | 3.34e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1104     |\n",
      "|    time_elapsed    | 3849     |\n",
      "|    total_timesteps | 2225664  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2229696, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0156      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2229696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058959182 |\n",
      "|    clip_fraction        | 0.305        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.4          |\n",
      "|    explained_variance   | 0.192        |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | 0.00704      |\n",
      "|    n_updates            | 11050        |\n",
      "|    policy_gradient_loss | 0.0121       |\n",
      "|    std                  | 0.0024       |\n",
      "|    value_loss           | 1.56e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1106     |\n",
      "|    time_elapsed    | 3856     |\n",
      "|    total_timesteps | 2229696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2233728, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2233728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012794792 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | -0.0409     |\n",
      "|    learning_rate        | 0.00128     |\n",
      "|    loss                 | 0.00249     |\n",
      "|    n_updates            | 11070       |\n",
      "|    policy_gradient_loss | 0.00891     |\n",
      "|    std                  | 0.00239     |\n",
      "|    value_loss           | 3.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1108     |\n",
      "|    time_elapsed    | 3863     |\n",
      "|    total_timesteps | 2233728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025749879 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.41        |\n",
      "|    explained_variance   | -0.0872     |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 11090       |\n",
      "|    policy_gradient_loss | 0.00779     |\n",
      "|    std                  | 0.00237     |\n",
      "|    value_loss           | 4.6e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1110     |\n",
      "|    time_elapsed    | 3869     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2241792, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.012     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2241792    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07762712 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.38       |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.00127    |\n",
      "|    loss                 | 0.0358     |\n",
      "|    n_updates            | 11110      |\n",
      "|    policy_gradient_loss | 0.012      |\n",
      "|    std                  | 0.00238    |\n",
      "|    value_loss           | 3.94e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1112     |\n",
      "|    time_elapsed    | 3876     |\n",
      "|    total_timesteps | 2241792  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2245824, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2245824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012654142 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | -0.0934     |\n",
      "|    learning_rate        | 0.00126     |\n",
      "|    loss                 | 0.00632     |\n",
      "|    n_updates            | 11130       |\n",
      "|    policy_gradient_loss | 0.00429     |\n",
      "|    std                  | 0.00238     |\n",
      "|    value_loss           | 6.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1114     |\n",
      "|    time_elapsed    | 3883     |\n",
      "|    total_timesteps | 2245824  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2249856, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0135     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2249856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009075463 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.4         |\n",
      "|    explained_variance   | -0.0643     |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 11150       |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00237     |\n",
      "|    value_loss           | 5.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1116     |\n",
      "|    time_elapsed    | 3890     |\n",
      "|    total_timesteps | 2249856  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2253888, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2253888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012402379 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.42        |\n",
      "|    explained_variance   | 0.0457      |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | 0.00476     |\n",
      "|    n_updates            | 11170       |\n",
      "|    policy_gradient_loss | 0.00869     |\n",
      "|    std                  | 0.00233     |\n",
      "|    value_loss           | 6.83e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1118     |\n",
      "|    time_elapsed    | 3897     |\n",
      "|    total_timesteps | 2253888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014597448 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.46        |\n",
      "|    explained_variance   | -0.049      |\n",
      "|    learning_rate        | 0.00124     |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 11190       |\n",
      "|    policy_gradient_loss | 0.00703     |\n",
      "|    std                  | 0.00229     |\n",
      "|    value_loss           | 3.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1120     |\n",
      "|    time_elapsed    | 3904     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2261952, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2261952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031274173 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.47        |\n",
      "|    explained_variance   | -0.0408     |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 11210       |\n",
      "|    policy_gradient_loss | 0.0234      |\n",
      "|    std                  | 0.00228     |\n",
      "|    value_loss           | 1.95e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1122     |\n",
      "|    time_elapsed    | 3911     |\n",
      "|    total_timesteps | 2261952  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2265984, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0148     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2265984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015792834 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.51        |\n",
      "|    explained_variance   | 0.0533      |\n",
      "|    learning_rate        | 0.00123     |\n",
      "|    loss                 | 0.000766    |\n",
      "|    n_updates            | 11230       |\n",
      "|    policy_gradient_loss | 0.0153      |\n",
      "|    std                  | 0.00222     |\n",
      "|    value_loss           | 9.73e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1124     |\n",
      "|    time_elapsed    | 3917     |\n",
      "|    total_timesteps | 2265984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270016, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0134     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2270016     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031605955 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.0796     |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 11250       |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.00221     |\n",
      "|    value_loss           | 2.49e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1126     |\n",
      "|    time_elapsed    | 3924     |\n",
      "|    total_timesteps | 2270016  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2274048, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0166      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2274048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053311475 |\n",
      "|    clip_fraction        | 0.257        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.51         |\n",
      "|    explained_variance   | -0.0615      |\n",
      "|    learning_rate        | 0.00121      |\n",
      "|    loss                 | 0.00358      |\n",
      "|    n_updates            | 11270        |\n",
      "|    policy_gradient_loss | 0.011        |\n",
      "|    std                  | 0.00222      |\n",
      "|    value_loss           | 2.94e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1128     |\n",
      "|    time_elapsed    | 3931     |\n",
      "|    total_timesteps | 2274048  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0125    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2278080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01960819 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.53       |\n",
      "|    explained_variance   | 0.0234     |\n",
      "|    learning_rate        | 0.00121    |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 11290      |\n",
      "|    policy_gradient_loss | 0.0126     |\n",
      "|    std                  | 0.0022     |\n",
      "|    value_loss           | 7.35e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1130     |\n",
      "|    time_elapsed    | 3938     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2282112, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2282112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009348136 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | -0.0215     |\n",
      "|    learning_rate        | 0.0012      |\n",
      "|    loss                 | 0.0071      |\n",
      "|    n_updates            | 11310       |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    std                  | 0.00219     |\n",
      "|    value_loss           | 6.15e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1132     |\n",
      "|    time_elapsed    | 3945     |\n",
      "|    total_timesteps | 2282112  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2286144, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0156      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2286144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087790545 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.52         |\n",
      "|    explained_variance   | -0.0476      |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.00672      |\n",
      "|    n_updates            | 11330        |\n",
      "|    policy_gradient_loss | 0.00377      |\n",
      "|    std                  | 0.00222      |\n",
      "|    value_loss           | 3.7e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1134     |\n",
      "|    time_elapsed    | 3952     |\n",
      "|    total_timesteps | 2286144  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290176, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2290176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012754365 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.00966    |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00137     |\n",
      "|    n_updates            | 11350       |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.00221     |\n",
      "|    value_loss           | 6.2e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1136     |\n",
      "|    time_elapsed    | 3959     |\n",
      "|    total_timesteps | 2290176  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2294208, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2294208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022098966 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | -0.00181    |\n",
      "|    learning_rate        | 0.00118     |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 11370       |\n",
      "|    policy_gradient_loss | 0.0098      |\n",
      "|    std                  | 0.00221     |\n",
      "|    value_loss           | 2.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1138     |\n",
      "|    time_elapsed    | 3966     |\n",
      "|    total_timesteps | 2294208  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0127    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2298240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02673547 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.0896     |\n",
      "|    learning_rate        | 0.00117    |\n",
      "|    loss                 | 0.0297     |\n",
      "|    n_updates            | 11390      |\n",
      "|    policy_gradient_loss | 0.00978    |\n",
      "|    std                  | 0.00219    |\n",
      "|    value_loss           | 2.07e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1140     |\n",
      "|    time_elapsed    | 3973     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2302272, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0135     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2302272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017272618 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.000721   |\n",
      "|    n_updates            | 11410       |\n",
      "|    policy_gradient_loss | 0.0079      |\n",
      "|    std                  | 0.00222     |\n",
      "|    value_loss           | 3.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0151  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1142     |\n",
      "|    time_elapsed    | 3980     |\n",
      "|    total_timesteps | 2302272  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2306304, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2306304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048743114 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.0775     |\n",
      "|    learning_rate        | 0.00116     |\n",
      "|    loss                 | 0.0918      |\n",
      "|    n_updates            | 11430       |\n",
      "|    policy_gradient_loss | 0.0113      |\n",
      "|    std                  | 0.00223     |\n",
      "|    value_loss           | 3.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1144     |\n",
      "|    time_elapsed    | 3986     |\n",
      "|    total_timesteps | 2306304  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310336, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2310336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011373758 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | 0.0647      |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | 0.00235     |\n",
      "|    n_updates            | 11450       |\n",
      "|    policy_gradient_loss | 0.00906     |\n",
      "|    std                  | 0.00222     |\n",
      "|    value_loss           | 1.7e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1146     |\n",
      "|    time_elapsed    | 3993     |\n",
      "|    total_timesteps | 2310336  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2314368, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0123    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2314368    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04027611 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | -0.0372    |\n",
      "|    learning_rate        | 0.00115    |\n",
      "|    loss                 | 0.0825     |\n",
      "|    n_updates            | 11470      |\n",
      "|    policy_gradient_loss | 0.018      |\n",
      "|    std                  | 0.00224    |\n",
      "|    value_loss           | 3.74e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1148     |\n",
      "|    time_elapsed    | 4000     |\n",
      "|    total_timesteps | 2314368  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061873145 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.5         |\n",
      "|    explained_variance   | -0.042      |\n",
      "|    learning_rate        | 0.00114     |\n",
      "|    loss                 | 0.0668      |\n",
      "|    n_updates            | 11490       |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00224     |\n",
      "|    value_loss           | 4.17e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1150     |\n",
      "|    time_elapsed    | 4007     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2322432, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0144    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2322432    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01152517 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.49       |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.00113    |\n",
      "|    loss                 | 0.0136     |\n",
      "|    n_updates            | 11510      |\n",
      "|    policy_gradient_loss | 0.0026     |\n",
      "|    std                  | 0.00225    |\n",
      "|    value_loss           | 1.04e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0151  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1152     |\n",
      "|    time_elapsed    | 4014     |\n",
      "|    total_timesteps | 2322432  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2326464, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.014      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2326464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031356145 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.48        |\n",
      "|    explained_variance   | -0.0116     |\n",
      "|    learning_rate        | 0.00113     |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 11530       |\n",
      "|    policy_gradient_loss | 0.00722     |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 1.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1154     |\n",
      "|    time_elapsed    | 4021     |\n",
      "|    total_timesteps | 2326464  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330496, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0196     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2330496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023238257 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.48        |\n",
      "|    explained_variance   | -0.046      |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 11550       |\n",
      "|    policy_gradient_loss | 0.0335      |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 3.01e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1156     |\n",
      "|    time_elapsed    | 4027     |\n",
      "|    total_timesteps | 2330496  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2334528, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2334528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028687544 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.49        |\n",
      "|    explained_variance   | 0.000745    |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 11570       |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.00223     |\n",
      "|    value_loss           | 1.31e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0157  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1158     |\n",
      "|    time_elapsed    | 4034     |\n",
      "|    total_timesteps | 2334528  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0135     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412026 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.47        |\n",
      "|    explained_variance   | 0.00316     |\n",
      "|    learning_rate        | 0.00111     |\n",
      "|    loss                 | -0.00643    |\n",
      "|    n_updates            | 11590       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    std                  | 0.00225     |\n",
      "|    value_loss           | 2.98e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1160     |\n",
      "|    time_elapsed    | 4041     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2342592, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0151      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2342592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084724985 |\n",
      "|    clip_fraction        | 0.271        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.49         |\n",
      "|    explained_variance   | -0.0335      |\n",
      "|    learning_rate        | 0.0011       |\n",
      "|    loss                 | 0.0164       |\n",
      "|    n_updates            | 11610        |\n",
      "|    policy_gradient_loss | 0.0134       |\n",
      "|    std                  | 0.00224      |\n",
      "|    value_loss           | 3.96e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1162     |\n",
      "|    time_elapsed    | 4049     |\n",
      "|    total_timesteps | 2342592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2346624, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2346624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016080108 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.0616     |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.00114     |\n",
      "|    n_updates            | 11630       |\n",
      "|    policy_gradient_loss | 0.00389     |\n",
      "|    std                  | 0.00219     |\n",
      "|    value_loss           | 4.15e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0159  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1164     |\n",
      "|    time_elapsed    | 4056     |\n",
      "|    total_timesteps | 2346624  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350656, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2350656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043078795 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.52        |\n",
      "|    explained_variance   | -0.0562     |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | 0.00736     |\n",
      "|    n_updates            | 11650       |\n",
      "|    policy_gradient_loss | 0.0223      |\n",
      "|    std                  | 0.0022      |\n",
      "|    value_loss           | 2.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.016   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1166     |\n",
      "|    time_elapsed    | 4063     |\n",
      "|    total_timesteps | 2350656  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2354688, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0133    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2354688    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03654859 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.52       |\n",
      "|    explained_variance   | 0.0206     |\n",
      "|    learning_rate        | 0.00108    |\n",
      "|    loss                 | 0.02       |\n",
      "|    n_updates            | 11670      |\n",
      "|    policy_gradient_loss | 0.00589    |\n",
      "|    std                  | 0.00221    |\n",
      "|    value_loss           | 4.03e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1168     |\n",
      "|    time_elapsed    | 4069     |\n",
      "|    total_timesteps | 2354688  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0152    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2358720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02037498 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.0125     |\n",
      "|    learning_rate        | 0.00107    |\n",
      "|    loss                 | -0.00295   |\n",
      "|    n_updates            | 11690      |\n",
      "|    policy_gradient_loss | 0.00323    |\n",
      "|    std                  | 0.00223    |\n",
      "|    value_loss           | 3.75e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0157  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1170     |\n",
      "|    time_elapsed    | 4076     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2362752, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0144      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2362752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049691973 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.51         |\n",
      "|    explained_variance   | 0.0162       |\n",
      "|    learning_rate        | 0.00107      |\n",
      "|    loss                 | 0.00278      |\n",
      "|    n_updates            | 11710        |\n",
      "|    policy_gradient_loss | 0.0077       |\n",
      "|    std                  | 0.00222      |\n",
      "|    value_loss           | 6.15e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0159  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1172     |\n",
      "|    time_elapsed    | 4083     |\n",
      "|    total_timesteps | 2362752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2366784, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0142    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2366784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08311477 |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.53       |\n",
      "|    explained_variance   | 0.0482     |\n",
      "|    learning_rate        | 0.00106    |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 11730      |\n",
      "|    policy_gradient_loss | 0.00857    |\n",
      "|    std                  | 0.00221    |\n",
      "|    value_loss           | 1.97e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1174     |\n",
      "|    time_elapsed    | 4090     |\n",
      "|    total_timesteps | 2366784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370816, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0161     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2370816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018624308 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.53        |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.00559     |\n",
      "|    n_updates            | 11750       |\n",
      "|    policy_gradient_loss | 0.00362     |\n",
      "|    std                  | 0.00221     |\n",
      "|    value_loss           | 5e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.016   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1176     |\n",
      "|    time_elapsed    | 4097     |\n",
      "|    total_timesteps | 2370816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2374848, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2374848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009439086 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | -0.0532     |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | 0.00512     |\n",
      "|    n_updates            | 11770       |\n",
      "|    policy_gradient_loss | 0.00571     |\n",
      "|    std                  | 0.00218     |\n",
      "|    value_loss           | 3.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1178     |\n",
      "|    time_elapsed    | 4104     |\n",
      "|    total_timesteps | 2374848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0148     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004947538 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.0633     |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | 0.00145     |\n",
      "|    n_updates            | 11790       |\n",
      "|    policy_gradient_loss | 0.0132      |\n",
      "|    std                  | 0.00216     |\n",
      "|    value_loss           | 3.43e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1180     |\n",
      "|    time_elapsed    | 4111     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2382912, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0208     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2382912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045333516 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.57        |\n",
      "|    explained_variance   | -0.132      |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.0132      |\n",
      "|    n_updates            | 11810       |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.00216     |\n",
      "|    value_loss           | 2.79e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1182     |\n",
      "|    time_elapsed    | 4117     |\n",
      "|    total_timesteps | 2382912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2386944, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0123     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2386944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011673568 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.6         |\n",
      "|    explained_variance   | -0.0605     |\n",
      "|    learning_rate        | 0.00103     |\n",
      "|    loss                 | 0.000989    |\n",
      "|    n_updates            | 11830       |\n",
      "|    policy_gradient_loss | 0.00695     |\n",
      "|    std                  | 0.00212     |\n",
      "|    value_loss           | 2.97e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1184     |\n",
      "|    time_elapsed    | 4124     |\n",
      "|    total_timesteps | 2386944  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390976, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2390976    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04713671 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | -0.0387    |\n",
      "|    learning_rate        | 0.00102    |\n",
      "|    loss                 | 0.0488     |\n",
      "|    n_updates            | 11850      |\n",
      "|    policy_gradient_loss | 0.0145     |\n",
      "|    std                  | 0.00215    |\n",
      "|    value_loss           | 2.26e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1186     |\n",
      "|    time_elapsed    | 4131     |\n",
      "|    total_timesteps | 2390976  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2395008, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0213     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2395008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046087332 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | -0.0516     |\n",
      "|    learning_rate        | 0.00101     |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 11870       |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.00214     |\n",
      "|    value_loss           | 4.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1188     |\n",
      "|    time_elapsed    | 4138     |\n",
      "|    total_timesteps | 2395008  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0147     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021632496 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | -0.0256     |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.00016     |\n",
      "|    n_updates            | 11890       |\n",
      "|    policy_gradient_loss | 0.0071      |\n",
      "|    std                  | 0.00216     |\n",
      "|    value_loss           | 2.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1190     |\n",
      "|    time_elapsed    | 4145     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2403072, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2403072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008929133 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | -0.0752     |\n",
      "|    learning_rate        | 0.000998    |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 11910       |\n",
      "|    policy_gradient_loss | 0.00171     |\n",
      "|    std                  | 0.00215     |\n",
      "|    value_loss           | 4.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1192     |\n",
      "|    time_elapsed    | 4152     |\n",
      "|    total_timesteps | 2403072  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2407104, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0138     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2407104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029819336 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.61        |\n",
      "|    explained_variance   | 0.0402      |\n",
      "|    learning_rate        | 0.000992    |\n",
      "|    loss                 | 0.00154     |\n",
      "|    n_updates            | 11930       |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    std                  | 0.00213     |\n",
      "|    value_loss           | 6.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1194     |\n",
      "|    time_elapsed    | 4159     |\n",
      "|    total_timesteps | 2407104  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2411136, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2411136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011115154 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.62        |\n",
      "|    explained_variance   | -0.0665     |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 11950       |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.00211     |\n",
      "|    value_loss           | 3.34e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1196     |\n",
      "|    time_elapsed    | 4166     |\n",
      "|    total_timesteps | 2411136  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2415168, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0144    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2415168    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01536813 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.6        |\n",
      "|    explained_variance   | -0.092     |\n",
      "|    learning_rate        | 0.000978   |\n",
      "|    loss                 | -0.00112   |\n",
      "|    n_updates            | 11970      |\n",
      "|    policy_gradient_loss | 0.00574    |\n",
      "|    std                  | 0.00213    |\n",
      "|    value_loss           | 5.22e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0157  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1198     |\n",
      "|    time_elapsed    | 4173     |\n",
      "|    total_timesteps | 2415168  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009413478 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | 0.0292      |\n",
      "|    learning_rate        | 0.000971    |\n",
      "|    loss                 | 0.00267     |\n",
      "|    n_updates            | 11990       |\n",
      "|    policy_gradient_loss | 0.000798    |\n",
      "|    std                  | 0.00211     |\n",
      "|    value_loss           | 9.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0162  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 4180     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2423232, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0185     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2423232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007129151 |\n",
      "|    clip_fraction        | 0.0973      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.59        |\n",
      "|    explained_variance   | -0.00426    |\n",
      "|    learning_rate        | 0.000965    |\n",
      "|    loss                 | 0.00586     |\n",
      "|    n_updates            | 12010       |\n",
      "|    policy_gradient_loss | 0.00139     |\n",
      "|    std                  | 0.00212     |\n",
      "|    value_loss           | 6.02e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0169  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1202     |\n",
      "|    time_elapsed    | 4186     |\n",
      "|    total_timesteps | 2423232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2427264, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2427264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015885826 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.62        |\n",
      "|    explained_variance   | -0.0532     |\n",
      "|    learning_rate        | 0.000958    |\n",
      "|    loss                 | 0.0068      |\n",
      "|    n_updates            | 12030       |\n",
      "|    policy_gradient_loss | 0.00846     |\n",
      "|    std                  | 0.00208     |\n",
      "|    value_loss           | 8.11e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1204     |\n",
      "|    time_elapsed    | 4193     |\n",
      "|    total_timesteps | 2427264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2431296, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0134     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2431296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009452491 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.62        |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.000951    |\n",
      "|    loss                 | -0.00269    |\n",
      "|    n_updates            | 12050       |\n",
      "|    policy_gradient_loss | 0.00536     |\n",
      "|    std                  | 0.00209     |\n",
      "|    value_loss           | 7.47e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0162  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1206     |\n",
      "|    time_elapsed    | 4200     |\n",
      "|    total_timesteps | 2431296  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2435328, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0156    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2435328    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03928338 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.65       |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.000944   |\n",
      "|    loss                 | 0.00957    |\n",
      "|    n_updates            | 12070      |\n",
      "|    policy_gradient_loss | 0.014      |\n",
      "|    std                  | 0.00205    |\n",
      "|    value_loss           | 3.16e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0166  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1208     |\n",
      "|    time_elapsed    | 4207     |\n",
      "|    total_timesteps | 2435328  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0138     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028193835 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.0959     |\n",
      "|    learning_rate        | 0.000938    |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 12090       |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.00207     |\n",
      "|    value_loss           | 3.08e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0157  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1210     |\n",
      "|    time_elapsed    | 4214     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2443392, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2443392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009181957 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.65        |\n",
      "|    explained_variance   | -0.00559    |\n",
      "|    learning_rate        | 0.000931    |\n",
      "|    loss                 | -0.00296    |\n",
      "|    n_updates            | 12110       |\n",
      "|    policy_gradient_loss | 0.00509     |\n",
      "|    std                  | 0.00207     |\n",
      "|    value_loss           | 5.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0155  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1212     |\n",
      "|    time_elapsed    | 4221     |\n",
      "|    total_timesteps | 2443392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2447424, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2447424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017568399 |\n",
      "|    clip_fraction        | 0.0743       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.63         |\n",
      "|    explained_variance   | -0.0728      |\n",
      "|    learning_rate        | 0.000924     |\n",
      "|    loss                 | 0.00185      |\n",
      "|    n_updates            | 12130        |\n",
      "|    policy_gradient_loss | 0.00195      |\n",
      "|    std                  | 0.0021       |\n",
      "|    value_loss           | 4.77e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1214     |\n",
      "|    time_elapsed    | 4227     |\n",
      "|    total_timesteps | 2447424  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2451456, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2451456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024097018 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | 0.0131      |\n",
      "|    learning_rate        | 0.000918    |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 12150       |\n",
      "|    policy_gradient_loss | 0.0076      |\n",
      "|    std                  | 0.00209     |\n",
      "|    value_loss           | 8.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1216     |\n",
      "|    time_elapsed    | 4234     |\n",
      "|    total_timesteps | 2451456  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2455488, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0164     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2455488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012679203 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.0453     |\n",
      "|    learning_rate        | 0.000911    |\n",
      "|    loss                 | -0.00369    |\n",
      "|    n_updates            | 12170       |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.00207     |\n",
      "|    value_loss           | 4.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.015   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1218     |\n",
      "|    time_elapsed    | 4241     |\n",
      "|    total_timesteps | 2455488  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009622305 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.071      |\n",
      "|    learning_rate        | 0.000904    |\n",
      "|    loss                 | 0.00739     |\n",
      "|    n_updates            | 12190       |\n",
      "|    policy_gradient_loss | 0.00547     |\n",
      "|    std                  | 0.00209     |\n",
      "|    value_loss           | 3e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1220     |\n",
      "|    time_elapsed    | 4248     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2463552, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.019      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2463552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025744582 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.07       |\n",
      "|    learning_rate        | 0.000897    |\n",
      "|    loss                 | 0.00556     |\n",
      "|    n_updates            | 12210       |\n",
      "|    policy_gradient_loss | 0.00383     |\n",
      "|    std                  | 0.00211     |\n",
      "|    value_loss           | 1.26e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1222     |\n",
      "|    time_elapsed    | 4255     |\n",
      "|    total_timesteps | 2463552  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2467584, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2467584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009292409 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.62        |\n",
      "|    explained_variance   | -0.0558     |\n",
      "|    learning_rate        | 0.000891    |\n",
      "|    loss                 | -0.00466    |\n",
      "|    n_updates            | 12230       |\n",
      "|    policy_gradient_loss | 0.000461    |\n",
      "|    std                  | 0.00212     |\n",
      "|    value_loss           | 9.19e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1224     |\n",
      "|    time_elapsed    | 4262     |\n",
      "|    total_timesteps | 2467584  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2471616, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0249     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2471616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005612687 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | -0.0173     |\n",
      "|    learning_rate        | 0.000884    |\n",
      "|    loss                 | -0.00275    |\n",
      "|    n_updates            | 12250       |\n",
      "|    policy_gradient_loss | 0.00618     |\n",
      "|    std                  | 0.0021      |\n",
      "|    value_loss           | 3.44e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0156  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1226     |\n",
      "|    time_elapsed    | 4268     |\n",
      "|    total_timesteps | 2471616  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2475648, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2475648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024690801 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.0661     |\n",
      "|    learning_rate        | 0.000877    |\n",
      "|    loss                 | -0.000773   |\n",
      "|    n_updates            | 12270       |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 0.00212     |\n",
      "|    value_loss           | 4.59e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0159  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1228     |\n",
      "|    time_elapsed    | 4275     |\n",
      "|    total_timesteps | 2475648  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0162      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2479680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033223846 |\n",
      "|    clip_fraction        | 0.229        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.65         |\n",
      "|    explained_variance   | -0.0166      |\n",
      "|    learning_rate        | 0.000871     |\n",
      "|    loss                 | 0.000202     |\n",
      "|    n_updates            | 12290        |\n",
      "|    policy_gradient_loss | 0.0178       |\n",
      "|    std                  | 0.0021       |\n",
      "|    value_loss           | 2.72e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1230     |\n",
      "|    time_elapsed    | 4282     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2483712, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0148   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2483712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0102363 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.66      |\n",
      "|    explained_variance   | -0.0125   |\n",
      "|    learning_rate        | 0.000864  |\n",
      "|    loss                 | 0.00373   |\n",
      "|    n_updates            | 12310     |\n",
      "|    policy_gradient_loss | 0.00722   |\n",
      "|    std                  | 0.0021    |\n",
      "|    value_loss           | 2.53e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.016   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1232     |\n",
      "|    time_elapsed    | 4289     |\n",
      "|    total_timesteps | 2483712  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2487744, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.017      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2487744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009657677 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.65        |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.000857    |\n",
      "|    loss                 | 2.45e-05    |\n",
      "|    n_updates            | 12330       |\n",
      "|    policy_gradient_loss | 0.00639     |\n",
      "|    std                  | 0.00211     |\n",
      "|    value_loss           | 2.96e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1234     |\n",
      "|    time_elapsed    | 4296     |\n",
      "|    total_timesteps | 2487744  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2491776, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0147    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2491776    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01141702 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.67       |\n",
      "|    explained_variance   | 0.0173     |\n",
      "|    learning_rate        | 0.00085    |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 12350      |\n",
      "|    policy_gradient_loss | 0.0109     |\n",
      "|    std                  | 0.00207    |\n",
      "|    value_loss           | 1.29e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0166  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1236     |\n",
      "|    time_elapsed    | 4303     |\n",
      "|    total_timesteps | 2491776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2495808, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0173    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2495808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03678733 |\n",
      "|    clip_fraction        | 0.339      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.67       |\n",
      "|    explained_variance   | 0.0684     |\n",
      "|    learning_rate        | 0.000844   |\n",
      "|    loss                 | 0.00985    |\n",
      "|    n_updates            | 12370      |\n",
      "|    policy_gradient_loss | 0.0108     |\n",
      "|    std                  | 0.00207    |\n",
      "|    value_loss           | 4.05e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1238     |\n",
      "|    time_elapsed    | 4310     |\n",
      "|    total_timesteps | 2495808  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0207     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017491953 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.66        |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.000837    |\n",
      "|    loss                 | -7.59e-06   |\n",
      "|    n_updates            | 12390       |\n",
      "|    policy_gradient_loss | 0.00306     |\n",
      "|    std                  | 0.0021      |\n",
      "|    value_loss           | 6.53e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0172  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1240     |\n",
      "|    time_elapsed    | 4317     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2503872, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2503872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004797865 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.65        |\n",
      "|    explained_variance   | -0.0392     |\n",
      "|    learning_rate        | 0.00083     |\n",
      "|    loss                 | -0.000834   |\n",
      "|    n_updates            | 12410       |\n",
      "|    policy_gradient_loss | 0.00755     |\n",
      "|    std                  | 0.0021      |\n",
      "|    value_loss           | 2.37e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0168  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1242     |\n",
      "|    time_elapsed    | 4324     |\n",
      "|    total_timesteps | 2503872  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2507904, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0141    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2507904    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06749193 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.66       |\n",
      "|    explained_variance   | -0.0264    |\n",
      "|    learning_rate        | 0.000824   |\n",
      "|    loss                 | 0.106      |\n",
      "|    n_updates            | 12430      |\n",
      "|    policy_gradient_loss | 0.0124     |\n",
      "|    std                  | 0.00209    |\n",
      "|    value_loss           | 7.16e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0177  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1244     |\n",
      "|    time_elapsed    | 4331     |\n",
      "|    total_timesteps | 2507904  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2511936, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0145     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2511936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016228855 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.65        |\n",
      "|    explained_variance   | -0.000279   |\n",
      "|    learning_rate        | 0.000817    |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 12450       |\n",
      "|    policy_gradient_loss | 0.00357     |\n",
      "|    std                  | 0.00211     |\n",
      "|    value_loss           | 1.01e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.018   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1246     |\n",
      "|    time_elapsed    | 4338     |\n",
      "|    total_timesteps | 2511936  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2515968, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0229      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2515968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013600083 |\n",
      "|    clip_fraction        | 0.23         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.63         |\n",
      "|    explained_variance   | -0.0895      |\n",
      "|    learning_rate        | 0.00081      |\n",
      "|    loss                 | -0.000971    |\n",
      "|    n_updates            | 12470        |\n",
      "|    policy_gradient_loss | 0.0102       |\n",
      "|    std                  | 0.0021       |\n",
      "|    value_loss           | 3.58e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0177  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1248     |\n",
      "|    time_elapsed    | 4345     |\n",
      "|    total_timesteps | 2515968  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0147     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009554093 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.0595     |\n",
      "|    learning_rate        | 0.000803    |\n",
      "|    loss                 | 0.000804    |\n",
      "|    n_updates            | 12490       |\n",
      "|    policy_gradient_loss | 0.00763     |\n",
      "|    std                  | 0.00209     |\n",
      "|    value_loss           | 7.06e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0177  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1250     |\n",
      "|    time_elapsed    | 4351     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2524032, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.017       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2524032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052840794 |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.62         |\n",
      "|    explained_variance   | 0.0257       |\n",
      "|    learning_rate        | 0.000797     |\n",
      "|    loss                 | -0.00307     |\n",
      "|    n_updates            | 12510        |\n",
      "|    policy_gradient_loss | 0.00508      |\n",
      "|    std                  | 0.00212      |\n",
      "|    value_loss           | 1.18e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0174  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1252     |\n",
      "|    time_elapsed    | 4358     |\n",
      "|    total_timesteps | 2524032  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2528064, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0159     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2528064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008074498 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | -0.0388     |\n",
      "|    learning_rate        | 0.00079     |\n",
      "|    loss                 | -0.000597   |\n",
      "|    n_updates            | 12530       |\n",
      "|    policy_gradient_loss | 0.00751     |\n",
      "|    std                  | 0.00211     |\n",
      "|    value_loss           | 3.77e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0178  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1254     |\n",
      "|    time_elapsed    | 4365     |\n",
      "|    total_timesteps | 2528064  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2532096, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0204     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2532096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012092892 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.0235     |\n",
      "|    learning_rate        | 0.000783    |\n",
      "|    loss                 | 0.00241     |\n",
      "|    n_updates            | 12550       |\n",
      "|    policy_gradient_loss | 0.00636     |\n",
      "|    std                  | 0.00209     |\n",
      "|    value_loss           | 3.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0184  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1256     |\n",
      "|    time_elapsed    | 4372     |\n",
      "|    total_timesteps | 2532096  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2536128, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2536128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020153876 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.000776    |\n",
      "|    loss                 | -0.00393    |\n",
      "|    n_updates            | 12570       |\n",
      "|    policy_gradient_loss | 0.00723     |\n",
      "|    std                  | 0.0021      |\n",
      "|    value_loss           | 3.39e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0173  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1258     |\n",
      "|    time_elapsed    | 4379     |\n",
      "|    total_timesteps | 2536128  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0225    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2540160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00496645 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.63       |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.00077    |\n",
      "|    loss                 | 0.000992   |\n",
      "|    n_updates            | 12590      |\n",
      "|    policy_gradient_loss | 0.00281    |\n",
      "|    std                  | 0.0021     |\n",
      "|    value_loss           | 3.38e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0176  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1260     |\n",
      "|    time_elapsed    | 4386     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2544192, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0144     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2544192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010284662 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.65        |\n",
      "|    explained_variance   | -0.0728     |\n",
      "|    learning_rate        | 0.000763    |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 12610       |\n",
      "|    policy_gradient_loss | 0.00871     |\n",
      "|    std                  | 0.00208     |\n",
      "|    value_loss           | 4.9e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1262     |\n",
      "|    time_elapsed    | 4392     |\n",
      "|    total_timesteps | 2544192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2548224, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0154     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2548224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013516384 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | 0.0784      |\n",
      "|    learning_rate        | 0.000756    |\n",
      "|    loss                 | 0.000521    |\n",
      "|    n_updates            | 12630       |\n",
      "|    policy_gradient_loss | 0.00979     |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 4.44e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.017   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1264     |\n",
      "|    time_elapsed    | 4399     |\n",
      "|    total_timesteps | 2548224  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2552256, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 251       |\n",
      "|    mean_reward          | -0.0156   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2552256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0118163 |\n",
      "|    clip_fraction        | 0.253     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 9.68      |\n",
      "|    explained_variance   | -0.0863   |\n",
      "|    learning_rate        | 0.00075   |\n",
      "|    loss                 | 0.00418   |\n",
      "|    n_updates            | 12650     |\n",
      "|    policy_gradient_loss | 0.0125    |\n",
      "|    std                  | 0.00206   |\n",
      "|    value_loss           | 4.02e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0173  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1266     |\n",
      "|    time_elapsed    | 4406     |\n",
      "|    total_timesteps | 2552256  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2556288, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0172     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2556288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027280603 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | -0.0843     |\n",
      "|    learning_rate        | 0.000743    |\n",
      "|    loss                 | -0.000381   |\n",
      "|    n_updates            | 12670       |\n",
      "|    policy_gradient_loss | 0.00454     |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 3.57e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0165  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1268     |\n",
      "|    time_elapsed    | 4413     |\n",
      "|    total_timesteps | 2556288  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006837707 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | -0.0249     |\n",
      "|    learning_rate        | 0.000736    |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 12690       |\n",
      "|    policy_gradient_loss | 0.00537     |\n",
      "|    std                  | 0.00205     |\n",
      "|    value_loss           | 4.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0168  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1270     |\n",
      "|    time_elapsed    | 4420     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2564352, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0162      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2564352      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028773448 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.68         |\n",
      "|    explained_variance   | -0.0138      |\n",
      "|    learning_rate        | 0.000729     |\n",
      "|    loss                 | -0.00208     |\n",
      "|    n_updates            | 12710        |\n",
      "|    policy_gradient_loss | 0.00128      |\n",
      "|    std                  | 0.00205      |\n",
      "|    value_loss           | 3.48e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0165  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1272     |\n",
      "|    time_elapsed    | 4427     |\n",
      "|    total_timesteps | 2564352  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2568384, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2568384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004637512 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.000723    |\n",
      "|    loss                 | 0.00258     |\n",
      "|    n_updates            | 12730       |\n",
      "|    policy_gradient_loss | 0.00564     |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1274     |\n",
      "|    time_elapsed    | 4434     |\n",
      "|    total_timesteps | 2568384  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2572416, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0153     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2572416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011506347 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | -0.043      |\n",
      "|    learning_rate        | 0.000716    |\n",
      "|    loss                 | 0.00897     |\n",
      "|    n_updates            | 12750       |\n",
      "|    policy_gradient_loss | 0.0031      |\n",
      "|    std                  | 0.00207     |\n",
      "|    value_loss           | 3.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1276     |\n",
      "|    time_elapsed    | 4441     |\n",
      "|    total_timesteps | 2572416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2576448, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0143      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2576448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057055512 |\n",
      "|    clip_fraction        | 0.202        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.67         |\n",
      "|    explained_variance   | 0.0996       |\n",
      "|    learning_rate        | 0.000709     |\n",
      "|    loss                 | 0.0118       |\n",
      "|    n_updates            | 12770        |\n",
      "|    policy_gradient_loss | 0.0037       |\n",
      "|    std                  | 0.00206      |\n",
      "|    value_loss           | 4.49e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0169  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1278     |\n",
      "|    time_elapsed    | 4448     |\n",
      "|    total_timesteps | 2576448  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024723481 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | 0.0826      |\n",
      "|    learning_rate        | 0.000703    |\n",
      "|    loss                 | 0.00399     |\n",
      "|    n_updates            | 12790       |\n",
      "|    policy_gradient_loss | 0.00373     |\n",
      "|    std                  | 0.00207     |\n",
      "|    value_loss           | 8.17e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.017   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1280     |\n",
      "|    time_elapsed    | 4454     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2584512, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2584512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013998398 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | -0.0873     |\n",
      "|    learning_rate        | 0.000696    |\n",
      "|    loss                 | -0.00118    |\n",
      "|    n_updates            | 12810       |\n",
      "|    policy_gradient_loss | 0.00417     |\n",
      "|    std                  | 0.00205     |\n",
      "|    value_loss           | 3.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0167  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1282     |\n",
      "|    time_elapsed    | 4461     |\n",
      "|    total_timesteps | 2584512  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2588544, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2588544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064848445 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.000689    |\n",
      "|    loss                 | 0.0918      |\n",
      "|    n_updates            | 12830       |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 9.91e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.017   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1284     |\n",
      "|    time_elapsed    | 4468     |\n",
      "|    total_timesteps | 2588544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2592576, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2592576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009210484 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | -0.151      |\n",
      "|    learning_rate        | 0.000682    |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 12850       |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    std                  | 0.00208     |\n",
      "|    value_loss           | 3.16e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1286     |\n",
      "|    time_elapsed    | 4475     |\n",
      "|    total_timesteps | 2592576  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2596608, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2596608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007700567 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.69        |\n",
      "|    explained_variance   | -0.0338     |\n",
      "|    learning_rate        | 0.000676    |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 12870       |\n",
      "|    policy_gradient_loss | 0.00381     |\n",
      "|    std                  | 0.00205     |\n",
      "|    value_loss           | 2.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1288     |\n",
      "|    time_elapsed    | 4482     |\n",
      "|    total_timesteps | 2596608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0235     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032623436 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | -0.0637     |\n",
      "|    learning_rate        | 0.000669    |\n",
      "|    loss                 | 0.00225     |\n",
      "|    n_updates            | 12890       |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    std                  | 0.00207     |\n",
      "|    value_loss           | 5.5e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.016   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1290     |\n",
      "|    time_elapsed    | 4489     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2604672, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0203     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2604672     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007299274 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.66        |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.000662    |\n",
      "|    loss                 | 0.00258     |\n",
      "|    n_updates            | 12910       |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    std                  | 0.0021      |\n",
      "|    value_loss           | 2.99e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0161  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1292     |\n",
      "|    time_elapsed    | 4496     |\n",
      "|    total_timesteps | 2604672  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2608704, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0136      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2608704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014901889 |\n",
      "|    clip_fraction        | 0.097        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.64         |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.000656     |\n",
      "|    loss                 | 0.000516     |\n",
      "|    n_updates            | 12930        |\n",
      "|    policy_gradient_loss | 0.00144      |\n",
      "|    std                  | 0.00212      |\n",
      "|    value_loss           | 2.59e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0162  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1294     |\n",
      "|    time_elapsed    | 4503     |\n",
      "|    total_timesteps | 2608704  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2612736, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0158    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2612736    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00877424 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.63       |\n",
      "|    explained_variance   | -0.088     |\n",
      "|    learning_rate        | 0.000649   |\n",
      "|    loss                 | 0.000413   |\n",
      "|    n_updates            | 12950      |\n",
      "|    policy_gradient_loss | 0.00472    |\n",
      "|    std                  | 0.00212    |\n",
      "|    value_loss           | 4.27e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1296     |\n",
      "|    time_elapsed    | 4510     |\n",
      "|    total_timesteps | 2612736  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2616768, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0187      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2616768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050916253 |\n",
      "|    clip_fraction        | 0.176        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.63         |\n",
      "|    explained_variance   | -0.0484      |\n",
      "|    learning_rate        | 0.000642     |\n",
      "|    loss                 | 0.00674      |\n",
      "|    n_updates            | 12970        |\n",
      "|    policy_gradient_loss | 0.00744      |\n",
      "|    std                  | 0.00211      |\n",
      "|    value_loss           | 2.34e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1298     |\n",
      "|    time_elapsed    | 4517     |\n",
      "|    total_timesteps | 2616768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009533197 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.62        |\n",
      "|    explained_variance   | 0.0385      |\n",
      "|    learning_rate        | 0.000635    |\n",
      "|    loss                 | 0.000682    |\n",
      "|    n_updates            | 12990       |\n",
      "|    policy_gradient_loss | 0.00102     |\n",
      "|    std                  | 0.00214     |\n",
      "|    value_loss           | 6.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1300     |\n",
      "|    time_elapsed    | 4523     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2624832, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0143     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2624832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021075647 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.6         |\n",
      "|    explained_variance   | -0.128      |\n",
      "|    learning_rate        | 0.000629    |\n",
      "|    loss                 | 0.00648     |\n",
      "|    n_updates            | 13010       |\n",
      "|    policy_gradient_loss | 0.0068      |\n",
      "|    std                  | 0.00215     |\n",
      "|    value_loss           | 1.52e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1302     |\n",
      "|    time_elapsed    | 4530     |\n",
      "|    total_timesteps | 2624832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2628864, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0142    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2628864    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00958107 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.58       |\n",
      "|    explained_variance   | -0.0416    |\n",
      "|    learning_rate        | 0.000622   |\n",
      "|    loss                 | 0.0151     |\n",
      "|    n_updates            | 13030      |\n",
      "|    policy_gradient_loss | 0.00506    |\n",
      "|    std                  | 0.00217    |\n",
      "|    value_loss           | 4.55e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1304     |\n",
      "|    time_elapsed    | 4537     |\n",
      "|    total_timesteps | 2628864  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2632896, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0183      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2632896      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023568422 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.6          |\n",
      "|    explained_variance   | -0.0235      |\n",
      "|    learning_rate        | 0.000615     |\n",
      "|    loss                 | -0.000318    |\n",
      "|    n_updates            | 13050        |\n",
      "|    policy_gradient_loss | 0.00397      |\n",
      "|    std                  | 0.00213      |\n",
      "|    value_loss           | 1.02e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1306     |\n",
      "|    time_elapsed    | 4544     |\n",
      "|    total_timesteps | 2632896  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2636928, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0154      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2636928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075119683 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.64         |\n",
      "|    explained_variance   | -0.146       |\n",
      "|    learning_rate        | 0.000608     |\n",
      "|    loss                 | -0.00971     |\n",
      "|    n_updates            | 13070        |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    std                  | 0.00209      |\n",
      "|    value_loss           | 2.86e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1308     |\n",
      "|    time_elapsed    | 4551     |\n",
      "|    total_timesteps | 2636928  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0147     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019031037 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.64        |\n",
      "|    explained_variance   | -0.152      |\n",
      "|    learning_rate        | 0.000602    |\n",
      "|    loss                 | 0.0138      |\n",
      "|    n_updates            | 13090       |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.0021      |\n",
      "|    value_loss           | 2.93e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1310     |\n",
      "|    time_elapsed    | 4558     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2644992, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0166     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2644992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015748868 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.65        |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.000595    |\n",
      "|    loss                 | -0.00417    |\n",
      "|    n_updates            | 13110       |\n",
      "|    policy_gradient_loss | 0.00139     |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 1.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1312     |\n",
      "|    time_elapsed    | 4565     |\n",
      "|    total_timesteps | 2644992  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2649024, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2649024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009335538 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | -0.158      |\n",
      "|    learning_rate        | 0.000588    |\n",
      "|    loss                 | 0.00344     |\n",
      "|    n_updates            | 13130       |\n",
      "|    policy_gradient_loss | 0.00562     |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 4.38e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1314     |\n",
      "|    time_elapsed    | 4571     |\n",
      "|    total_timesteps | 2649024  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2653056, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0121     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2653056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006990679 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.000582    |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 13150       |\n",
      "|    policy_gradient_loss | 0.00174     |\n",
      "|    std                  | 0.00206     |\n",
      "|    value_loss           | 1e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1316     |\n",
      "|    time_elapsed    | 4578     |\n",
      "|    total_timesteps | 2653056  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2657088, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0125    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2657088    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01243509 |\n",
      "|    clip_fraction        | 0.0875     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.68       |\n",
      "|    explained_variance   | -0.162     |\n",
      "|    learning_rate        | 0.000575   |\n",
      "|    loss                 | 0.00045    |\n",
      "|    n_updates            | 13170      |\n",
      "|    policy_gradient_loss | 0.00245    |\n",
      "|    std                  | 0.00204    |\n",
      "|    value_loss           | 2.53e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1318     |\n",
      "|    time_elapsed    | 4585     |\n",
      "|    total_timesteps | 2657088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0125      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070658177 |\n",
      "|    clip_fraction        | 0.0677       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.69         |\n",
      "|    explained_variance   | -0.103       |\n",
      "|    learning_rate        | 0.000568     |\n",
      "|    loss                 | 0.00321      |\n",
      "|    n_updates            | 13190        |\n",
      "|    policy_gradient_loss | 0.00126      |\n",
      "|    std                  | 0.00204      |\n",
      "|    value_loss           | 2.86e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1320     |\n",
      "|    time_elapsed    | 4592     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2665152, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.021      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2665152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009698565 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | -0.117      |\n",
      "|    learning_rate        | 0.000561    |\n",
      "|    loss                 | 0.00161     |\n",
      "|    n_updates            | 13210       |\n",
      "|    policy_gradient_loss | 0.000288    |\n",
      "|    std                  | 0.00205     |\n",
      "|    value_loss           | 4.35e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1322     |\n",
      "|    time_elapsed    | 4600     |\n",
      "|    total_timesteps | 2665152  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2669184, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0127      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2669184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040888237 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.7          |\n",
      "|    explained_variance   | 0.264        |\n",
      "|    learning_rate        | 0.000555     |\n",
      "|    loss                 | 0.00628      |\n",
      "|    n_updates            | 13230        |\n",
      "|    policy_gradient_loss | 0.00334      |\n",
      "|    std                  | 0.00203      |\n",
      "|    value_loss           | 4.7e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1324     |\n",
      "|    time_elapsed    | 4607     |\n",
      "|    total_timesteps | 2669184  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2673216, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0103      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2673216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054126065 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.7          |\n",
      "|    explained_variance   | -0.148       |\n",
      "|    learning_rate        | 0.000548     |\n",
      "|    loss                 | 0.00939      |\n",
      "|    n_updates            | 13250        |\n",
      "|    policy_gradient_loss | 0.00504      |\n",
      "|    std                  | 0.00203      |\n",
      "|    value_loss           | 9.19e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1326     |\n",
      "|    time_elapsed    | 4615     |\n",
      "|    total_timesteps | 2673216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2677248, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0132      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2677248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023692884 |\n",
      "|    clip_fraction        | 0.084        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.68         |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.000541     |\n",
      "|    loss                 | -0.00113     |\n",
      "|    n_updates            | 13270        |\n",
      "|    policy_gradient_loss | 0.00143      |\n",
      "|    std                  | 0.00205      |\n",
      "|    value_loss           | 1.28e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1328     |\n",
      "|    time_elapsed    | 4622     |\n",
      "|    total_timesteps | 2677248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0149     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2681280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022081874 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | -0.103      |\n",
      "|    learning_rate        | 0.000535    |\n",
      "|    loss                 | 0.00417     |\n",
      "|    n_updates            | 13290       |\n",
      "|    policy_gradient_loss | 0.00461     |\n",
      "|    std                  | 0.00204     |\n",
      "|    value_loss           | 4.08e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1330     |\n",
      "|    time_elapsed    | 4629     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2685312, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2685312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012893112 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.69        |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.000528    |\n",
      "|    loss                 | 0.000506    |\n",
      "|    n_updates            | 13310       |\n",
      "|    policy_gradient_loss | 0.00515     |\n",
      "|    std                  | 0.00203     |\n",
      "|    value_loss           | 2.78e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1332     |\n",
      "|    time_elapsed    | 4637     |\n",
      "|    total_timesteps | 2685312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2689344, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2689344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008630965 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.71        |\n",
      "|    explained_variance   | -0.0211     |\n",
      "|    learning_rate        | 0.000521    |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 13330       |\n",
      "|    policy_gradient_loss | 0.00296     |\n",
      "|    std                  | 0.00202     |\n",
      "|    value_loss           | 2.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1334     |\n",
      "|    time_elapsed    | 4644     |\n",
      "|    total_timesteps | 2689344  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2693376, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0128    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2693376    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01107871 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.71       |\n",
      "|    explained_variance   | -0.0863    |\n",
      "|    learning_rate        | 0.000514   |\n",
      "|    loss                 | -0.00185   |\n",
      "|    n_updates            | 13350      |\n",
      "|    policy_gradient_loss | 0.00155    |\n",
      "|    std                  | 0.00202    |\n",
      "|    value_loss           | 3.43e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1336     |\n",
      "|    time_elapsed    | 4651     |\n",
      "|    total_timesteps | 2693376  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2697408, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2697408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012118001 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.71        |\n",
      "|    explained_variance   | -0.0236     |\n",
      "|    learning_rate        | 0.000508    |\n",
      "|    loss                 | -0.00765    |\n",
      "|    n_updates            | 13370       |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    std                  | 0.00201     |\n",
      "|    value_loss           | 2.01e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1338     |\n",
      "|    time_elapsed    | 4658     |\n",
      "|    total_timesteps | 2697408  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0199     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2701440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011463111 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.72        |\n",
      "|    explained_variance   | 0.088       |\n",
      "|    learning_rate        | 0.000501    |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 13390       |\n",
      "|    policy_gradient_loss | -0.000283   |\n",
      "|    std                  | 0.00202     |\n",
      "|    value_loss           | 2.97e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 1340     |\n",
      "|    time_elapsed    | 4665     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2705472, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0144      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2705472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036754266 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.7          |\n",
      "|    explained_variance   | -0.113       |\n",
      "|    learning_rate        | 0.000494     |\n",
      "|    loss                 | 0.00114      |\n",
      "|    n_updates            | 13410        |\n",
      "|    policy_gradient_loss | 0.00432      |\n",
      "|    std                  | 0.00202      |\n",
      "|    value_loss           | 5.64e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1342     |\n",
      "|    time_elapsed    | 4672     |\n",
      "|    total_timesteps | 2705472  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2709504, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0134      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2709504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030051009 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.72         |\n",
      "|    explained_variance   | -0.051       |\n",
      "|    learning_rate        | 0.000488     |\n",
      "|    loss                 | -0.00488     |\n",
      "|    n_updates            | 13430        |\n",
      "|    policy_gradient_loss | -0.000549    |\n",
      "|    std                  | 0.002        |\n",
      "|    value_loss           | 3.72e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1344     |\n",
      "|    time_elapsed    | 4680     |\n",
      "|    total_timesteps | 2709504  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2713536, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0142      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2713536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004331189 |\n",
      "|    clip_fraction        | 0.0587       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.72         |\n",
      "|    explained_variance   | -0.0613      |\n",
      "|    learning_rate        | 0.000481     |\n",
      "|    loss                 | -0.000645    |\n",
      "|    n_updates            | 13450        |\n",
      "|    policy_gradient_loss | 0.00192      |\n",
      "|    std                  | 0.002        |\n",
      "|    value_loss           | 3.15e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1346     |\n",
      "|    time_elapsed    | 4687     |\n",
      "|    total_timesteps | 2713536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2717568, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.013      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2717568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008843355 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.74        |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.000474    |\n",
      "|    loss                 | 0.00508     |\n",
      "|    n_updates            | 13470       |\n",
      "|    policy_gradient_loss | 0.0023      |\n",
      "|    std                  | 0.00198     |\n",
      "|    value_loss           | 1.52e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1348     |\n",
      "|    time_elapsed    | 4694     |\n",
      "|    total_timesteps | 2717568  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0123      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2721600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062955637 |\n",
      "|    clip_fraction        | 0.0886       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.75         |\n",
      "|    explained_variance   | -0.111       |\n",
      "|    learning_rate        | 0.000467     |\n",
      "|    loss                 | 0.00187      |\n",
      "|    n_updates            | 13490        |\n",
      "|    policy_gradient_loss | 0.00243      |\n",
      "|    std                  | 0.00197      |\n",
      "|    value_loss           | 4.29e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1350     |\n",
      "|    time_elapsed    | 4701     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2725632, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2725632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017575623 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.75        |\n",
      "|    explained_variance   | 0.0143      |\n",
      "|    learning_rate        | 0.000461    |\n",
      "|    loss                 | -0.000689   |\n",
      "|    n_updates            | 13510       |\n",
      "|    policy_gradient_loss | 0.00601     |\n",
      "|    std                  | 0.00198     |\n",
      "|    value_loss           | 6.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1352     |\n",
      "|    time_elapsed    | 4708     |\n",
      "|    total_timesteps | 2725632  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2729664, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2729664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096378215 |\n",
      "|    clip_fraction        | 0.0944       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.75         |\n",
      "|    explained_variance   | -0.176       |\n",
      "|    learning_rate        | 0.000454     |\n",
      "|    loss                 | 0.00881      |\n",
      "|    n_updates            | 13530        |\n",
      "|    policy_gradient_loss | 0.00209      |\n",
      "|    std                  | 0.00196      |\n",
      "|    value_loss           | 2.06e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1354     |\n",
      "|    time_elapsed    | 4715     |\n",
      "|    total_timesteps | 2729664  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2733696, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2733696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014290624 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.76        |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.000447    |\n",
      "|    loss                 | 0.00897     |\n",
      "|    n_updates            | 13550       |\n",
      "|    policy_gradient_loss | 0.00295     |\n",
      "|    std                  | 0.00196     |\n",
      "|    value_loss           | 2.65e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1356     |\n",
      "|    time_elapsed    | 4723     |\n",
      "|    total_timesteps | 2733696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2737728, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0207     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2737728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007121642 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.76        |\n",
      "|    explained_variance   | 0.0894      |\n",
      "|    learning_rate        | 0.00044     |\n",
      "|    loss                 | 0.00327     |\n",
      "|    n_updates            | 13570       |\n",
      "|    policy_gradient_loss | 0.0021      |\n",
      "|    std                  | 0.00196     |\n",
      "|    value_loss           | 2.08e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1358     |\n",
      "|    time_elapsed    | 4730     |\n",
      "|    total_timesteps | 2737728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0204     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014966436 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.76        |\n",
      "|    explained_variance   | -0.0737     |\n",
      "|    learning_rate        | 0.000434    |\n",
      "|    loss                 | 0.00808     |\n",
      "|    n_updates            | 13590       |\n",
      "|    policy_gradient_loss | 0.00271     |\n",
      "|    std                  | 0.00198     |\n",
      "|    value_loss           | 5.18e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1360     |\n",
      "|    time_elapsed    | 4737     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2745792, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.015      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2745792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010922726 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.75        |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.000427    |\n",
      "|    loss                 | -0.00184    |\n",
      "|    n_updates            | 13610       |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    std                  | 0.00198     |\n",
      "|    value_loss           | 3.55e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1362     |\n",
      "|    time_elapsed    | 4744     |\n",
      "|    total_timesteps | 2745792  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2749824, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0121      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2749824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071124807 |\n",
      "|    clip_fraction        | 0.0876       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.76         |\n",
      "|    explained_variance   | -0.0807      |\n",
      "|    learning_rate        | 0.00042      |\n",
      "|    loss                 | 0.0117       |\n",
      "|    n_updates            | 13630        |\n",
      "|    policy_gradient_loss | -0.000696    |\n",
      "|    std                  | 0.00196      |\n",
      "|    value_loss           | 3.98e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1364     |\n",
      "|    time_elapsed    | 4751     |\n",
      "|    total_timesteps | 2749824  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2753856, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2753856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009361565 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.78        |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.000414    |\n",
      "|    loss                 | -0.000929   |\n",
      "|    n_updates            | 13650       |\n",
      "|    policy_gradient_loss | 0.0014      |\n",
      "|    std                  | 0.00195     |\n",
      "|    value_loss           | 8.63e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1366     |\n",
      "|    time_elapsed    | 4758     |\n",
      "|    total_timesteps | 2753856  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2757888, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0102     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2757888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023635464 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.79        |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.000407    |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 13670       |\n",
      "|    policy_gradient_loss | 0.00777     |\n",
      "|    std                  | 0.00195     |\n",
      "|    value_loss           | 2.3e-08     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1368     |\n",
      "|    time_elapsed    | 4766     |\n",
      "|    total_timesteps | 2757888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0206     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2761920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009477042 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.79        |\n",
      "|    explained_variance   | -0.0837     |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 13690       |\n",
      "|    policy_gradient_loss | 0.00445     |\n",
      "|    std                  | 0.00194     |\n",
      "|    value_loss           | 7.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0151  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1370     |\n",
      "|    time_elapsed    | 4773     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2765952, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0127      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2765952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031477602 |\n",
      "|    clip_fraction        | 0.099        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.79         |\n",
      "|    explained_variance   | -0.0144      |\n",
      "|    learning_rate        | 0.000393     |\n",
      "|    loss                 | -0.000318    |\n",
      "|    n_updates            | 13710        |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    std                  | 0.00194      |\n",
      "|    value_loss           | 4.87e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1372     |\n",
      "|    time_elapsed    | 4780     |\n",
      "|    total_timesteps | 2765952  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2769984, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0136      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2769984      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082796095 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.79         |\n",
      "|    explained_variance   | -0.104       |\n",
      "|    learning_rate        | 0.000387     |\n",
      "|    loss                 | 0.0052       |\n",
      "|    n_updates            | 13730        |\n",
      "|    policy_gradient_loss | -0.000456    |\n",
      "|    std                  | 0.00194      |\n",
      "|    value_loss           | 3.92e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1374     |\n",
      "|    time_elapsed    | 4787     |\n",
      "|    total_timesteps | 2769984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2774016, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0193      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2774016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046532387 |\n",
      "|    clip_fraction        | 0.0543       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.79         |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 0.00038      |\n",
      "|    loss                 | -0.000655    |\n",
      "|    n_updates            | 13750        |\n",
      "|    policy_gradient_loss | 0.00233      |\n",
      "|    std                  | 0.00194      |\n",
      "|    value_loss           | 1.65e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1376     |\n",
      "|    time_elapsed    | 4794     |\n",
      "|    total_timesteps | 2774016  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2778048, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0141     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2778048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010499011 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.79        |\n",
      "|    explained_variance   | -0.0364     |\n",
      "|    learning_rate        | 0.000373    |\n",
      "|    loss                 | -0.00359    |\n",
      "|    n_updates            | 13770       |\n",
      "|    policy_gradient_loss | 0.000597    |\n",
      "|    std                  | 0.00194     |\n",
      "|    value_loss           | 3.62e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1378     |\n",
      "|    time_elapsed    | 4801     |\n",
      "|    total_timesteps | 2778048  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006494372 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.79        |\n",
      "|    explained_variance   | -0.0394     |\n",
      "|    learning_rate        | 0.000367    |\n",
      "|    loss                 | 0.00553     |\n",
      "|    n_updates            | 13790       |\n",
      "|    policy_gradient_loss | 0.000617    |\n",
      "|    std                  | 0.00195     |\n",
      "|    value_loss           | 1.89e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0141  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1380     |\n",
      "|    time_elapsed    | 4808     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2786112, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0117      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2786112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069561685 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.8          |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.00036      |\n",
      "|    loss                 | -0.00261     |\n",
      "|    n_updates            | 13810        |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    std                  | 0.00194      |\n",
      "|    value_loss           | 6.19e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1382     |\n",
      "|    time_elapsed    | 4816     |\n",
      "|    total_timesteps | 2786112  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790144, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0151     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2790144     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007615421 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.8         |\n",
      "|    explained_variance   | 0.0422      |\n",
      "|    learning_rate        | 0.000353    |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 13830       |\n",
      "|    policy_gradient_loss | -0.000141   |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 4.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0148  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1384     |\n",
      "|    time_elapsed    | 4823     |\n",
      "|    total_timesteps | 2790144  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2794176, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.014      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2794176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011074191 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | 0.0497      |\n",
      "|    learning_rate        | 0.000346    |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 13850       |\n",
      "|    policy_gradient_loss | 0.000879    |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 3.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0154  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1386     |\n",
      "|    time_elapsed    | 4830     |\n",
      "|    total_timesteps | 2794176  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2798208, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0112      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2798208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050856173 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.81         |\n",
      "|    explained_variance   | -0.031       |\n",
      "|    learning_rate        | 0.00034      |\n",
      "|    loss                 | 0.000817     |\n",
      "|    n_updates            | 13870        |\n",
      "|    policy_gradient_loss | 0.000918     |\n",
      "|    std                  | 0.00193      |\n",
      "|    value_loss           | 5.34e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0153  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1388     |\n",
      "|    time_elapsed    | 4837     |\n",
      "|    total_timesteps | 2798208  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0168     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2802240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009948845 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.8         |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.000333    |\n",
      "|    loss                 | -0.00521    |\n",
      "|    n_updates            | 13890       |\n",
      "|    policy_gradient_loss | -0.00035    |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 3.44e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1390     |\n",
      "|    time_elapsed    | 4844     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2806272, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0146      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2806272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031292904 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.81         |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.000326     |\n",
      "|    loss                 | 0.00207      |\n",
      "|    n_updates            | 13910        |\n",
      "|    policy_gradient_loss | 0.000207     |\n",
      "|    std                  | 0.00193      |\n",
      "|    value_loss           | 4.99e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0166  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1392     |\n",
      "|    time_elapsed    | 4851     |\n",
      "|    total_timesteps | 2806272  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810304, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0152     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2810304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009262152 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.00032     |\n",
      "|    loss                 | -0.00821    |\n",
      "|    n_updates            | 13930       |\n",
      "|    policy_gradient_loss | 0.00166     |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 6.39e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0168  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1394     |\n",
      "|    time_elapsed    | 4859     |\n",
      "|    total_timesteps | 2810304  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2814336, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0129      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2814336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068411627 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.8          |\n",
      "|    explained_variance   | 0.0703       |\n",
      "|    learning_rate        | 0.000313     |\n",
      "|    loss                 | 0.00283      |\n",
      "|    n_updates            | 13950        |\n",
      "|    policy_gradient_loss | -0.000967    |\n",
      "|    std                  | 0.00195      |\n",
      "|    value_loss           | 7.03e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0164  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1396     |\n",
      "|    time_elapsed    | 4866     |\n",
      "|    total_timesteps | 2814336  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2818368, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2818368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013746086 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.8         |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.000306    |\n",
      "|    loss                 | 0.00892     |\n",
      "|    n_updates            | 13970       |\n",
      "|    policy_gradient_loss | 0.00915     |\n",
      "|    std                  | 0.00194     |\n",
      "|    value_loss           | 1.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0162  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1398     |\n",
      "|    time_elapsed    | 4873     |\n",
      "|    total_timesteps | 2818368  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0146     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007820843 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.8         |\n",
      "|    explained_variance   | -0.0134     |\n",
      "|    learning_rate        | 0.000299    |\n",
      "|    loss                 | -0.00528    |\n",
      "|    n_updates            | 13990       |\n",
      "|    policy_gradient_loss | -0.00065    |\n",
      "|    std                  | 0.00194     |\n",
      "|    value_loss           | 4.56e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0158  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 4881     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2826432, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0146      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2826432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016570606 |\n",
      "|    clip_fraction        | 0.0514       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.8          |\n",
      "|    explained_variance   | -0.123       |\n",
      "|    learning_rate        | 0.000293     |\n",
      "|    loss                 | -0.000725    |\n",
      "|    n_updates            | 14010        |\n",
      "|    policy_gradient_loss | 0.00171      |\n",
      "|    std                  | 0.00194      |\n",
      "|    value_loss           | 2.36e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0152  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1402     |\n",
      "|    time_elapsed    | 4888     |\n",
      "|    total_timesteps | 2826432  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830464, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0101     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2830464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011327826 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | 0.0595      |\n",
      "|    learning_rate        | 0.000286    |\n",
      "|    loss                 | 0.00159     |\n",
      "|    n_updates            | 14030       |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 8.55e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1404     |\n",
      "|    time_elapsed    | 4895     |\n",
      "|    total_timesteps | 2830464  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2834496, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2834496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005591045 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | -0.13       |\n",
      "|    learning_rate        | 0.000279    |\n",
      "|    loss                 | -0.00327    |\n",
      "|    n_updates            | 14050       |\n",
      "|    policy_gradient_loss | 0.0028      |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 3e-08       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1406     |\n",
      "|    time_elapsed    | 4902     |\n",
      "|    total_timesteps | 2834496  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2838528, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0131     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2838528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005093504 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.000272    |\n",
      "|    loss                 | 0.00829     |\n",
      "|    n_updates            | 14070       |\n",
      "|    policy_gradient_loss | 0.000852    |\n",
      "|    std                  | 0.00193     |\n",
      "|    value_loss           | 2.45e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1408     |\n",
      "|    time_elapsed    | 4909     |\n",
      "|    total_timesteps | 2838528  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0128      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2842560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058833277 |\n",
      "|    clip_fraction        | 0.0798       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.81         |\n",
      "|    explained_variance   | -0.0787      |\n",
      "|    learning_rate        | 0.000266     |\n",
      "|    loss                 | -0.00406     |\n",
      "|    n_updates            | 14090        |\n",
      "|    policy_gradient_loss | 0.00019      |\n",
      "|    std                  | 0.00193      |\n",
      "|    value_loss           | 3.92e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0133  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1410     |\n",
      "|    time_elapsed    | 4916     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2846592, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0273     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2846592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003608453 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.000259    |\n",
      "|    loss                 | -0.00405    |\n",
      "|    n_updates            | 14110       |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    std                  | 0.00192     |\n",
      "|    value_loss           | 2.26e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0132  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1412     |\n",
      "|    time_elapsed    | 4924     |\n",
      "|    total_timesteps | 2846592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850624, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0117     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2850624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006783884 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.81        |\n",
      "|    explained_variance   | -0.0584     |\n",
      "|    learning_rate        | 0.000252    |\n",
      "|    loss                 | -0.000803   |\n",
      "|    n_updates            | 14130       |\n",
      "|    policy_gradient_loss | -0.000865   |\n",
      "|    std                  | 0.00192     |\n",
      "|    value_loss           | 4.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1414     |\n",
      "|    time_elapsed    | 4931     |\n",
      "|    total_timesteps | 2850624  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2854656, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2854656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003470827 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.82        |\n",
      "|    explained_variance   | -0.103      |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.00228    |\n",
      "|    n_updates            | 14150       |\n",
      "|    policy_gradient_loss | 0.0033      |\n",
      "|    std                  | 0.00191     |\n",
      "|    value_loss           | 4.71e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1416     |\n",
      "|    time_elapsed    | 4938     |\n",
      "|    total_timesteps | 2854656  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2858688, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0149      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2858688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023773834 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.83         |\n",
      "|    explained_variance   | -0.127       |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | 0.00582      |\n",
      "|    n_updates            | 14170        |\n",
      "|    policy_gradient_loss | -0.00024     |\n",
      "|    std                  | 0.00191      |\n",
      "|    value_loss           | 2.72e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 1418     |\n",
      "|    time_elapsed    | 4945     |\n",
      "|    total_timesteps | 2858688  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013248753 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.83        |\n",
      "|    explained_variance   | -0.105      |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | -0.00159    |\n",
      "|    n_updates            | 14190       |\n",
      "|    policy_gradient_loss | -5.41e-05   |\n",
      "|    std                  | 0.00191     |\n",
      "|    value_loss           | 2.65e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1420     |\n",
      "|    time_elapsed    | 4952     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2866752, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2866752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006028971 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.83         |\n",
      "|    explained_variance   | -0.131       |\n",
      "|    learning_rate        | 0.000225     |\n",
      "|    loss                 | -0.0011      |\n",
      "|    n_updates            | 14210        |\n",
      "|    policy_gradient_loss | 0.00172      |\n",
      "|    std                  | 0.00191      |\n",
      "|    value_loss           | 3.65e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1422     |\n",
      "|    time_elapsed    | 4960     |\n",
      "|    total_timesteps | 2866752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870784, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2870784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008503916 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.84        |\n",
      "|    explained_variance   | -0.0219     |\n",
      "|    learning_rate        | 0.000219    |\n",
      "|    loss                 | -0.00812    |\n",
      "|    n_updates            | 14230       |\n",
      "|    policy_gradient_loss | 0.000973    |\n",
      "|    std                  | 0.0019      |\n",
      "|    value_loss           | 7.31e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0122  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1424     |\n",
      "|    time_elapsed    | 4967     |\n",
      "|    total_timesteps | 2870784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2874816, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0131      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2874816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050280206 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.84         |\n",
      "|    explained_variance   | -0.157       |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | 0.00702      |\n",
      "|    n_updates            | 14250        |\n",
      "|    policy_gradient_loss | 0.00022      |\n",
      "|    std                  | 0.00189      |\n",
      "|    value_loss           | 2.63e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1426     |\n",
      "|    time_elapsed    | 4974     |\n",
      "|    total_timesteps | 2874816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2878848, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.014       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2878848      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026970217 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.85         |\n",
      "|    explained_variance   | -0.0498      |\n",
      "|    learning_rate        | 0.000205     |\n",
      "|    loss                 | 0.00103      |\n",
      "|    n_updates            | 14270        |\n",
      "|    policy_gradient_loss | 0.000588     |\n",
      "|    std                  | 0.00189      |\n",
      "|    value_loss           | 6.26e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0123  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1428     |\n",
      "|    time_elapsed    | 4981     |\n",
      "|    total_timesteps | 2878848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011338544 |\n",
      "|    clip_fraction        | 0.0819       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.85         |\n",
      "|    explained_variance   | 0.162        |\n",
      "|    learning_rate        | 0.000199     |\n",
      "|    loss                 | 0.00553      |\n",
      "|    n_updates            | 14290        |\n",
      "|    policy_gradient_loss | 0.00148      |\n",
      "|    std                  | 0.00189      |\n",
      "|    value_loss           | 6.74e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.013   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1430     |\n",
      "|    time_elapsed    | 4988     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2886912, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0185       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2886912       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027758928 |\n",
      "|    clip_fraction        | 0.0304        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 9.85          |\n",
      "|    explained_variance   | 0.0805        |\n",
      "|    learning_rate        | 0.000192      |\n",
      "|    loss                 | 0.00203       |\n",
      "|    n_updates            | 14310         |\n",
      "|    policy_gradient_loss | 0.000245      |\n",
      "|    std                  | 0.00189       |\n",
      "|    value_loss           | 1.56e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0135  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1432     |\n",
      "|    time_elapsed    | 4995     |\n",
      "|    total_timesteps | 2886912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890944, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 251        |\n",
      "|    mean_reward          | -0.0118    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2890944    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00266656 |\n",
      "|    clip_fraction        | 0.0405     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 9.85       |\n",
      "|    explained_variance   | 0.307      |\n",
      "|    learning_rate        | 0.000185   |\n",
      "|    loss                 | -0.00259   |\n",
      "|    n_updates            | 14330      |\n",
      "|    policy_gradient_loss | 4.64e-05   |\n",
      "|    std                  | 0.00189    |\n",
      "|    value_loss           | 5.99e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1434     |\n",
      "|    time_elapsed    | 5003     |\n",
      "|    total_timesteps | 2890944  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2894976, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0127     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2894976     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004569539 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.85        |\n",
      "|    explained_variance   | -0.127      |\n",
      "|    learning_rate        | 0.000178    |\n",
      "|    loss                 | -0.00291    |\n",
      "|    n_updates            | 14350       |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    std                  | 0.00189     |\n",
      "|    value_loss           | 4.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1436     |\n",
      "|    time_elapsed    | 5010     |\n",
      "|    total_timesteps | 2894976  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2899008, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0125     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2899008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004192235 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | -0.0764     |\n",
      "|    learning_rate        | 0.000172    |\n",
      "|    loss                 | -0.00149    |\n",
      "|    n_updates            | 14370       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 5.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0142  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1438     |\n",
      "|    time_elapsed    | 5017     |\n",
      "|    total_timesteps | 2899008  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2903040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003216731 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.0623      |\n",
      "|    learning_rate        | 0.000165    |\n",
      "|    loss                 | -0.00207    |\n",
      "|    n_updates            | 14390       |\n",
      "|    policy_gradient_loss | 0.0005      |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 4.03e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0146  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1440     |\n",
      "|    time_elapsed    | 5024     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2907072, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2907072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019103563 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.211       |\n",
      "|    learning_rate        | 0.000158     |\n",
      "|    loss                 | 0.007        |\n",
      "|    n_updates            | 14410        |\n",
      "|    policy_gradient_loss | 0.000742     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 1.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1442     |\n",
      "|    time_elapsed    | 5031     |\n",
      "|    total_timesteps | 2907072  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2911104, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0182      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2911104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023255933 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.000152     |\n",
      "|    loss                 | -0.00124     |\n",
      "|    n_updates            | 14430        |\n",
      "|    policy_gradient_loss | 0.000105     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 1.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0143  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1444     |\n",
      "|    time_elapsed    | 5039     |\n",
      "|    total_timesteps | 2911104  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2915136, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0128     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2915136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008361734 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.000145    |\n",
      "|    loss                 | 0.00388     |\n",
      "|    n_updates            | 14450       |\n",
      "|    policy_gradient_loss | -0.000569   |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 3.64e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0138  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1446     |\n",
      "|    time_elapsed    | 5046     |\n",
      "|    total_timesteps | 2915136  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2919168, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.017       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2919168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038439685 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.0802      |\n",
      "|    learning_rate        | 0.000138     |\n",
      "|    loss                 | 0.00272      |\n",
      "|    n_updates            | 14470        |\n",
      "|    policy_gradient_loss | -0.000944    |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 4.39e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0137  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1448     |\n",
      "|    time_elapsed    | 5053     |\n",
      "|    total_timesteps | 2919168  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2923200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005512868 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | -0.0803     |\n",
      "|    learning_rate        | 0.000131    |\n",
      "|    loss                 | 0.00187     |\n",
      "|    n_updates            | 14490       |\n",
      "|    policy_gradient_loss | 0.000542    |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 2.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1450     |\n",
      "|    time_elapsed    | 5060     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2927232, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0114     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2927232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005298527 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.000125    |\n",
      "|    loss                 | -0.00185    |\n",
      "|    n_updates            | 14510       |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 2.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1452     |\n",
      "|    time_elapsed    | 5067     |\n",
      "|    total_timesteps | 2927232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2931264, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.014       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2931264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076106368 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.116       |\n",
      "|    learning_rate        | 0.000118     |\n",
      "|    loss                 | 0.00278      |\n",
      "|    n_updates            | 14530        |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 2.83e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1454     |\n",
      "|    time_elapsed    | 5075     |\n",
      "|    total_timesteps | 2931264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2935296, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2935296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002134616 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | -0.07       |\n",
      "|    learning_rate        | 0.000111    |\n",
      "|    loss                 | -0.000218   |\n",
      "|    n_updates            | 14550       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 6.07e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0149  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1456     |\n",
      "|    time_elapsed    | 5082     |\n",
      "|    total_timesteps | 2935296  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2939328, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0113      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2939328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022775806 |\n",
      "|    clip_fraction        | 0.0657       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | 0.261        |\n",
      "|    learning_rate        | 0.000104     |\n",
      "|    loss                 | 0.000144     |\n",
      "|    n_updates            | 14570        |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 2.72e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1458     |\n",
      "|    time_elapsed    | 5089     |\n",
      "|    total_timesteps | 2939328  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0126     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2943360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001940968 |\n",
      "|    clip_fraction        | 0.0174      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.00624     |\n",
      "|    learning_rate        | 9.78e-05    |\n",
      "|    loss                 | -0.00283    |\n",
      "|    n_updates            | 14590       |\n",
      "|    policy_gradient_loss | 1.86e-05    |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 7.58e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0147  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1460     |\n",
      "|    time_elapsed    | 5096     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2947392, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2947392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011593746 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.0437      |\n",
      "|    learning_rate        | 9.1e-05      |\n",
      "|    loss                 | -0.000927    |\n",
      "|    n_updates            | 14610        |\n",
      "|    policy_gradient_loss | -6.45e-06    |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 2.55e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0144  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1462     |\n",
      "|    time_elapsed    | 5103     |\n",
      "|    total_timesteps | 2947392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2951424, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0141       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2951424       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081084855 |\n",
      "|    clip_fraction        | 0.0126        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 9.86          |\n",
      "|    explained_variance   | 0.204         |\n",
      "|    learning_rate        | 8.43e-05      |\n",
      "|    loss                 | -7.73e-05     |\n",
      "|    n_updates            | 14630         |\n",
      "|    policy_gradient_loss | -0.00017      |\n",
      "|    std                  | 0.00188       |\n",
      "|    value_loss           | 9.53e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1464     |\n",
      "|    time_elapsed    | 5111     |\n",
      "|    total_timesteps | 2951424  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2955456, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0114      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2955456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037875187 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.0703      |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | 0.00145      |\n",
      "|    n_updates            | 14650        |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 3.11e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0136  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1466     |\n",
      "|    time_elapsed    | 5118     |\n",
      "|    total_timesteps | 2955456  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2959488, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.00915    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2959488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005577269 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | -0.092      |\n",
      "|    learning_rate        | 7.09e-05    |\n",
      "|    loss                 | -0.00291    |\n",
      "|    n_updates            | 14670       |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.00188     |\n",
      "|    value_loss           | 2.35e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1468     |\n",
      "|    time_elapsed    | 5125     |\n",
      "|    total_timesteps | 2959488  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0132      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029964051 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.195       |\n",
      "|    learning_rate        | 6.42e-05     |\n",
      "|    loss                 | 0.000627     |\n",
      "|    n_updates            | 14690        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 3.26e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0125  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1470     |\n",
      "|    time_elapsed    | 5132     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2967552, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0145      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2967552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032945303 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.165       |\n",
      "|    learning_rate        | 5.74e-05     |\n",
      "|    loss                 | -0.00348     |\n",
      "|    n_updates            | 14710        |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.00188      |\n",
      "|    value_loss           | 3.79e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0121  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1472     |\n",
      "|    time_elapsed    | 5140     |\n",
      "|    total_timesteps | 2967552  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2971584, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0119      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2971584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004430819 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.86         |\n",
      "|    explained_variance   | -0.131       |\n",
      "|    learning_rate        | 5.07e-05     |\n",
      "|    loss                 | -0.000375    |\n",
      "|    n_updates            | 14730        |\n",
      "|    policy_gradient_loss | -0.000296    |\n",
      "|    std                  | 0.00187      |\n",
      "|    value_loss           | 3.91e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1474     |\n",
      "|    time_elapsed    | 5147     |\n",
      "|    total_timesteps | 2971584  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2975616, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0133      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2975616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049023316 |\n",
      "|    clip_fraction        | 0.0693       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.87         |\n",
      "|    explained_variance   | 0.22         |\n",
      "|    learning_rate        | 4.4e-05      |\n",
      "|    loss                 | -0.0047      |\n",
      "|    n_updates            | 14750        |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    std                  | 0.00187      |\n",
      "|    value_loss           | 1.34e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0124  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1476     |\n",
      "|    time_elapsed    | 5154     |\n",
      "|    total_timesteps | 2975616  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2979648, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0118     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2979648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004914144 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.87        |\n",
      "|    explained_variance   | -0.12       |\n",
      "|    learning_rate        | 3.73e-05    |\n",
      "|    loss                 | -0.00467    |\n",
      "|    n_updates            | 14770       |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    std                  | 0.00187     |\n",
      "|    value_loss           | 3.78e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1478     |\n",
      "|    time_elapsed    | 5161     |\n",
      "|    total_timesteps | 2979648  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0217      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051909816 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.87         |\n",
      "|    explained_variance   | -0.183       |\n",
      "|    learning_rate        | 3.06e-05     |\n",
      "|    loss                 | 0.00574      |\n",
      "|    n_updates            | 14790        |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.00187      |\n",
      "|    value_loss           | 2.53e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0126  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1480     |\n",
      "|    time_elapsed    | 5169     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2987712, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0127      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2987712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060897865 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.87         |\n",
      "|    explained_variance   | -0.121       |\n",
      "|    learning_rate        | 2.38e-05     |\n",
      "|    loss                 | 0.001        |\n",
      "|    n_updates            | 14810        |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    std                  | 0.00187      |\n",
      "|    value_loss           | 3.24e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1482     |\n",
      "|    time_elapsed    | 5176     |\n",
      "|    total_timesteps | 2987712  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2991744, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -0.0116     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2991744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002222739 |\n",
      "|    clip_fraction        | 0.000446    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 9.87        |\n",
      "|    explained_variance   | -0.189      |\n",
      "|    learning_rate        | 1.71e-05    |\n",
      "|    loss                 | -0.000172   |\n",
      "|    n_updates            | 14830       |\n",
      "|    policy_gradient_loss | -0.000358   |\n",
      "|    std                  | 0.00187     |\n",
      "|    value_loss           | 3.42e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1484     |\n",
      "|    time_elapsed    | 5183     |\n",
      "|    total_timesteps | 2991744  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2995776, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 251           |\n",
      "|    mean_reward          | -0.0123       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2995776       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030970448 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 9.87          |\n",
      "|    explained_variance   | -0.0885       |\n",
      "|    learning_rate        | 1.04e-05      |\n",
      "|    loss                 | 0.000344      |\n",
      "|    n_updates            | 14850         |\n",
      "|    policy_gradient_loss | 9.33e-05      |\n",
      "|    std                  | 0.00187       |\n",
      "|    value_loss           | 4.76e-08      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0128  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1486     |\n",
      "|    time_elapsed    | 5190     |\n",
      "|    total_timesteps | 2995776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2999808, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 251.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | -0.0108      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2999808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022102918 |\n",
      "|    clip_fraction        | 0.000446     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 9.87         |\n",
      "|    explained_variance   | 0.0551       |\n",
      "|    learning_rate        | 3.68e-06     |\n",
      "|    loss                 | -0.00595     |\n",
      "|    n_updates            | 14870        |\n",
      "|    policy_gradient_loss | -0.0009      |\n",
      "|    std                  | 0.00187      |\n",
      "|    value_loss           | 9.95e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 251      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 1488     |\n",
      "|    time_elapsed    | 5197     |\n",
      "|    total_timesteps | 2999808  |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Eval num_timesteps=80640, episode_reward=-0.59 +/- 0.10\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.589      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008735238 |\n",
      "|    clip_fraction        | 0.0504      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | -0.0532     |\n",
      "|    learning_rate        | 0.00493     |\n",
      "|    loss                 | -0.00188    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00208    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -16.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 758      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.66 +/- 0.10\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.04e+03   |\n",
      "|    mean_reward          | -0.655     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 161280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02024898 |\n",
      "|    clip_fraction        | 0.0777     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.49      |\n",
      "|    explained_variance   | 0.0319     |\n",
      "|    learning_rate        | 0.0048     |\n",
      "|    loss                 | -0.00565   |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.00497   |\n",
      "|    std                  | 0.83       |\n",
      "|    value_loss           | 0.000195   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -15.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 723      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.41 +/- 0.10\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.413      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016194323 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 0.0585      |\n",
      "|    learning_rate        | 0.00466     |\n",
      "|    loss                 | -0.00571    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    std                  | 0.733       |\n",
      "|    value_loss           | 0.000875    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -14.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 713      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.53 +/- 0.14\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.526      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023386749 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00453     |\n",
      "|    loss                 | -0.00659    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    std                  | 0.629       |\n",
      "|    value_loss           | 0.00087     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -13.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 455      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.58 +/- 0.14\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.585      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028588982 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0044      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00783    |\n",
      "|    std                  | 0.556       |\n",
      "|    value_loss           | 6.41e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -12.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 571      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.52 +/- 0.10\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.04e+03   |\n",
      "|    mean_reward          | -0.524     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 483840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02025446 |\n",
      "|    clip_fraction        | 0.0782     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.16      |\n",
      "|    explained_variance   | 0.0995     |\n",
      "|    learning_rate        | 0.00426    |\n",
      "|    loss                 | -0.00653   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00378   |\n",
      "|    std                  | 0.475      |\n",
      "|    value_loss           | 0.000293   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -11.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 687      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.46 +/- 0.16\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.455      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021899018 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.777      |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00413     |\n",
      "|    loss                 | -0.00656    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 0.407       |\n",
      "|    value_loss           | 8.38e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -10      |\n",
      "| time/              |          |\n",
      "|    fps             | 702      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 803      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.44 +/- 0.14\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.442      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019699642 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00399     |\n",
      "|    loss                 | -0.00494    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    std                  | 0.347       |\n",
      "|    value_loss           | 4.95e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -8.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 919      |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.44 +/- 0.14\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.439      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022239752 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0083     |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.00386     |\n",
      "|    loss                 | -0.0094     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    std                  | 0.296       |\n",
      "|    value_loss           | 1.87e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -6.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 700      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 1036     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.26 +/- 0.15\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.258      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023085363 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.361       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00372     |\n",
      "|    loss                 | -0.00748    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    std                  | 0.247       |\n",
      "|    value_loss           | 1.39e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -5.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1152     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.30 +/- 0.14\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.299      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017984558 |\n",
      "|    clip_fraction        | 0.0712      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.729       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00359     |\n",
      "|    loss                 | -0.00577    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    std                  | 0.206       |\n",
      "|    value_loss           | 2.18e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -4.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 699      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 1268     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.29 +/- 0.10\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.291      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022469617 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.13        |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.00345     |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 6.6e-06     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -3.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1384     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.24 +/- 0.08\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.238      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011678753 |\n",
      "|    clip_fraction        | 0.0398      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.46        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.00332     |\n",
      "|    loss                 | -0.00745    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -2.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1500     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.30 +/- 0.12\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.297      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021205055 |\n",
      "|    clip_fraction        | 0.0869      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.83        |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.00319     |\n",
      "|    loss                 | -0.00669    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 3.61e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -2.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 1616     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.31 +/- 0.13\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.311      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020370422 |\n",
      "|    clip_fraction        | 0.0905      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.19        |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00305     |\n",
      "|    loss                 | -0.00746    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    std                  | 0.103       |\n",
      "|    value_loss           | 2.05e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -2.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 1732     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.28 +/- 0.08\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.275      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018499315 |\n",
      "|    clip_fraction        | 0.0716      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.53        |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00292     |\n",
      "|    loss                 | -0.00807    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.0858      |\n",
      "|    value_loss           | 1.55e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -1.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 1848     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.32 +/- 0.09\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.323      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008293956 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.84        |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00278     |\n",
      "|    loss                 | -0.00321    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    std                  | 0.0746      |\n",
      "|    value_loss           | 4.24e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -1.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 1963     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.30 +/- 0.08\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.295      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013817736 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.15        |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00265     |\n",
      "|    loss                 | -0.00662    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    std                  | 0.0635      |\n",
      "|    value_loss           | 1.57e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -1.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 2079     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.26 +/- 0.07\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.265      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009947016 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.41        |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.00251     |\n",
      "|    loss                 | -0.0054     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    std                  | 0.0559      |\n",
      "|    value_loss           | 7.86e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -1.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 2194     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.27 +/- 0.09\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.272      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012480624 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.7         |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.00238     |\n",
      "|    loss                 | -0.00716    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    std                  | 0.0483      |\n",
      "|    value_loss           | 1.04e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.917   |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 2310     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.31 +/- 0.09\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.313      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011368636 |\n",
      "|    clip_fraction        | 0.0593      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.94        |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00224     |\n",
      "|    loss                 | -0.00336    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    std                  | 0.0431      |\n",
      "|    value_loss           | 4.83e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.811   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 2426     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.32 +/- 0.12\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.321      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013377115 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.17        |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00211     |\n",
      "|    loss                 | -0.00285    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    std                  | 0.0381      |\n",
      "|    value_loss           | 2.89e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.714   |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 2541     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.24 +/- 0.07\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.237      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011025008 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.37        |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00198     |\n",
      "|    loss                 | -0.00581    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    std                  | 0.0345      |\n",
      "|    value_loss           | 2.59e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.642   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 2657     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.31 +/- 0.05\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.04e+03   |\n",
      "|    mean_reward          | -0.313     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1935360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00796985 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 4.5        |\n",
      "|    explained_variance   | 0.182      |\n",
      "|    learning_rate        | 0.00184    |\n",
      "|    loss                 | -0.00197   |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | 0.00106    |\n",
      "|    std                  | 0.0328     |\n",
      "|    value_loss           | 5.72e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.594   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 2773     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.27 +/- 0.09\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.274      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008865293 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.61        |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00171     |\n",
      "|    loss                 | 0.000449    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    std                  | 0.0308      |\n",
      "|    value_loss           | 6.73e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.561   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 2890     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.32 +/- 0.11\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.04e+03     |\n",
      "|    mean_reward          | -0.322       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2096640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061066933 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.75         |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 0.00157      |\n",
      "|    loss                 | 0.000458     |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.000978    |\n",
      "|    std                  | 0.0288       |\n",
      "|    value_loss           | 4.77e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.522   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 3006     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.24 +/- 0.05\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.04e+03     |\n",
      "|    mean_reward          | -0.238       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2177280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060303556 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.88         |\n",
      "|    explained_variance   | 0.351        |\n",
      "|    learning_rate        | 0.00144      |\n",
      "|    loss                 | -0.00312     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    std                  | 0.027        |\n",
      "|    value_loss           | 2.11e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.495   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 3122     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.26 +/- 0.06\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.263      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007983086 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.99        |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.0013      |\n",
      "|    loss                 | 0.000801    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    std                  | 0.0256      |\n",
      "|    value_loss           | 4.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.475   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 3238     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.27 +/- 0.10\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.271      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005008937 |\n",
      "|    clip_fraction        | 0.0824      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.09        |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.00117     |\n",
      "|    loss                 | -0.00111    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.000439   |\n",
      "|    std                  | 0.0245      |\n",
      "|    value_loss           | 2.77e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 3354     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.23 +/- 0.09\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.233      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037486274 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.17        |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00216     |\n",
      "|    std                  | 0.0234      |\n",
      "|    value_loss           | 3.25e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.446   |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 3471     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.23 +/- 0.08\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.234      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005621202 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.22        |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.000901    |\n",
      "|    loss                 | -0.00413    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    std                  | 0.0228      |\n",
      "|    value_loss           | 1.02e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.426   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 3586     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.23 +/- 0.11\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.228      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005601148 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.28        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.000766    |\n",
      "|    loss                 | -0.00143    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    std                  | 0.0222      |\n",
      "|    value_loss           | 2.38e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.411   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 3701     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.22 +/- 0.06\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.22       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008690584 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.34        |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.000632    |\n",
      "|    loss                 | 0.000557    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.000746   |\n",
      "|    std                  | 0.0215      |\n",
      "|    value_loss           | 5.17e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.413   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 3816     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.25 +/- 0.05\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.04e+03    |\n",
      "|    mean_reward          | -0.252      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005893987 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.39        |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.000498    |\n",
      "|    loss                 | -0.00405    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.0211      |\n",
      "|    value_loss           | 2.06e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.404   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 3932     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.20 +/- 0.04\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.04e+03     |\n",
      "|    mean_reward          | -0.198       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2822400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043917135 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.42         |\n",
      "|    explained_variance   | 0.234        |\n",
      "|    learning_rate        | 0.000363     |\n",
      "|    loss                 | -0.000367    |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    std                  | 0.0207       |\n",
      "|    value_loss           | 6.26e-07     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.403   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 4048     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.20 +/- 0.06\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.04e+03     |\n",
      "|    mean_reward          | -0.197       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2903040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033844626 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.45         |\n",
      "|    explained_variance   | 0.29         |\n",
      "|    learning_rate        | 0.000229     |\n",
      "|    loss                 | -0.0018      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.0204       |\n",
      "|    value_loss           | 2.47e-07     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.394   |\n",
      "| time/              |          |\n",
      "|    fps             | 697      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 4164     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.23 +/- 0.06\n",
      "Episode length: 5039.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.04e+03     |\n",
      "|    mean_reward          | -0.233       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037411186 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.48         |\n",
      "|    explained_variance   | 0.396        |\n",
      "|    learning_rate        | 9.44e-05     |\n",
      "|    loss                 | -0.00273     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 0.0202       |\n",
      "|    value_loss           | 4.29e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.04e+03 |\n",
      "|    ep_rew_mean     | -0.391   |\n",
      "| time/              |          |\n",
      "|    fps             | 696      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 4280     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x15de30fc490>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Horizons\n",
    "\n",
    "# Default Big Batch\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]),filename='eval_logs-Train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelB',\n",
    "    log_path='./logs/eval_logs/evB',\n",
    "    eval_freq=252*20*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*10*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*20*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# Default Lower LR\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]),filename='eval_logs-Train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelLLR',\n",
    "    log_path='./logs/eval_logs/evLLR',\n",
    "    eval_freq=252*20*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.001), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# Default Both\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs')\n",
    "]),filename='eval_logs-Train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelLLRB',\n",
    "    log_path='./logs/eval_logs/evLLRB',\n",
    "    eval_freq=252*20*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*10*5, learning_rate=linear_schedule(0.001), policy_kwargs=policy_kwargs, n_steps=252*20*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "## 1 yr\n",
    "#envs = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs', resetdate=1.0),\n",
    "#    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs',resetdate=1.0)\n",
    "#]),filename='logs1-Train')\n",
    "#ev_env = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs',resetdate=1.0),\n",
    "#]))\n",
    "#\n",
    "#eval_callback = EvalCallback(\n",
    "#    ev_env,\n",
    "#    best_model_save_path='./logs/best_model1',\n",
    "#    log_path='./logs/eval_logs1/ev',\n",
    "#    eval_freq=252*8*1,\n",
    "#     deterministic=True,\n",
    "#    render=False,\n",
    "#    verbose = True,\n",
    "#    n_eval_episodes = 8\n",
    "#)\n",
    "\n",
    "# Instantiate the agent\n",
    "#model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*1, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*1, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "#model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# 20 yr\n",
    "#envs = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'xs',resetdate=20.0),\n",
    "#    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'xs',resetdate=20.0)\n",
    "#]),filename='/logs/eval_logs20/Train')\n",
    "#ev_env = VecMonitor(DummyVecEnv([\n",
    "#    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs',resetdate=20.0),\n",
    "#]))\n",
    "\n",
    "#eval_callback = EvalCallback(\n",
    "#    ev_env,\n",
    "#    best_model_save_path='./logs/best_model20',\n",
    "#    log_path='./logs/eval_logs20/ev',\n",
    "#    eval_freq=252*8*20,\n",
    "#    deterministic=True,\n",
    "#    render=False,\n",
    "#    verbose = True,\n",
    "#    n_eval_episodes = 8\n",
    "#)\n",
    "#\n",
    "#model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*20, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*20, normalize_advantage=True, gamma = 0.9, verbose = 1) #\n",
    "\n",
    "#model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0821     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010723251 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | -1.45       |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.00504    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    std                  | 0.955       |\n",
      "|    value_loss           | 0.0433      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.11 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 1.26e+03 |\n",
      "|    mean_reward          | -0.113   |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 40320    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.025522 |\n",
      "|    clip_fraction        | 0.101    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -2.5     |\n",
      "|    explained_variance   | -0.0132  |\n",
      "|    learning_rate        | 0.00495  |\n",
      "|    loss                 | -0.00994 |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | -0.00723 |\n",
      "|    std                  | 0.827    |\n",
      "|    value_loss           | 8.56e-05 |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.16 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017175281 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.0236      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.00521    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 0.000308    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.21    |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.29 +/- 0.13\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.292      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023156755 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    std                  | 0.638       |\n",
      "|    value_loss           | 7.01e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.21 +/- 0.10\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.214      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020326426 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    std                  | 0.546       |\n",
      "|    value_loss           | 6.53e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 619      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.13 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.128      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018459143 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.0776     |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.459       |\n",
      "|    value_loss           | 9.32e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.15 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.152      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019727128 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.756      |\n",
      "|    explained_variance   | 0.0468      |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.388       |\n",
      "|    value_loss           | 1.83e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 620      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.14 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.136      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029839583 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.0839      |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.312       |\n",
      "|    value_loss           | 2.07e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022112578 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0664      |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.258       |\n",
      "|    value_loss           | 5.59e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0916     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020685723 |\n",
      "|    clip_fraction        | 0.0706      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.401       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    std                  | 0.212       |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0754     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021335598 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.85        |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 2.33e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.09    |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 355      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0822     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024510479 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.21        |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | -0.00932    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0098     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 3.15e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.901   |\n",
      "| time/              |          |\n",
      "|    fps             | 626      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.111      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023239316 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.6         |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00829    |\n",
      "|    std                  | 0.118       |\n",
      "|    value_loss           | 2.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.755   |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 417      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0929      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 282240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0132650845 |\n",
      "|    clip_fraction        | 0.066        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 1.94         |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.00455      |\n",
      "|    loss                 | -0.0076      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00469     |\n",
      "|    std                  | 0.0998       |\n",
      "|    value_loss           | 8.29e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.646   |\n",
      "| time/              |          |\n",
      "|    fps             | 629      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 448      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0967     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009220513 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.26        |\n",
      "|    explained_variance   | 0.0821      |\n",
      "|    learning_rate        | 0.00451     |\n",
      "|    loss                 | -0.00512    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    std                  | 0.0875      |\n",
      "|    value_loss           | 8.49e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 479      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017336858 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.5         |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.00448     |\n",
      "|    loss                 | -0.00989    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    std                  | 0.0769      |\n",
      "|    value_loss           | 1.52e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.462   |\n",
      "| time/              |          |\n",
      "|    fps             | 630      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 511      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.068      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019976418 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.82        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.00639    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00859    |\n",
      "|    std                  | 0.0658      |\n",
      "|    value_loss           | 6.08e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.398   |\n",
      "| time/              |          |\n",
      "|    fps             | 631      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 542      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0765    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 362880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02256465 |\n",
      "|    clip_fraction        | 0.0982     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.19       |\n",
      "|    explained_variance   | 0.385      |\n",
      "|    learning_rate        | 0.00441    |\n",
      "|    loss                 | -0.0132    |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.0538     |\n",
      "|    value_loss           | 2.55e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.347   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 574      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0931     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015039275 |\n",
      "|    clip_fraction        | 0.0814      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.48        |\n",
      "|    explained_variance   | 0.0443      |\n",
      "|    learning_rate        | 0.00438     |\n",
      "|    loss                 | -0.00493    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00545    |\n",
      "|    std                  | 0.0468      |\n",
      "|    value_loss           | 5.62e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.303   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 606      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0953     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 403200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008537458 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.73        |\n",
      "|    explained_variance   | 0.0732      |\n",
      "|    learning_rate        | 0.00434     |\n",
      "|    loss                 | 0.00148     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.000596   |\n",
      "|    std                  | 0.0423      |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.266   |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 637      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0834      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068197325 |\n",
      "|    clip_fraction        | 0.0737       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.91         |\n",
      "|    explained_variance   | 0.109        |\n",
      "|    learning_rate        | 0.00431      |\n",
      "|    loss                 | -0.00452     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.0383       |\n",
      "|    value_loss           | 7.9e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.233   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 668      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.096      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 443520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011853132 |\n",
      "|    clip_fraction        | 0.0698      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.1         |\n",
      "|    explained_variance   | -0.0578     |\n",
      "|    learning_rate        | 0.00428     |\n",
      "|    loss                 | -0.00427    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    std                  | 0.0346      |\n",
      "|    value_loss           | 2.37e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.203   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 700      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.099      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013406276 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.36        |\n",
      "|    explained_variance   | 0.0239      |\n",
      "|    learning_rate        | 0.00424     |\n",
      "|    loss                 | -0.00488    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    std                  | 0.0308      |\n",
      "|    value_loss           | 2.67e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.179   |\n",
      "| time/              |          |\n",
      "|    fps             | 633      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 731      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0775     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004034814 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.53        |\n",
      "|    explained_variance   | 0.0945      |\n",
      "|    learning_rate        | 0.00421     |\n",
      "|    loss                 | -0.0014     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 9.85e-05    |\n",
      "|    std                  | 0.0284      |\n",
      "|    value_loss           | 1.14e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.167   |\n",
      "| time/              |          |\n",
      "|    fps             | 634      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 762      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0718      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110414345 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 4.72         |\n",
      "|    explained_variance   | -0.0568      |\n",
      "|    learning_rate        | 0.00418      |\n",
      "|    loss                 | -0.000749    |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    std                  | 0.0254       |\n",
      "|    value_loss           | 3.88e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.154   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0968     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014865362 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.88        |\n",
      "|    explained_variance   | -0.0045     |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | 0.01        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00209     |\n",
      "|    std                  | 0.0237      |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.146   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 824      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008424804 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 4.98        |\n",
      "|    explained_variance   | -0.22       |\n",
      "|    learning_rate        | 0.00411     |\n",
      "|    loss                 | 0.000843    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    std                  | 0.0225      |\n",
      "|    value_loss           | 1.21e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.139   |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 855      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0919     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012272627 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.11        |\n",
      "|    explained_variance   | -0.773      |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | 0.00805     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | 0.00333     |\n",
      "|    std                  | 0.0212      |\n",
      "|    value_loss           | 1.62e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.131   |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 887      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0747     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010833957 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.25        |\n",
      "|    explained_variance   | -0.314      |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | 0.0042      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00081    |\n",
      "|    std                  | 0.0198      |\n",
      "|    value_loss           | 2.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.129   |\n",
      "| time/              |          |\n",
      "|    fps             | 636      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 918      |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.13 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.132      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007425678 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.32        |\n",
      "|    explained_variance   | -0.0836     |\n",
      "|    learning_rate        | 0.00401     |\n",
      "|    loss                 | -0.00257    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.000862   |\n",
      "|    std                  | 0.019       |\n",
      "|    value_loss           | 7.27e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.122   |\n",
      "| time/              |          |\n",
      "|    fps             | 638      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 947      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.079      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 624960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004356243 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.46        |\n",
      "|    explained_variance   | -0.0769     |\n",
      "|    learning_rate        | 0.00398     |\n",
      "|    loss                 | -0.003      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | 0.000498    |\n",
      "|    std                  | 0.0178      |\n",
      "|    value_loss           | 5e-07       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.119   |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 977      |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0935      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 645120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011667311 |\n",
      "|    clip_fraction        | 0.135        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.54         |\n",
      "|    explained_variance   | 0.0521       |\n",
      "|    learning_rate        | 0.00394      |\n",
      "|    loss                 | -0.00116     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | 0.00206      |\n",
      "|    std                  | 0.0173       |\n",
      "|    value_loss           | 6.71e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.115   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 1007     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0786     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009847259 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.58        |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.00391     |\n",
      "|    loss                 | 0.00715     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.0034      |\n",
      "|    std                  | 0.0169      |\n",
      "|    value_loss           | 2.1e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.111   |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 1037     |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.10 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.101     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 685440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00878986 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 5.69       |\n",
      "|    explained_variance   | -0.0513    |\n",
      "|    learning_rate        | 0.00387    |\n",
      "|    loss                 | 0.00251    |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | 0.000355   |\n",
      "|    std                  | 0.0159     |\n",
      "|    value_loss           | 5.05e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1067     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0889     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009499628 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.79        |\n",
      "|    explained_variance   | -0.048      |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | -0.00116    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00336     |\n",
      "|    std                  | 0.0152      |\n",
      "|    value_loss           | 8.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1097     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0984     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012863376 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.85        |\n",
      "|    explained_variance   | 0.083       |\n",
      "|    learning_rate        | 0.00381     |\n",
      "|    loss                 | -0.00301    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.00173     |\n",
      "|    std                  | 0.0149      |\n",
      "|    value_loss           | 1.86e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1127     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.101       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 745920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038115887 |\n",
      "|    clip_fraction        | 0.0824       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 5.9          |\n",
      "|    explained_variance   | 0.253        |\n",
      "|    learning_rate        | 0.00377      |\n",
      "|    loss                 | -0.00293     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | 0.000942     |\n",
      "|    std                  | 0.0145       |\n",
      "|    value_loss           | 1.35e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.107   |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1157     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.114      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002480811 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | -0.023      |\n",
      "|    learning_rate        | 0.00374     |\n",
      "|    loss                 | 0.00258     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -1.83e-05   |\n",
      "|    std                  | 0.0142      |\n",
      "|    value_loss           | 1.24e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.106   |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1186     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0905     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009796102 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 5.99        |\n",
      "|    explained_variance   | 0.034       |\n",
      "|    learning_rate        | 0.00371     |\n",
      "|    loss                 | 0.0081      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | 0.00481     |\n",
      "|    std                  | 0.014       |\n",
      "|    value_loss           | 3.75e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.108   |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1220     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.08       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022207394 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.06        |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.00275     |\n",
      "|    std                  | 0.0135      |\n",
      "|    value_loss           | 1.69e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.109   |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1253     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 826560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087083345 |\n",
      "|    clip_fraction        | 0.0859       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.11         |\n",
      "|    explained_variance   | 0.0412       |\n",
      "|    learning_rate        | 0.00364      |\n",
      "|    loss                 | -0.00339     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.0132       |\n",
      "|    value_loss           | 7.2e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.108   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1289     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0813      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 846720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077942666 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.2          |\n",
      "|    explained_variance   | 0.0249       |\n",
      "|    learning_rate        | 0.00361      |\n",
      "|    loss                 | 0.00396      |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.00101      |\n",
      "|    std                  | 0.0126       |\n",
      "|    value_loss           | 4.75e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1321     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0815    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 866880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03337884 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.25       |\n",
      "|    explained_variance   | -0.0908    |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | 0.0532     |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | 0.00911    |\n",
      "|    std                  | 0.0124     |\n",
      "|    value_loss           | 6.7e-08    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1352     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0854     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011429547 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.0396      |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | -0.00281    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 0.00417     |\n",
      "|    std                  | 0.0123      |\n",
      "|    value_loss           | 4.05e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0984  |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1384     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0699     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 907200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006768914 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.0035      |\n",
      "|    loss                 | 0.000317    |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | 0.00719     |\n",
      "|    std                  | 0.0119      |\n",
      "|    value_loss           | 1.81e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1416     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0839      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 927360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046077454 |\n",
      "|    clip_fraction        | 0.275        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.37         |\n",
      "|    explained_variance   | 0.541        |\n",
      "|    learning_rate        | 0.00347      |\n",
      "|    loss                 | -0.00109     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | 0.0125       |\n",
      "|    std                  | 0.0116       |\n",
      "|    value_loss           | 1.05e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1447     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0805     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010386317 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.41        |\n",
      "|    explained_variance   | 0.0248      |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | 0.00787     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | 0.00514     |\n",
      "|    std                  | 0.0113      |\n",
      "|    value_loss           | 9.02e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1479     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0915     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011046486 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.48        |\n",
      "|    explained_variance   | -0.147      |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | -0.000855   |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.00522     |\n",
      "|    std                  | 0.0109      |\n",
      "|    value_loss           | 3.79e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1510     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.092     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 987840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01639882 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.55       |\n",
      "|    explained_variance   | -0.175     |\n",
      "|    learning_rate        | 0.00337    |\n",
      "|    loss                 | 0.00147    |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.000768  |\n",
      "|    std                  | 0.0106     |\n",
      "|    value_loss           | 1.99e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1542     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0823     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006566501 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.59        |\n",
      "|    explained_variance   | 0.0144      |\n",
      "|    learning_rate        | 0.00334     |\n",
      "|    loss                 | -0.000828   |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | 0.00281     |\n",
      "|    std                  | 0.0105      |\n",
      "|    value_loss           | 8.14e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1573     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0971    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1028160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00720708 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.0033     |\n",
      "|    loss                 | 0.00165    |\n",
      "|    n_updates            | 1010       |\n",
      "|    policy_gradient_loss | 0.00242    |\n",
      "|    std                  | 0.0101     |\n",
      "|    value_loss           | 8.86e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0998  |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1605     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0806      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1048320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099548735 |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.68         |\n",
      "|    explained_variance   | 0.269        |\n",
      "|    learning_rate        | 0.00327      |\n",
      "|    loss                 | 0.00427      |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | 0.00442      |\n",
      "|    std                  | 0.01         |\n",
      "|    value_loss           | 1.05e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1637     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.109     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1068480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04811139 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.72       |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.00324    |\n",
      "|    loss                 | 0.0386     |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | 0.00897    |\n",
      "|    std                  | 0.00976    |\n",
      "|    value_loss           | 1.47e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1668     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.091      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017303891 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.74        |\n",
      "|    explained_variance   | -0.00527    |\n",
      "|    learning_rate        | 0.0032      |\n",
      "|    loss                 | 0.0226      |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.00974     |\n",
      "|    std                  | 0.00964     |\n",
      "|    value_loss           | 8.34e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.103   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1699     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0914      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1108800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045084055 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.78         |\n",
      "|    explained_variance   | -0.122       |\n",
      "|    learning_rate        | 0.00317      |\n",
      "|    loss                 | 0.0043       |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | 0.0065       |\n",
      "|    std                  | 0.00951      |\n",
      "|    value_loss           | 6.6e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1731     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0921     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034084328 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.81        |\n",
      "|    explained_variance   | 0.0223      |\n",
      "|    learning_rate        | 0.00314     |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    std                  | 0.00934     |\n",
      "|    value_loss           | 1.09e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1762     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.09       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023455383 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.85        |\n",
      "|    explained_variance   | 0.0465      |\n",
      "|    learning_rate        | 0.0031      |\n",
      "|    loss                 | 0.00099     |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    std                  | 0.00918     |\n",
      "|    value_loss           | 7.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1794     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0989      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1169280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066533946 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 6.87         |\n",
      "|    explained_variance   | 0.286        |\n",
      "|    learning_rate        | 0.00307      |\n",
      "|    loss                 | -0.00279     |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    std                  | 0.00908      |\n",
      "|    value_loss           | 8.39e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0993  |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1826     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.103     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1189440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03472507 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.89       |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.00303    |\n",
      "|    loss                 | 0.0484     |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | 0.0253     |\n",
      "|    std                  | 0.00895    |\n",
      "|    value_loss           | 3.24e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1857     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0942     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012181774 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | -0.0179     |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | 0.0069      |\n",
      "|    std                  | 0.00891     |\n",
      "|    value_loss           | 1.28e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1888     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.111      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013691448 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.91        |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00297     |\n",
      "|    loss                 | 0.00099     |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.00381     |\n",
      "|    std                  | 0.00895     |\n",
      "|    value_loss           | 1.16e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.105   |\n",
      "| time/              |          |\n",
      "|    fps             | 640      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 1918     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029722001 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 0.021       |\n",
      "|    std                  | 0.00892     |\n",
      "|    value_loss           | 1.4e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 1948     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0962     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007634489 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.0029      |\n",
      "|    loss                 | 0.000801    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.00152     |\n",
      "|    std                  | 0.00895     |\n",
      "|    value_loss           | 1.11e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 1978     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.11      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1290240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00888527 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 6.96       |\n",
      "|    explained_variance   | 0.00604    |\n",
      "|    learning_rate        | 0.00287    |\n",
      "|    loss                 | 0.00162    |\n",
      "|    n_updates            | 1270       |\n",
      "|    policy_gradient_loss | 0.00691    |\n",
      "|    std                  | 0.00882    |\n",
      "|    value_loss           | 8.63e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 2008     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0922     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009660026 |\n",
      "|    clip_fraction        | 0.0969      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.01        |\n",
      "|    explained_variance   | 0.0234      |\n",
      "|    learning_rate        | 0.00283     |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | 0.00317     |\n",
      "|    std                  | 0.00851     |\n",
      "|    value_loss           | 1.12e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0999  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 2038     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.12 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.124     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1330560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03209876 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.04       |\n",
      "|    explained_variance   | 0.0387     |\n",
      "|    learning_rate        | 0.0028     |\n",
      "|    loss                 | 0.00573    |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | 0.00787    |\n",
      "|    std                  | 0.00841    |\n",
      "|    value_loss           | 8.04e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0964  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2069     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0802     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017232325 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.06        |\n",
      "|    explained_variance   | 0.0909      |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | 0.00145     |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.0068      |\n",
      "|    std                  | 0.00837     |\n",
      "|    value_loss           | 9.19e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0968  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2100     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0974    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1370880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00633361 |\n",
      "|    clip_fraction        | 0.105      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.1        |\n",
      "|    explained_variance   | -0.0556    |\n",
      "|    learning_rate        | 0.00273    |\n",
      "|    loss                 | -0.00275   |\n",
      "|    n_updates            | 1350       |\n",
      "|    policy_gradient_loss | 0.000705   |\n",
      "|    std                  | 0.00813    |\n",
      "|    value_loss           | 6.62e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0956  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2132     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.069       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1391040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076019475 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.15         |\n",
      "|    explained_variance   | -0.0179      |\n",
      "|    learning_rate        | 0.0027       |\n",
      "|    loss                 | -0.00207     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | 0.00651      |\n",
      "|    std                  | 0.00798      |\n",
      "|    value_loss           | 5.15e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.092   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 2163     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0999      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1411200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039361166 |\n",
      "|    clip_fraction        | 0.211        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.17         |\n",
      "|    explained_variance   | 0.286        |\n",
      "|    learning_rate        | 0.00266      |\n",
      "|    loss                 | -0.00205     |\n",
      "|    n_updates            | 1390         |\n",
      "|    policy_gradient_loss | 0.01         |\n",
      "|    std                  | 0.00793      |\n",
      "|    value_loss           | 1.18e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0939  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2195     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.09 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0876     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011334674 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.2         |\n",
      "|    explained_variance   | 0.0703      |\n",
      "|    learning_rate        | 0.00263     |\n",
      "|    loss                 | 0.00223     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | 0.0093      |\n",
      "|    std                  | 0.0078      |\n",
      "|    value_loss           | 1.33e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0938  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2226     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0936     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007444677 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.23        |\n",
      "|    explained_variance   | 0.0339      |\n",
      "|    learning_rate        | 0.0026      |\n",
      "|    loss                 | 0.00366     |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | 0.000797    |\n",
      "|    std                  | 0.00767     |\n",
      "|    value_loss           | 4.3e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0942  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2257     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.118       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1471680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065627783 |\n",
      "|    clip_fraction        | 0.185        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.26         |\n",
      "|    explained_variance   | 0.0313       |\n",
      "|    learning_rate        | 0.00256      |\n",
      "|    loss                 | 0.0135       |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | 0.00719      |\n",
      "|    std                  | 0.00751      |\n",
      "|    value_loss           | 1.08e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0937  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 2289     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.087      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019422768 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.29        |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | 0.0277      |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.0094      |\n",
      "|    std                  | 0.00748     |\n",
      "|    value_loss           | 2.91e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0953  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 2321     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.101     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1512000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02030265 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.32       |\n",
      "|    explained_variance   | 0.0267     |\n",
      "|    learning_rate        | 0.0025     |\n",
      "|    loss                 | 0.00789    |\n",
      "|    n_updates            | 1490       |\n",
      "|    policy_gradient_loss | 0.00588    |\n",
      "|    std                  | 0.00739    |\n",
      "|    value_loss           | 1.78e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0983  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2352     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0947     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1532160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040407185 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.35        |\n",
      "|    explained_variance   | 0.0638      |\n",
      "|    learning_rate        | 0.00246     |\n",
      "|    loss                 | -0.000337   |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.00593     |\n",
      "|    std                  | 0.00721     |\n",
      "|    value_loss           | 3.01e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2383     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0958     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012411038 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.39        |\n",
      "|    explained_variance   | 0.0602      |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | 0.00144     |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | 0.00558     |\n",
      "|    std                  | 0.00711     |\n",
      "|    value_loss           | 8.05e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.101   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 2415     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0703    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1572480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01810858 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.42       |\n",
      "|    explained_variance   | 0.0113     |\n",
      "|    learning_rate        | 0.0024     |\n",
      "|    loss                 | 0.00856    |\n",
      "|    n_updates            | 1550       |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    std                  | 0.00707    |\n",
      "|    value_loss           | 1.31e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0983  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2447     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0919      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1592640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047726235 |\n",
      "|    clip_fraction        | 0.157        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.45         |\n",
      "|    explained_variance   | 0.317        |\n",
      "|    learning_rate        | 0.00236      |\n",
      "|    loss                 | -0.000972    |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | 0.00491      |\n",
      "|    std                  | 0.00699      |\n",
      "|    value_loss           | 3.2e-05      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0999  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2478     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.104     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1612800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02242171 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 7.48       |\n",
      "|    explained_variance   | 0.245      |\n",
      "|    learning_rate        | 0.00233    |\n",
      "|    loss                 | 0.0264     |\n",
      "|    n_updates            | 1590       |\n",
      "|    policy_gradient_loss | 0.0251     |\n",
      "|    std                  | 0.00692    |\n",
      "|    value_loss           | 4.58e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0993  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2510     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0882     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016115362 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.49        |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | -0.000165   |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | 0.00517     |\n",
      "|    std                  | 0.00688     |\n",
      "|    value_loss           | 1.92e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0987  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2542     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0889     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1653120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008250698 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.53        |\n",
      "|    explained_variance   | -0.0375     |\n",
      "|    learning_rate        | 0.00226     |\n",
      "|    loss                 | 0.0017      |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.00217     |\n",
      "|    std                  | 0.00668     |\n",
      "|    value_loss           | 3.86e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0987  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 2573     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0873     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003378028 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.57        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | 0.000878    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.00582     |\n",
      "|    std                  | 0.00659     |\n",
      "|    value_loss           | 1.58e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0984  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2605     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0908     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1693440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011858498 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.59        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.00219     |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | 0.00426     |\n",
      "|    std                  | 0.00657     |\n",
      "|    value_loss           | 1.71e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0977  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 2636     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0958      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1713600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032127225 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.63         |\n",
      "|    explained_variance   | 0.0948       |\n",
      "|    learning_rate        | 0.00216      |\n",
      "|    loss                 | 0.000761     |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | 0.00443      |\n",
      "|    std                  | 0.00647      |\n",
      "|    value_loss           | 1.36e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.096   |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 2667     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0794      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1733760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039362116 |\n",
      "|    clip_fraction        | 0.25         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.66         |\n",
      "|    explained_variance   | 0.0924       |\n",
      "|    learning_rate        | 0.00213      |\n",
      "|    loss                 | 0.000622     |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | 0.0121       |\n",
      "|    std                  | 0.00629      |\n",
      "|    value_loss           | 4.85e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0969  |\n",
      "| time/              |          |\n",
      "|    fps             | 642      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 2697     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1753920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007543559 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.68        |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.00209     |\n",
      "|    loss                 | 0.000276    |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | 0.00205     |\n",
      "|    std                  | 0.0062      |\n",
      "|    value_loss           | 1.13e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0967  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2727     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.106       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1774080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041798893 |\n",
      "|    clip_fraction        | 0.162        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.69         |\n",
      "|    explained_variance   | 0.0257       |\n",
      "|    learning_rate        | 0.00206      |\n",
      "|    loss                 | 0.00462      |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | 0.00753      |\n",
      "|    std                  | 0.00612      |\n",
      "|    value_loss           | 5.58e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0944  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2757     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0841      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1794240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049392367 |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.73         |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.00203      |\n",
      "|    loss                 | 4.61e-05     |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | 0.00651      |\n",
      "|    std                  | 0.00596      |\n",
      "|    value_loss           | 3.58e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0917  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 2787     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0828     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1814400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017484445 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.75        |\n",
      "|    explained_variance   | -0.884      |\n",
      "|    learning_rate        | 0.00199     |\n",
      "|    loss                 | -0.00105    |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.00137     |\n",
      "|    std                  | 0.00586     |\n",
      "|    value_loss           | 6.9e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0927  |\n",
      "| time/              |          |\n",
      "|    fps             | 643      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2817     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0782     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017918581 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.76        |\n",
      "|    explained_variance   | -0.00334    |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | 0.00544     |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | 0.00367     |\n",
      "|    std                  | 0.00592     |\n",
      "|    value_loss           | 1.31e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0905  |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 2847     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0757     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018322663 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | 0.0003      |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | 0.008       |\n",
      "|    std                  | 0.00591     |\n",
      "|    value_loss           | 2.57e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0904  |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 2877     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0891     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010951934 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.8         |\n",
      "|    explained_variance   | -0.0197     |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | 0.000626    |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    std                  | 0.00585     |\n",
      "|    value_loss           | 7.61e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0894  |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2908     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.09 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0932     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014341401 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.83        |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00186     |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.00354     |\n",
      "|    std                  | 0.00575     |\n",
      "|    value_loss           | 2.94e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0919  |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 2937     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0815      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1915200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042887437 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.8          |\n",
      "|    explained_variance   | 0.0345       |\n",
      "|    learning_rate        | 0.00182      |\n",
      "|    loss                 | -0.00264     |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | 0.000311     |\n",
      "|    std                  | 0.00583      |\n",
      "|    value_loss           | 2.2e-07      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0914  |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 2967     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.105    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1935360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0220937 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 7.83      |\n",
      "|    explained_variance   | -0.0511   |\n",
      "|    learning_rate        | 0.00179   |\n",
      "|    loss                 | 0.00127   |\n",
      "|    n_updates            | 1910      |\n",
      "|    policy_gradient_loss | 0.00642   |\n",
      "|    std                  | 0.0057    |\n",
      "|    value_loss           | 1.59e-07  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0922  |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 2997     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0853      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1955520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042605074 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.84         |\n",
      "|    explained_variance   | -0.138       |\n",
      "|    learning_rate        | 0.00176      |\n",
      "|    loss                 | -0.000716    |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | 0.00634      |\n",
      "|    std                  | 0.00571      |\n",
      "|    value_loss           | 1.66e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.093   |\n",
      "| time/              |          |\n",
      "|    fps             | 645      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 3027     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.10 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0973     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013017456 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.86        |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.00519     |\n",
      "|    std                  | 0.00567     |\n",
      "|    value_loss           | 7.73e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0909  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 3057     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0839      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1995840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066641667 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.89         |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.00169      |\n",
      "|    loss                 | 0.0125       |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | 0.00136      |\n",
      "|    std                  | 0.00557      |\n",
      "|    value_loss           | 7.39e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 3087     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0764      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2016000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017243964 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.93         |\n",
      "|    explained_variance   | 0.252        |\n",
      "|    learning_rate        | 0.00166      |\n",
      "|    loss                 | 0.0014       |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | 0.00329      |\n",
      "|    std                  | 0.00545      |\n",
      "|    value_loss           | 5.02e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0875  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3117     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0892      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2036160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014746897 |\n",
      "|    clip_fraction        | 0.189        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.96         |\n",
      "|    explained_variance   | 0.252        |\n",
      "|    learning_rate        | 0.00162      |\n",
      "|    loss                 | -0.000405    |\n",
      "|    n_updates            | 2010         |\n",
      "|    policy_gradient_loss | 0.00791      |\n",
      "|    std                  | 0.00538      |\n",
      "|    value_loss           | 7.65e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0876  |\n",
      "| time/              |          |\n",
      "|    fps             | 646      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3147     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008053374 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.99        |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | 0.00811     |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.00895     |\n",
      "|    std                  | 0.00531     |\n",
      "|    value_loss           | 8.25e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0896  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3177     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.109       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2076480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060468665 |\n",
      "|    clip_fraction        | 0.0789       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8            |\n",
      "|    explained_variance   | 0.259        |\n",
      "|    learning_rate        | 0.00156      |\n",
      "|    loss                 | 0.00138      |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | 0.0018       |\n",
      "|    std                  | 0.00541      |\n",
      "|    value_loss           | 3.54e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0883  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3207     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.12 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.118       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2096640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028728864 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 7.96         |\n",
      "|    explained_variance   | 0.0829       |\n",
      "|    learning_rate        | 0.00152      |\n",
      "|    loss                 | -0.000251    |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | 0.00288      |\n",
      "|    std                  | 0.00541      |\n",
      "|    value_loss           | 7.95e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0868  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3237     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0718     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046288997 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.96        |\n",
      "|    explained_variance   | 0.0353      |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.0147      |\n",
      "|    std                  | 0.0054      |\n",
      "|    value_loss           | 9.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0869  |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3267     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011233828 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.97        |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | 0.00757     |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | 0.00966     |\n",
      "|    std                  | 0.0053      |\n",
      "|    value_loss           | 9.61e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0871  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3297     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0836     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015427571 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 7.99        |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.00142     |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | 0.019       |\n",
      "|    std                  | 0.00527     |\n",
      "|    value_loss           | 1.14e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0904  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3327     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0933      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2177280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046281726 |\n",
      "|    clip_fraction        | 0.0936       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8            |\n",
      "|    explained_variance   | 0.385        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.00422      |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | 0.00249      |\n",
      "|    std                  | 0.00523      |\n",
      "|    value_loss           | 3.61e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 3357     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0823      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2197440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034503082 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.04         |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 0.00135      |\n",
      "|    loss                 | -0.000924    |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | 0.00826      |\n",
      "|    std                  | 0.00511      |\n",
      "|    value_loss           | 8.03e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0914  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 3387     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0679     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008784971 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.06        |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.000873   |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.00262     |\n",
      "|    std                  | 0.00505     |\n",
      "|    value_loss           | 2.35e-07    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0933  |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 3417     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0824      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2237760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071106157 |\n",
      "|    clip_fraction        | 0.0802       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.08         |\n",
      "|    explained_variance   | 0.265        |\n",
      "|    learning_rate        | 0.00129      |\n",
      "|    loss                 | 0.00324      |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | 0.000762     |\n",
      "|    std                  | 0.00501      |\n",
      "|    value_loss           | 9.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0986  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 3447     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0948     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2257920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014041918 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.09        |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.00125     |\n",
      "|    loss                 | -0.000318   |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 0.00273     |\n",
      "|    std                  | 0.00499     |\n",
      "|    value_loss           | 7.93e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0985  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 3477     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0792     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012052328 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.1         |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -1.74e-05   |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.00212     |\n",
      "|    std                  | 0.00496     |\n",
      "|    value_loss           | 1.11e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0994  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 3507     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.104      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2298240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007480203 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.13        |\n",
      "|    explained_variance   | -0.0122     |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.00153     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    std                  | 0.00493     |\n",
      "|    value_loss           | 1.1e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0931  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 3537     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0814     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003306241 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.15        |\n",
      "|    explained_variance   | 0.0216      |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.00165    |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | 0.00268     |\n",
      "|    std                  | 0.00489     |\n",
      "|    value_loss           | 1.98e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0948  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3567     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0929     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031965222 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.15        |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | 0.00531     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.00674     |\n",
      "|    std                  | 0.00486     |\n",
      "|    value_loss           | 1.85e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0936  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 3597     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.13 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.128       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2358720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075013777 |\n",
      "|    clip_fraction        | 0.0548       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.17         |\n",
      "|    explained_variance   | 0.0335       |\n",
      "|    learning_rate        | 0.00109      |\n",
      "|    loss                 | -0.00296     |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -0.000119    |\n",
      "|    std                  | 0.00478      |\n",
      "|    value_loss           | 1.55e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0896  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 3627     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2378880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034033833 |\n",
      "|    clip_fraction        | 0.0944       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.21         |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.00105      |\n",
      "|    loss                 | -5.14e-05    |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | 0.000884     |\n",
      "|    std                  | 0.00474      |\n",
      "|    value_loss           | 6.62e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0903  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 3657     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.11 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.111       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2399040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053641936 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.24         |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.00102      |\n",
      "|    loss                 | 0.00462      |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | 0.00236      |\n",
      "|    std                  | 0.00468      |\n",
      "|    value_loss           | 5.84e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 3687     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0696     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007991882 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.27        |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | -0.000588   |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | 0.000795    |\n",
      "|    std                  | 0.00463     |\n",
      "|    value_loss           | 1.02e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0919  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3717     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.12 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.119       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2439360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017898947 |\n",
      "|    clip_fraction        | 0.22         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.27         |\n",
      "|    explained_variance   | 0.164        |\n",
      "|    learning_rate        | 0.000951     |\n",
      "|    loss                 | 0.000547     |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | 0.00958      |\n",
      "|    std                  | 0.0046       |\n",
      "|    value_loss           | 3.35e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0956  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 3746     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.109      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026888961 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.29        |\n",
      "|    explained_variance   | -1.46       |\n",
      "|    learning_rate        | 0.000918    |\n",
      "|    loss                 | 0.00496     |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    std                  | 0.00456     |\n",
      "|    value_loss           | 5.8e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0954  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 3777     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0898      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2479680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024748559 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.31         |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.000884     |\n",
      "|    loss                 | -0.00147     |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | 0.00421      |\n",
      "|    std                  | 0.00457      |\n",
      "|    value_loss           | 1.56e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0963  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3807     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0738     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026914181 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.32        |\n",
      "|    explained_variance   | -0.00715    |\n",
      "|    learning_rate        | 0.00085     |\n",
      "|    loss                 | -0.00156    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | 0.00799     |\n",
      "|    std                  | 0.00451     |\n",
      "|    value_loss           | 1.6e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0951  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 3838     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0803      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063321833 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.34         |\n",
      "|    explained_variance   | 0.09         |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | -0.00143     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | 0.000911     |\n",
      "|    std                  | 0.00448      |\n",
      "|    value_loss           | 7.93e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0909  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3869     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.101       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2540160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045633214 |\n",
      "|    clip_fraction        | 0.193        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.35         |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 0.000783     |\n",
      "|    loss                 | 0.000697     |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | 0.0103       |\n",
      "|    std                  | 0.00447      |\n",
      "|    value_loss           | 6.9e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0918  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 3901     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002173331 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.36        |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00075     |\n",
      "|    loss                 | 0.000899    |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.000886    |\n",
      "|    std                  | 0.00441     |\n",
      "|    value_loss           | 1.06e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0893  |\n",
      "| time/              |          |\n",
      "|    fps             | 651      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 3932     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0752      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2580480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005878748 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.36         |\n",
      "|    explained_variance   | -0.749       |\n",
      "|    learning_rate        | 0.000716     |\n",
      "|    loss                 | 0.000761     |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | 0.000611     |\n",
      "|    std                  | 0.00438      |\n",
      "|    value_loss           | 3.82e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0898  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 3964     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0924     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2600640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005808655 |\n",
      "|    clip_fraction        | 0.0684      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | -0.00442    |\n",
      "|    learning_rate        | 0.000682    |\n",
      "|    loss                 | -0.000624   |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    std                  | 0.00437     |\n",
      "|    value_loss           | 7.84e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0902  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 3996     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.10 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008273062 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.37        |\n",
      "|    explained_variance   | -0.0597     |\n",
      "|    learning_rate        | 0.000649    |\n",
      "|    loss                 | 0.00443     |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.000681    |\n",
      "|    std                  | 0.00436     |\n",
      "|    value_loss           | 1.25e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0915  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 4027     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.073      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014383091 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.000615    |\n",
      "|    loss                 | -0.0038     |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.000754    |\n",
      "|    std                  | 0.00432     |\n",
      "|    value_loss           | 5.83e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0915  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 4059     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0871      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2661120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036311583 |\n",
      "|    clip_fraction        | 0.073        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.41         |\n",
      "|    explained_variance   | 0.146        |\n",
      "|    learning_rate        | 0.000582     |\n",
      "|    loss                 | 0.00108      |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | 0.00138      |\n",
      "|    std                  | 0.00424      |\n",
      "|    value_loss           | 1.54e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0945  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 4090     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.11 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2681280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006294459 |\n",
      "|    clip_fraction        | 0.0154      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.43        |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.000548    |\n",
      "|    loss                 | -0.000527   |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | 1.19e-05    |\n",
      "|    std                  | 0.00421     |\n",
      "|    value_loss           | 1.23e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0927  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 4122     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0973      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2701440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011812674 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.45         |\n",
      "|    explained_variance   | 0.0391       |\n",
      "|    learning_rate        | 0.000514     |\n",
      "|    loss                 | 0.00241      |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | 0.000755     |\n",
      "|    std                  | 0.00421      |\n",
      "|    value_loss           | 1.04e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0914  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 4153     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0901      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2721600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076014725 |\n",
      "|    clip_fraction        | 0.0965       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.45         |\n",
      "|    explained_variance   | 0.0272       |\n",
      "|    learning_rate        | 0.000481     |\n",
      "|    loss                 | 0.00259      |\n",
      "|    n_updates            | 2690         |\n",
      "|    policy_gradient_loss | 0.00177      |\n",
      "|    std                  | 0.00422      |\n",
      "|    value_loss           | 3.58e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0925  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 4185     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.121       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2741760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014179926 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.45         |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.000447     |\n",
      "|    loss                 | 0.000601     |\n",
      "|    n_updates            | 2710         |\n",
      "|    policy_gradient_loss | 4.42e-06     |\n",
      "|    std                  | 0.00422      |\n",
      "|    value_loss           | 4.32e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0937  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 4216     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0982       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2761920       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036303687 |\n",
      "|    clip_fraction        | 0.0628        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 8.46          |\n",
      "|    explained_variance   | 0.115         |\n",
      "|    learning_rate        | 0.000414      |\n",
      "|    loss                 | 0.000523      |\n",
      "|    n_updates            | 2730          |\n",
      "|    policy_gradient_loss | -0.000419     |\n",
      "|    std                  | 0.00418       |\n",
      "|    value_loss           | 6.25e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0971  |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 4248     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.12 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.123      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008040197 |\n",
      "|    clip_fraction        | 0.0632      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00038     |\n",
      "|    loss                 | 0.000126    |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    std                  | 0.00413     |\n",
      "|    value_loss           | 2.07e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.094   |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 4279     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.103       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066010677 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.49         |\n",
      "|    explained_variance   | 0.212        |\n",
      "|    learning_rate        | 0.000346     |\n",
      "|    loss                 | 0.00344      |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.000213    |\n",
      "|    std                  | 0.00413      |\n",
      "|    value_loss           | 8.73e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0938  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 4311     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.07 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0701     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003954416 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.000313    |\n",
      "|    loss                 | 1.39e-05    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.000452   |\n",
      "|    std                  | 0.00414     |\n",
      "|    value_loss           | 2.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0952  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4343     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0867    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2842560    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00292302 |\n",
      "|    clip_fraction        | 0.0261     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | 0.129      |\n",
      "|    learning_rate        | 0.000279   |\n",
      "|    loss                 | -0.000274  |\n",
      "|    n_updates            | 2810       |\n",
      "|    policy_gradient_loss | -0.000689  |\n",
      "|    std                  | 0.00409    |\n",
      "|    value_loss           | 5.83e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0939  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 4374     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0976      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2862720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062413784 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.52         |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | -0.000265    |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    std                  | 0.00405      |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0921  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 4406     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.101       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010325789 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.52         |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -5.69e-05    |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | 0.000649     |\n",
      "|    std                  | 0.00407      |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0918  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 4437     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.09 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0879     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2903040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001047187 |\n",
      "|    clip_fraction        | 0.011       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 8.53        |\n",
      "|    explained_variance   | -0.0425     |\n",
      "|    learning_rate        | 0.000178    |\n",
      "|    loss                 | -0.000115   |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | -0.000455   |\n",
      "|    std                  | 0.00406     |\n",
      "|    value_loss           | 6.27e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 4468     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.09 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0932      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2923200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053445213 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.53         |\n",
      "|    explained_variance   | 0.436        |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.00161     |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.000983    |\n",
      "|    std                  | 0.00406      |\n",
      "|    value_loss           | 2.93e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0885  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 4500     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0949      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036447414 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.53         |\n",
      "|    explained_variance   | 0.145        |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | -0.00234     |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.000517    |\n",
      "|    std                  | 0.00405      |\n",
      "|    value_loss           | 1.57e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0896  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 4531     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0846      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023656888 |\n",
      "|    clip_fraction        | 0.00814      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.53         |\n",
      "|    explained_variance   | 0.139        |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | -0.00118     |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    std                  | 0.00405      |\n",
      "|    value_loss           | 6.67e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0895  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 4563     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0857      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2983680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029166439 |\n",
      "|    clip_fraction        | 0.00842      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 8.54         |\n",
      "|    explained_variance   | 0.167        |\n",
      "|    learning_rate        | 4.4e-05      |\n",
      "|    loss                 | 0.000106     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000872    |\n",
      "|    std                  | 0.00405      |\n",
      "|    value_loss           | 7.94e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0899  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 4593     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.0719       |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 3003840       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00093743193 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | 8.54          |\n",
      "|    explained_variance   | 0.135         |\n",
      "|    learning_rate        | 1.04e-05      |\n",
      "|    loss                 | -0.000887     |\n",
      "|    n_updates            | 2970          |\n",
      "|    policy_gradient_loss | -0.00032      |\n",
      "|    std                  | 0.00405       |\n",
      "|    value_loss           | 1.3e-07       |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.0892  |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 4623     |\n",
      "|    total_timesteps | 3003840  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x15de2bb2cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Big observation\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'small'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'small')\n",
    "]),filename='logsBigO-train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'small'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigObs',\n",
    "    log_path='./logs/eval_logsBigO/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# Auto Encoder Observation\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small-More-Trust', obs = 'auto'),\n",
    "    lambda: tradingEng(paths2,action = 'small-More-Trust', obs = 'auto')\n",
    "]),filename='logsBigO-train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'auto'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelAutoObs',\n",
    "    log_path='./logs/eval_logsAutoO/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.13 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007813591 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.8       |\n",
      "|    explained_variance   | -0.0434     |\n",
      "|    learning_rate        | 0.00498     |\n",
      "|    loss                 | -0.00487    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000691    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.89    |\n",
      "| time/              |          |\n",
      "|    fps             | 745      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0956     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008708409 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.5       |\n",
      "|    explained_variance   | 0.00468     |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.0053     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.000175    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0984     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008767962 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.2       |\n",
      "|    explained_variance   | 0.00246     |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.00677    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.000181    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 698      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0722     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009694656 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.9       |\n",
      "|    explained_variance   | 0.0471      |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.00819    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 5.9e-05     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 695      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008811181 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.7       |\n",
      "|    explained_variance   | 0.00938     |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.00525    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 0.000286    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.122      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010137793 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.8       |\n",
      "|    explained_variance   | 0.0014      |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.00502    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00712    |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 0.00018     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 692      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.24 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.244      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010475271 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.6       |\n",
      "|    explained_variance   | 0.0157      |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.00661    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 0.000387    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 691      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.15 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.145      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008215299 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.5       |\n",
      "|    explained_variance   | 0.0422      |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 6.4e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.06    |\n",
      "| time/              |          |\n",
      "|    fps             | 690      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.22 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.217      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008070685 |\n",
      "|    clip_fraction        | 0.092       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.4       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.00587    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 5.13e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.28 +/- 0.10\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.276      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008980809 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.2       |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.00512    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    std                  | 0.961       |\n",
      "|    value_loss           | 8.89e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.18 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.18       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010514153 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55         |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.0106     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00701    |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 0.00012     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -3.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 321      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.16 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.158      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010131642 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.7       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.00461     |\n",
      "|    loss                 | -0.00402    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 6.61e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 351      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.22 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.218      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011522803 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.5       |\n",
      "|    explained_variance   | 0.0513      |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | -0.00496    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    std                  | 0.947       |\n",
      "|    value_loss           | 0.000374    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 688      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 380      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=-0.22 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.22       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010133615 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.4       |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | -0.00714    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    std                  | 0.945       |\n",
      "|    value_loss           | 4.24e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 410      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.20 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.201      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008088073 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.4       |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00451     |\n",
      "|    loss                 | -0.00884    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00535    |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 8.63e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.89    |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 439      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=-0.22 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.221      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008751113 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.3       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.00448     |\n",
      "|    loss                 | -0.00735    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 4.21e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 469      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=-0.19 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.189      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009620424 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -54.1       |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00634    |\n",
      "|    std                  | 0.938       |\n",
      "|    value_loss           | 0.000137    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 498      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.17 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.168      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012469124 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.9       |\n",
      "|    explained_variance   | 0.0215      |\n",
      "|    learning_rate        | 0.00441     |\n",
      "|    loss                 | -0.00972    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 0.000754    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 528      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=-0.21 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.206      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009401004 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.7       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.00438     |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00701    |\n",
      "|    std                  | 0.93        |\n",
      "|    value_loss           | 4.88e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.87    |\n",
      "| time/              |          |\n",
      "|    fps             | 687      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 557      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=-0.21 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.209     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 403200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00948765 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -53.4      |\n",
      "|    explained_variance   | 0.178      |\n",
      "|    learning_rate        | 0.00434    |\n",
      "|    loss                 | -0.00857   |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.00653   |\n",
      "|    std                  | 0.923      |\n",
      "|    value_loss           | 0.0001     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 587      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.21 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.206       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108101005 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -53          |\n",
      "|    explained_variance   | 0.222        |\n",
      "|    learning_rate        | 0.00431      |\n",
      "|    loss                 | -0.00644     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    std                  | 0.913        |\n",
      "|    value_loss           | 0.000127     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 616      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.244       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100573385 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -52.8        |\n",
      "|    explained_variance   | 0.194        |\n",
      "|    learning_rate        | 0.00428      |\n",
      "|    loss                 | -0.00878     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00721     |\n",
      "|    std                  | 0.91         |\n",
      "|    value_loss           | 0.000123     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 646      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=-0.20 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.203      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009228308 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.5       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00424     |\n",
      "|    loss                 | -0.00567    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.904       |\n",
      "|    value_loss           | 9.14e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 675      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.23 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.226      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008586034 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.3       |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.00421     |\n",
      "|    loss                 | -0.0061     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00587    |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 0.000105    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 705      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.241      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011598617 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.1       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00418     |\n",
      "|    loss                 | -0.00603    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00746    |\n",
      "|    std                  | 0.896       |\n",
      "|    value_loss           | 0.000315    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 734      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=-0.31 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.312      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010029388 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.7       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00414     |\n",
      "|    loss                 | -0.00558    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    std                  | 0.887       |\n",
      "|    value_loss           | 7.3e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 764      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.38 +/- 0.11\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.379      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 544320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010389431 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.4       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.00411     |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00729    |\n",
      "|    std                  | 0.882       |\n",
      "|    value_loss           | 5.92e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=-0.25 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.251      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012316427 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.3       |\n",
      "|    explained_variance   | 0.0224      |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | -0.00726    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00537    |\n",
      "|    std                  | 0.879       |\n",
      "|    value_loss           | 0.000404    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 823      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=-0.17 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.172      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009726868 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -51.1       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | -0.0056     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00532    |\n",
      "|    std                  | 0.876       |\n",
      "|    value_loss           | 0.000125    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 852      |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.24 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.241      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011365154 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.9       |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 0.00401     |\n",
      "|    loss                 | -0.0056     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 0.000417    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 882      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=-0.20 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.199     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 624960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01388682 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -50.6      |\n",
      "|    explained_variance   | 0.0523     |\n",
      "|    learning_rate        | 0.00398    |\n",
      "|    loss                 | -0.00744   |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.00593   |\n",
      "|    std                  | 0.867      |\n",
      "|    value_loss           | 0.000366   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 911      |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=-0.17 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.165     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 645120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00925131 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -50.7      |\n",
      "|    explained_variance   | 0.0891     |\n",
      "|    learning_rate        | 0.00394    |\n",
      "|    loss                 | -0.00804   |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0056    |\n",
      "|    std                  | 0.871      |\n",
      "|    value_loss           | 0.000135   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.18 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.183      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010568845 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50.4       |\n",
      "|    explained_variance   | 0.0932      |\n",
      "|    learning_rate        | 0.00391     |\n",
      "|    loss                 | -0.00717    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 0.000228    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 685      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 970      |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685440, episode_reward=-0.19 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.188      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 685440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010422641 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50         |\n",
      "|    explained_variance   | 0.0441      |\n",
      "|    learning_rate        | 0.00387     |\n",
      "|    loss                 | -0.00654    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0055     |\n",
      "|    std                  | 0.857       |\n",
      "|    value_loss           | 0.000192    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 1002     |\n",
      "|    total_timesteps | 685440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705600, episode_reward=-0.16 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.161      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 705600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009099216 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.8       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00384     |\n",
      "|    loss                 | -0.00579    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00599    |\n",
      "|    std                  | 0.853       |\n",
      "|    value_loss           | 9.66e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 679      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 1038     |\n",
      "|    total_timesteps | 705600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.20 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.199      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011085879 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.5       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00381     |\n",
      "|    loss                 | -0.00874    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00715    |\n",
      "|    std                  | 0.848       |\n",
      "|    value_loss           | 9.9e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 678      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 1069     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745920, episode_reward=-0.21 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.212      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 745920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012620227 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.3       |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.00377     |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    std                  | 0.843       |\n",
      "|    value_loss           | 0.000161    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 677      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1100     |\n",
      "|    total_timesteps | 745920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766080, episode_reward=-0.15 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.154      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011306075 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49.2       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00374     |\n",
      "|    loss                 | -0.00997    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00573    |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 0.00017     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.62    |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 1131     |\n",
      "|    total_timesteps | 766080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786240, episode_reward=-0.22 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.22       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 786240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010444097 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -49         |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00371     |\n",
      "|    loss                 | -0.00718    |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00556    |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 0.000428    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 1162     |\n",
      "|    total_timesteps | 786240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806400, episode_reward=-0.22 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.221      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 806400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010446729 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.8       |\n",
      "|    explained_variance   | 0.0515      |\n",
      "|    learning_rate        | 0.00367     |\n",
      "|    loss                 | -0.00726    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.834       |\n",
      "|    value_loss           | 0.000301    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 675      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 1194     |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826560, episode_reward=-0.21 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.212      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011088555 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.5       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.00364     |\n",
      "|    loss                 | -0.00689    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    std                  | 0.831       |\n",
      "|    value_loss           | 0.000105    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 1231     |\n",
      "|    total_timesteps | 826560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846720, episode_reward=-0.22 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.221      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009392668 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.2       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.00361     |\n",
      "|    loss                 | -0.00875    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 3.22e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 667      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1267     |\n",
      "|    total_timesteps | 846720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866880, episode_reward=-0.21 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.214      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 866880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010981465 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.9       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.0064     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00584    |\n",
      "|    std                  | 0.821       |\n",
      "|    value_loss           | 0.000153    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 664      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 1304     |\n",
      "|    total_timesteps | 866880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887040, episode_reward=-0.17 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.168      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008395612 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.6       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | -0.00922    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00537    |\n",
      "|    std                  | 0.815       |\n",
      "|    value_loss           | 4.95e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 662      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1339     |\n",
      "|    total_timesteps | 887040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907200, episode_reward=-0.23 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.23       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 907200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009028485 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0035      |\n",
      "|    loss                 | -0.00485    |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00767    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 2.51e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 660      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1374     |\n",
      "|    total_timesteps | 907200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927360, episode_reward=-0.23 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.233      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010546002 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.1       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00347     |\n",
      "|    loss                 | -0.00761    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00702    |\n",
      "|    std                  | 0.806       |\n",
      "|    value_loss           | 8.04e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.45    |\n",
      "| time/              |          |\n",
      "|    fps             | 658      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1408     |\n",
      "|    total_timesteps | 927360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947520, episode_reward=-0.24 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.237      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008269471 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.8       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.00344     |\n",
      "|    loss                 | -0.00733    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    std                  | 0.8         |\n",
      "|    value_loss           | 2.36e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 655      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1445     |\n",
      "|    total_timesteps | 947520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967680, episode_reward=-0.25 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.254      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 967680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011049008 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.0034      |\n",
      "|    loss                 | -0.00626    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 0.000183    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1486     |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987840, episode_reward=-0.22 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.218      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009258028 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.3       |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.00337     |\n",
      "|    loss                 | -0.00813    |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    std                  | 0.792       |\n",
      "|    value_loss           | 2.25e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 644      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1532     |\n",
      "|    total_timesteps | 987840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=-0.23 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.229      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1008000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010084001 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00334     |\n",
      "|    loss                 | -0.00898    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.00699    |\n",
      "|    std                  | 0.786       |\n",
      "|    value_loss           | 6.04e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.35    |\n",
      "| time/              |          |\n",
      "|    fps             | 637      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1580     |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028160, episode_reward=-0.22 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.22       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1028160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010974983 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0033      |\n",
      "|    loss                 | -0.00736    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00654    |\n",
      "|    std                  | 0.783       |\n",
      "|    value_loss           | 0.000137    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 632      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1625     |\n",
      "|    total_timesteps | 1028160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048320, episode_reward=-0.27 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.275      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1048320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009903818 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00327     |\n",
      "|    loss                 | -0.00801    |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 7.41e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 627      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1670     |\n",
      "|    total_timesteps | 1048320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068480, episode_reward=-0.22 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.219      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1068480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008706525 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.00324     |\n",
      "|    loss                 | -0.00257    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    std                  | 0.772       |\n",
      "|    value_loss           | 2.55e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 622      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1715     |\n",
      "|    total_timesteps | 1068480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088640, episode_reward=-0.22 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.219      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1088640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009537574 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0032      |\n",
      "|    loss                 | -0.00896    |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.00766    |\n",
      "|    std                  | 0.769       |\n",
      "|    value_loss           | 3.01e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 618      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1761     |\n",
      "|    total_timesteps | 1088640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108800, episode_reward=-0.27 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.274      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1108800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013427266 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00317     |\n",
      "|    loss                 | -0.00386    |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    std                  | 0.762       |\n",
      "|    value_loss           | 0.000357    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 614      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1804     |\n",
      "|    total_timesteps | 1108800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128960, episode_reward=-0.16 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.158       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1128960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110338535 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.3        |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.00314      |\n",
      "|    loss                 | -0.00659     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.00561     |\n",
      "|    std                  | 0.761        |\n",
      "|    value_loss           | 0.000328     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1849     |\n",
      "|    total_timesteps | 1128960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1149120, episode_reward=-0.16 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.164      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1149120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010644411 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.0031      |\n",
      "|    loss                 | -0.00823    |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    std                  | 0.756       |\n",
      "|    value_loss           | 0.000104    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 607      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1890     |\n",
      "|    total_timesteps | 1149120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1169280, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.244      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1169280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010768422 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.00307     |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.00745    |\n",
      "|    std                  | 0.75        |\n",
      "|    value_loss           | 6.32e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1935     |\n",
      "|    total_timesteps | 1169280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1189440, episode_reward=-0.16 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.156     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1189440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00856602 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.3      |\n",
      "|    explained_variance   | 0.492      |\n",
      "|    learning_rate        | 0.00303    |\n",
      "|    loss                 | -0.0089    |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | -0.00648   |\n",
      "|    std                  | 0.745      |\n",
      "|    value_loss           | 2.2e-05    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1979     |\n",
      "|    total_timesteps | 1189440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1209600, episode_reward=-0.18 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.183      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1209600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008745253 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.003       |\n",
      "|    loss                 | -0.00545    |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 1.83e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 596      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 2028     |\n",
      "|    total_timesteps | 1209600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1229760, episode_reward=-0.24 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.244      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1229760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009485918 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.00297     |\n",
      "|    loss                 | -0.00881    |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    std                  | 0.732       |\n",
      "|    value_loss           | 6.06e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.21    |\n",
      "| time/              |          |\n",
      "|    fps             | 594      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 2068     |\n",
      "|    total_timesteps | 1229760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1249920, episode_reward=-0.22 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.217      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1249920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009179706 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.00293     |\n",
      "|    loss                 | -0.00748    |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.00717    |\n",
      "|    std                  | 0.727       |\n",
      "|    value_loss           | 2.9e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 593      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 2106     |\n",
      "|    total_timesteps | 1249920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270080, episode_reward=-0.22 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.221      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1270080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013898061 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0168      |\n",
      "|    learning_rate        | 0.0029      |\n",
      "|    loss                 | -0.00865    |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 0.000979    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 591      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 2146     |\n",
      "|    total_timesteps | 1270080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290240, episode_reward=-0.31 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.308       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1290240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094844885 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.7        |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.00287      |\n",
      "|    loss                 | -0.00802     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.00615     |\n",
      "|    std                  | 0.722        |\n",
      "|    value_loss           | 2.89e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 588      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 2191     |\n",
      "|    total_timesteps | 1290240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310400, episode_reward=-0.24 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.24        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1310400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093439445 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.00283      |\n",
      "|    loss                 | -0.00628     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.00663     |\n",
      "|    std                  | 0.714        |\n",
      "|    value_loss           | 1.97e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 586      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 2234     |\n",
      "|    total_timesteps | 1310400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330560, episode_reward=-0.29 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.286      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008984451 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41         |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.0028      |\n",
      "|    loss                 | -0.00658    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00723    |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 2275     |\n",
      "|    total_timesteps | 1330560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350720, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.235      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009364095 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.5       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00277     |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0075     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 1.36e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 584      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 2312     |\n",
      "|    total_timesteps | 1350720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370880, episode_reward=-0.21 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.209      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009240634 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.1       |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.00273     |\n",
      "|    loss                 | -0.00502    |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 6.25e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 2352     |\n",
      "|    total_timesteps | 1370880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1391040, episode_reward=-0.21 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.209      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1391040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010396158 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.8       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0027      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    std                  | 0.692       |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2       |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 2394     |\n",
      "|    total_timesteps | 1391040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1411200, episode_reward=-0.19 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.187      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1411200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008111798 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.6       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00266     |\n",
      "|    loss                 | -0.00789    |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.689       |\n",
      "|    value_loss           | 1.76e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.95    |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 2431     |\n",
      "|    total_timesteps | 1411200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1431360, episode_reward=-0.22 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.218      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1431360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008114223 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.4       |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.00263     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    std                  | 0.686       |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2477     |\n",
      "|    total_timesteps | 1431360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1451520, episode_reward=-0.21 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.211      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1451520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010012872 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39         |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0026      |\n",
      "|    loss                 | -0.00286    |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    std                  | 0.68        |\n",
      "|    value_loss           | 6.88e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 2521     |\n",
      "|    total_timesteps | 1451520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1471680, episode_reward=-0.22 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.216      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1471680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009122299 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.5       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00256     |\n",
      "|    loss                 | -0.00516    |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 1.64e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 2563     |\n",
      "|    total_timesteps | 1471680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1491840, episode_reward=-0.25 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.249      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009602208 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00253     |\n",
      "|    loss                 | -0.00617    |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 5.69e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 2602     |\n",
      "|    total_timesteps | 1491840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=-0.19 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.189      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1512000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014052974 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.9       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0025      |\n",
      "|    loss                 | -0.00578    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 0.00018     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 2642     |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532160, episode_reward=-0.24 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.239       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1532160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134575805 |\n",
      "|    clip_fraction        | 0.155        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -37.6        |\n",
      "|    explained_variance   | 0.127        |\n",
      "|    learning_rate        | 0.00246      |\n",
      "|    loss                 | -0.00772     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.0061      |\n",
      "|    std                  | 0.662        |\n",
      "|    value_loss           | 0.000291     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -2       |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2679     |\n",
      "|    total_timesteps | 1532160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552320, episode_reward=-0.22 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.216      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1552320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011256894 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.5       |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.00243     |\n",
      "|    loss                 | -0.00811    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.66        |\n",
      "|    value_loss           | 0.00014     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 2719     |\n",
      "|    total_timesteps | 1552320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572480, episode_reward=-0.26 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.256      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1572480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011097466 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -37.1       |\n",
      "|    explained_variance   | 0.0205      |\n",
      "|    learning_rate        | 0.0024      |\n",
      "|    loss                 | -0.00717    |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    std                  | 0.654       |\n",
      "|    value_loss           | 0.000517    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 2762     |\n",
      "|    total_timesteps | 1572480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592640, episode_reward=-0.20 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.203      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1592640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011521375 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.6       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.00236     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    std                  | 0.647       |\n",
      "|    value_loss           | 6.6e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.96    |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 2806     |\n",
      "|    total_timesteps | 1592640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612800, episode_reward=-0.22 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.218      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1612800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013705699 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -36.2       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00233     |\n",
      "|    loss                 | -0.00791    |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.00686    |\n",
      "|    std                  | 0.642       |\n",
      "|    value_loss           | 0.000126    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 2846     |\n",
      "|    total_timesteps | 1612800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632960, episode_reward=-0.19 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.191      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1632960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010036787 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35.8       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0023      |\n",
      "|    loss                 | -0.00635    |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    std                  | 0.636       |\n",
      "|    value_loss           | 2.48e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.94    |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2891     |\n",
      "|    total_timesteps | 1632960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1653120, episode_reward=-0.21 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.21        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1653120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098549845 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -35.3        |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.00226      |\n",
      "|    loss                 | -0.00839     |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.00572     |\n",
      "|    std                  | 0.628        |\n",
      "|    value_loss           | 4.01e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.86    |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 2938     |\n",
      "|    total_timesteps | 1653120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1673280, episode_reward=-0.20 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.199      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1673280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008308852 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -35         |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.00223     |\n",
      "|    loss                 | -0.00697    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.00632    |\n",
      "|    std                  | 0.624       |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.85    |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 2974     |\n",
      "|    total_timesteps | 1673280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1693440, episode_reward=-0.17 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.166     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1693440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00797152 |\n",
      "|    clip_fraction        | 0.0873     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -34.8      |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.00219    |\n",
      "|    loss                 | -0.00903   |\n",
      "|    n_updates            | 1670       |\n",
      "|    policy_gradient_loss | -0.00574   |\n",
      "|    std                  | 0.622      |\n",
      "|    value_loss           | 1.08e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.83    |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 3012     |\n",
      "|    total_timesteps | 1693440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1713600, episode_reward=-0.18 +/- 0.10\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.182      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1713600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009848231 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.6       |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.00216     |\n",
      "|    loss                 | -0.0081     |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    std                  | 0.62        |\n",
      "|    value_loss           | 3.15e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 3055     |\n",
      "|    total_timesteps | 1713600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1733760, episode_reward=-0.21 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.213       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1733760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131000355 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -34.3        |\n",
      "|    explained_variance   | 0.147        |\n",
      "|    learning_rate        | 0.00213      |\n",
      "|    loss                 | -0.00775     |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    std                  | 0.615        |\n",
      "|    value_loss           | 0.000151     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 3104     |\n",
      "|    total_timesteps | 1733760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1753920, episode_reward=-0.18 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.183      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1753920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009372619 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -34.1       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.00209     |\n",
      "|    loss                 | -0.00628    |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    std                  | 0.612       |\n",
      "|    value_loss           | 4.26e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 556      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 3153     |\n",
      "|    total_timesteps | 1753920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774080, episode_reward=-0.19 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.187      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1774080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011488773 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.7       |\n",
      "|    explained_variance   | 0.0104      |\n",
      "|    learning_rate        | 0.00206     |\n",
      "|    loss                 | -0.00911    |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 0.000708    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 3199     |\n",
      "|    total_timesteps | 1774080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794240, episode_reward=-0.19 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.189      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1794240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007438032 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33.5       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00203     |\n",
      "|    loss                 | -0.00677    |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 3243     |\n",
      "|    total_timesteps | 1794240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814400, episode_reward=-0.15 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.15        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1814400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114783365 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -33.2        |\n",
      "|    explained_variance   | 0.0757       |\n",
      "|    learning_rate        | 0.00199      |\n",
      "|    loss                 | -0.00825     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    std                  | 0.603        |\n",
      "|    value_loss           | 0.000181     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 3278     |\n",
      "|    total_timesteps | 1814400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834560, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.161      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1834560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009053522 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -33         |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00196     |\n",
      "|    loss                 | -0.00317    |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.00544    |\n",
      "|    std                  | 0.6         |\n",
      "|    value_loss           | 9.86e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 552      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 3318     |\n",
      "|    total_timesteps | 1834560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854720, episode_reward=-0.19 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.192      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1854720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008300254 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.7       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00193     |\n",
      "|    loss                 | -0.00684    |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    std                  | 0.596       |\n",
      "|    value_loss           | 1.9e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 550      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 3369     |\n",
      "|    total_timesteps | 1854720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874880, episode_reward=-0.19 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.191      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1874880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012713129 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.4       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00189     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    std                  | 0.591       |\n",
      "|    value_loss           | 0.000103    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 3415     |\n",
      "|    total_timesteps | 1874880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1895040, episode_reward=-0.18 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.175      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1895040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012813764 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -32.1       |\n",
      "|    explained_variance   | 0.0469      |\n",
      "|    learning_rate        | 0.00186     |\n",
      "|    loss                 | -0.00791    |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    std                  | 0.586       |\n",
      "|    value_loss           | 0.00013     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 546      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 3468     |\n",
      "|    total_timesteps | 1895040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1915200, episode_reward=-0.16 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.161      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1915200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008284515 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.8       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.00182     |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    std                  | 0.584       |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 545      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 3510     |\n",
      "|    total_timesteps | 1915200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1935360, episode_reward=-0.18 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.177      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1935360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008138062 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.5       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00179     |\n",
      "|    loss                 | -0.00926    |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.00545    |\n",
      "|    std                  | 0.58        |\n",
      "|    value_loss           | 0.00012     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 544      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 3555     |\n",
      "|    total_timesteps | 1935360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1955520, episode_reward=-0.16 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.157      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1955520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011099789 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -31.1       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.00176     |\n",
      "|    loss                 | -0.00969    |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.00708    |\n",
      "|    std                  | 0.574       |\n",
      "|    value_loss           | 5.59e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 544      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 3589     |\n",
      "|    total_timesteps | 1955520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1975680, episode_reward=-0.18 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.177      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1975680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013626274 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.8       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.00172     |\n",
      "|    loss                 | -0.00708    |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.00673    |\n",
      "|    std                  | 0.57        |\n",
      "|    value_loss           | 8.94e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 545      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 3620     |\n",
      "|    total_timesteps | 1975680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1995840, episode_reward=-0.18 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.181      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1995840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010051091 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.4       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.00169     |\n",
      "|    loss                 | -0.00407    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    std                  | 0.566       |\n",
      "|    value_loss           | 2.18e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 546      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 3651     |\n",
      "|    total_timesteps | 1995840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2016000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008527798 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.2       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.00166     |\n",
      "|    loss                 | -0.00724    |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    std                  | 0.564       |\n",
      "|    value_loss           | 1.86e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 547      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 3683     |\n",
      "|    total_timesteps | 2016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2036160, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.125      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2036160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011356278 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -30.1       |\n",
      "|    explained_variance   | 0.038       |\n",
      "|    learning_rate        | 0.00162     |\n",
      "|    loss                 | -0.00712    |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    std                  | 0.564       |\n",
      "|    value_loss           | 0.000166    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 3714     |\n",
      "|    total_timesteps | 2036160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2056320, episode_reward=-0.15 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.148      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2056320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010853169 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.8       |\n",
      "|    explained_variance   | 0.0758      |\n",
      "|    learning_rate        | 0.00159     |\n",
      "|    loss                 | -0.00743    |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    std                  | 0.558       |\n",
      "|    value_loss           | 4.46e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 549      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 3745     |\n",
      "|    total_timesteps | 2056320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2076480, episode_reward=-0.10 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2076480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012969578 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.4       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00156     |\n",
      "|    loss                 | -0.00561    |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | -0.00627    |\n",
      "|    std                  | 0.554       |\n",
      "|    value_loss           | 4.52e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 549      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 3776     |\n",
      "|    total_timesteps | 2076480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2096640, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.14       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2096640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009855007 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -29.1       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00152     |\n",
      "|    loss                 | -0.0065     |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    std                  | 0.55        |\n",
      "|    value_loss           | 3.73e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 550      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 3807     |\n",
      "|    total_timesteps | 2096640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2116800, episode_reward=-0.15 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.149      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2116800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013070353 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.8       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00149     |\n",
      "|    loss                 | -0.00764    |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    std                  | 0.548       |\n",
      "|    value_loss           | 0.000126    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 3839     |\n",
      "|    total_timesteps | 2116800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2136960, episode_reward=-0.19 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.19       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2136960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011160614 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.6       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.00146     |\n",
      "|    loss                 | -0.0081     |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 6.91e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 552      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 3870     |\n",
      "|    total_timesteps | 2136960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2157120, episode_reward=-0.16 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.16        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075879185 |\n",
      "|    clip_fraction        | 0.0806       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -28.2        |\n",
      "|    explained_variance   | 0.469        |\n",
      "|    learning_rate        | 0.00142      |\n",
      "|    loss                 | -0.00982     |\n",
      "|    n_updates            | 2130         |\n",
      "|    policy_gradient_loss | -0.00602     |\n",
      "|    std                  | 0.539        |\n",
      "|    value_loss           | 8.68e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 552      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 3901     |\n",
      "|    total_timesteps | 2157120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2177280, episode_reward=-0.14 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2177280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010861552 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.9       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    std                  | 0.536       |\n",
      "|    value_loss           | 2.76e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.57    |\n",
      "| time/              |          |\n",
      "|    fps             | 553      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 3932     |\n",
      "|    total_timesteps | 2177280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2197440, episode_reward=-0.16 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.158      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2197440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011105257 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.6       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00135     |\n",
      "|    loss                 | -0.0085     |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    std                  | 0.533       |\n",
      "|    value_loss           | 0.000101    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 3963     |\n",
      "|    total_timesteps | 2197440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2217600, episode_reward=-0.15 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.155      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2217600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011988658 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27.3       |\n",
      "|    explained_variance   | 0.0601      |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00505    |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    std                  | 0.53        |\n",
      "|    value_loss           | 0.000221    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 3995     |\n",
      "|    total_timesteps | 2217600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2237760, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.137      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2237760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009305491 |\n",
      "|    clip_fraction        | 0.0999      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -27         |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00129     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    std                  | 0.526       |\n",
      "|    value_loss           | 1.72e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 4026     |\n",
      "|    total_timesteps | 2237760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2257920, episode_reward=-0.15 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.147     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2257920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00966339 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -26.8      |\n",
      "|    explained_variance   | 0.542      |\n",
      "|    learning_rate        | 0.00125    |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 2230       |\n",
      "|    policy_gradient_loss | -0.00669   |\n",
      "|    std                  | 0.524      |\n",
      "|    value_loss           | 9.97e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 556      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 4057     |\n",
      "|    total_timesteps | 2257920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2278080, episode_reward=-0.13 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.132      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011599414 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.4       |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.00122     |\n",
      "|    loss                 | -0.00865    |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    std                  | 0.52        |\n",
      "|    value_loss           | 6.73e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 4088     |\n",
      "|    total_timesteps | 2278080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2298240, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.138       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096395705 |\n",
      "|    clip_fraction        | 0.0832       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -26.1        |\n",
      "|    explained_variance   | 0.0836       |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00742     |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    std                  | 0.515        |\n",
      "|    value_loss           | 7.43e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 557      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 4119     |\n",
      "|    total_timesteps | 2298240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2318400, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.143      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2318400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007608662 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.8       |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.00115     |\n",
      "|    loss                 | -0.00946    |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 0.512       |\n",
      "|    value_loss           | 5.54e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 4151     |\n",
      "|    total_timesteps | 2318400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2338560, episode_reward=-0.15 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.147      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008896416 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.6       |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.00112     |\n",
      "|    loss                 | -0.0076     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 1.62e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 4181     |\n",
      "|    total_timesteps | 2338560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2358720, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.159      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2358720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011037318 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.3       |\n",
      "|    explained_variance   | 0.0724      |\n",
      "|    learning_rate        | 0.00109     |\n",
      "|    loss                 | -0.00551    |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    std                  | 0.507       |\n",
      "|    value_loss           | 9.13e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 4212     |\n",
      "|    total_timesteps | 2358720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2378880, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009787422 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25.1       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00105     |\n",
      "|    loss                 | -0.0052     |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    std                  | 0.503       |\n",
      "|    value_loss           | 4.77e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 4241     |\n",
      "|    total_timesteps | 2378880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2399040, episode_reward=-0.16 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.164      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2399040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009392975 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -25         |\n",
      "|    explained_variance   | 0.0956      |\n",
      "|    learning_rate        | 0.00102     |\n",
      "|    loss                 | -0.00631    |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00451    |\n",
      "|    std                  | 0.502       |\n",
      "|    value_loss           | 0.000119    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 561      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 4271     |\n",
      "|    total_timesteps | 2399040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2419200, episode_reward=-0.18 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.184      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2419200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009656043 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.8       |\n",
      "|    explained_variance   | 0.0716      |\n",
      "|    learning_rate        | 0.000985    |\n",
      "|    loss                 | -0.00804    |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    std                  | 0.501       |\n",
      "|    value_loss           | 0.000173    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 562      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 4300     |\n",
      "|    total_timesteps | 2419200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2439360, episode_reward=-0.14 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.136      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2439360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009362834 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.5       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.000951    |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00784    |\n",
      "|    std                  | 0.496       |\n",
      "|    value_loss           | 5.66e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 563      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 4330     |\n",
      "|    total_timesteps | 2439360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2459520, episode_reward=-0.19 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.185      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2459520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011672186 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.2       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.000918    |\n",
      "|    loss                 | -0.00581    |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.00549    |\n",
      "|    std                  | 0.494       |\n",
      "|    value_loss           | 4.95e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 4359     |\n",
      "|    total_timesteps | 2459520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2479680, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.131      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2479680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009287515 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.8       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.000884    |\n",
      "|    loss                 | -0.00513    |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    std                  | 0.49        |\n",
      "|    value_loss           | 9.73e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 564      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 4389     |\n",
      "|    total_timesteps | 2479680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2499840, episode_reward=-0.14 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.139      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2499840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008735994 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.5       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.00085     |\n",
      "|    loss                 | -0.00917    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.00676    |\n",
      "|    std                  | 0.486       |\n",
      "|    value_loss           | 9.76e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 565      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 4419     |\n",
      "|    total_timesteps | 2499840  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.121       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099112755 |\n",
      "|    clip_fraction        | 0.08         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -23.1        |\n",
      "|    explained_variance   | 0.329        |\n",
      "|    learning_rate        | 0.000817     |\n",
      "|    loss                 | -0.00914     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00593     |\n",
      "|    std                  | 0.482        |\n",
      "|    value_loss           | 1.04e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.44    |\n",
      "| time/              |          |\n",
      "|    fps             | 566      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 4449     |\n",
      "|    total_timesteps | 2520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540160, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.127     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2540160    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00867398 |\n",
      "|    clip_fraction        | 0.0837     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -22.9      |\n",
      "|    explained_variance   | 0.412      |\n",
      "|    learning_rate        | 0.000783   |\n",
      "|    loss                 | -0.00839   |\n",
      "|    n_updates            | 2510       |\n",
      "|    policy_gradient_loss | -0.00706   |\n",
      "|    std                  | 0.48       |\n",
      "|    value_loss           | 6.85e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 4478     |\n",
      "|    total_timesteps | 2540160  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560320, episode_reward=-0.19 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.186      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009424299 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.6       |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.00075     |\n",
      "|    loss                 | -0.00505    |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    std                  | 0.476       |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 4508     |\n",
      "|    total_timesteps | 2560320  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580480, episode_reward=-0.12 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.125      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008576611 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.3       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.000716    |\n",
      "|    loss                 | -0.00508    |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    std                  | 0.474       |\n",
      "|    value_loss           | 5.53e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 568      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 4537     |\n",
      "|    total_timesteps | 2580480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600640, episode_reward=-0.13 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1.26e+03  |\n",
      "|    mean_reward          | -0.132    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2600640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0082137 |\n",
      "|    clip_fraction        | 0.0571    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -22.1     |\n",
      "|    explained_variance   | 0.112     |\n",
      "|    learning_rate        | 0.000682  |\n",
      "|    loss                 | -0.00726  |\n",
      "|    n_updates            | 2570      |\n",
      "|    policy_gradient_loss | -0.00447  |\n",
      "|    std                  | 0.473     |\n",
      "|    value_loss           | 0.000106  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.35    |\n",
      "| time/              |          |\n",
      "|    fps             | 569      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 4567     |\n",
      "|    total_timesteps | 2600640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620800, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009748158 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.9       |\n",
      "|    explained_variance   | 0.092       |\n",
      "|    learning_rate        | 0.000649    |\n",
      "|    loss                 | -0.0083     |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    std                  | 0.47        |\n",
      "|    value_loss           | 0.000147    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 4596     |\n",
      "|    total_timesteps | 2620800  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640960, episode_reward=-0.13 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.125      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009358283 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.7       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.000615    |\n",
      "|    loss                 | -0.00752    |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    std                  | 0.467       |\n",
      "|    value_loss           | 2.22e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 570      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 4626     |\n",
      "|    total_timesteps | 2640960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2661120, episode_reward=-0.17 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.169      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2661120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009924479 |\n",
      "|    clip_fraction        | 0.0582      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -21.4       |\n",
      "|    explained_variance   | 0.0529      |\n",
      "|    learning_rate        | 0.000582    |\n",
      "|    loss                 | -0.00917    |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    std                  | 0.465       |\n",
      "|    value_loss           | 0.000201    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 4655     |\n",
      "|    total_timesteps | 2661120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2681280, episode_reward=-0.18 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.181       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2681280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076402184 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -21.3        |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.000548     |\n",
      "|    loss                 | -0.00642     |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    std                  | 0.465        |\n",
      "|    value_loss           | 7.04e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.33    |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 4685     |\n",
      "|    total_timesteps | 2681280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2701440, episode_reward=-0.17 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.173       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2701440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073416615 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -21.1        |\n",
      "|    explained_variance   | 0.352        |\n",
      "|    learning_rate        | 0.000514     |\n",
      "|    loss                 | -0.00819     |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -0.00476     |\n",
      "|    std                  | 0.461        |\n",
      "|    value_loss           | 7.87e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.33    |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 4715     |\n",
      "|    total_timesteps | 2701440  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2721600, episode_reward=-0.19 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.193      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2721600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007267069 |\n",
      "|    clip_fraction        | 0.0257      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.8       |\n",
      "|    explained_variance   | 0.0963      |\n",
      "|    learning_rate        | 0.000481    |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    std                  | 0.458       |\n",
      "|    value_loss           | 4.74e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 4744     |\n",
      "|    total_timesteps | 2721600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2741760, episode_reward=-0.15 +/- 0.07\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.15       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2741760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008057657 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.5       |\n",
      "|    explained_variance   | 0.0854      |\n",
      "|    learning_rate        | 0.000447    |\n",
      "|    loss                 | -0.00604    |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    std                  | 0.456       |\n",
      "|    value_loss           | 0.000222    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 4774     |\n",
      "|    total_timesteps | 2741760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2761920, episode_reward=-0.14 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.142      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2761920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007045407 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.3       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.000414    |\n",
      "|    loss                 | -0.00778    |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    std                  | 0.454       |\n",
      "|    value_loss           | 8.31e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 574      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 4803     |\n",
      "|    total_timesteps | 2761920  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2782080, episode_reward=-0.16 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.156      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006833655 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.2       |\n",
      "|    explained_variance   | 0.067       |\n",
      "|    learning_rate        | 0.00038     |\n",
      "|    loss                 | -0.00654    |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 0.000115    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 575      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 4833     |\n",
      "|    total_timesteps | 2782080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2802240, episode_reward=-0.15 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.153       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069897836 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -20.1        |\n",
      "|    explained_variance   | 0.148        |\n",
      "|    learning_rate        | 0.000346     |\n",
      "|    loss                 | -0.0056      |\n",
      "|    n_updates            | 2770         |\n",
      "|    policy_gradient_loss | -0.0034      |\n",
      "|    std                  | 0.452        |\n",
      "|    value_loss           | 5.83e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 4862     |\n",
      "|    total_timesteps | 2802240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2822400, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.14       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2822400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006664691 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20         |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.000313    |\n",
      "|    loss                 | -0.00689    |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    std                  | 0.45        |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 4892     |\n",
      "|    total_timesteps | 2822400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2842560, episode_reward=-0.15 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.15       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2842560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004415011 |\n",
      "|    clip_fraction        | 0.00557     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.9       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.000279    |\n",
      "|    loss                 | -0.000295   |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.00273    |\n",
      "|    std                  | 0.45        |\n",
      "|    value_loss           | 8.1e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 4921     |\n",
      "|    total_timesteps | 2842560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2862720, episode_reward=-0.14 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2862720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004893963 |\n",
      "|    clip_fraction        | 0.00827     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.8       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.000246    |\n",
      "|    loss                 | -0.00666    |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 7.9e-06     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 4951     |\n",
      "|    total_timesteps | 2862720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2882880, episode_reward=-0.20 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.196       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2882880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040132515 |\n",
      "|    clip_fraction        | 0.00577      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.6        |\n",
      "|    explained_variance   | 0.473        |\n",
      "|    learning_rate        | 0.000212     |\n",
      "|    loss                 | -0.00612     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    std                  | 0.447        |\n",
      "|    value_loss           | 8.38e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 578      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 4980     |\n",
      "|    total_timesteps | 2882880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2903040, episode_reward=-0.22 +/- 0.09\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.219       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2903040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034057065 |\n",
      "|    clip_fraction        | 0.00232      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.5        |\n",
      "|    explained_variance   | 0.236        |\n",
      "|    learning_rate        | 0.000178     |\n",
      "|    loss                 | -0.00616     |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    std                  | 0.447        |\n",
      "|    value_loss           | 1.96e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 5010     |\n",
      "|    total_timesteps | 2903040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2923200, episode_reward=-0.13 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.128       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2923200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014346705 |\n",
      "|    clip_fraction        | 6.94e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.5        |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | -0.0034      |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 0.446        |\n",
      "|    value_loss           | 8.33e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 5039     |\n",
      "|    total_timesteps | 2923200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2943360, episode_reward=-0.19 +/- 0.08\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.194       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2943360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010895912 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.4        |\n",
      "|    explained_variance   | 0.229        |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | -0.00295     |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 0.446        |\n",
      "|    value_loss           | 2.56e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 5069     |\n",
      "|    total_timesteps | 2943360  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2963520, episode_reward=-0.13 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.135       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2963520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003989904 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.4        |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | -0.00239     |\n",
      "|    n_updates            | 2930         |\n",
      "|    policy_gradient_loss | -0.000854    |\n",
      "|    std                  | 0.445        |\n",
      "|    value_loss           | 2.33e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 5098     |\n",
      "|    total_timesteps | 2963520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2983680, episode_reward=-0.16 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | -0.165        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2983680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010952505 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -19.4         |\n",
      "|    explained_variance   | 0.204         |\n",
      "|    learning_rate        | 4.4e-05       |\n",
      "|    loss                 | -0.00113      |\n",
      "|    n_updates            | 2950          |\n",
      "|    policy_gradient_loss | -0.000423     |\n",
      "|    std                  | 0.445         |\n",
      "|    value_loss           | 2.43e-05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 581      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 5127     |\n",
      "|    total_timesteps | 2983680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3003840, episode_reward=-0.14 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.144       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3003840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.687288e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.3        |\n",
      "|    explained_variance   | 0.387        |\n",
      "|    learning_rate        | 1.04e-05     |\n",
      "|    loss                 | -0.000205    |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.000143    |\n",
      "|    std                  | 0.445        |\n",
      "|    value_loss           | 4.6e-06      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -1.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 582      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 5157     |\n",
      "|    total_timesteps | 3003840  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x15de3e0e4d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Big Action\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs')\n",
    "]),filename='logsBigA-Train')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelBigAct',\n",
    "    log_path='./eval_logsBigA/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012949443 |\n",
      "|    clip_fraction        | 0.00668      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -3.21        |\n",
      "|    learning_rate        | 0.00498      |\n",
      "|    loss                 | 0.00071      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000504    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00041      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 755      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031004709 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | -2.86        |\n",
      "|    learning_rate        | 0.00495      |\n",
      "|    loss                 | -0.00146     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00043     |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 1.26e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 727      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023125617 |\n",
      "|    clip_fraction        | 0.00618      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | -4.3         |\n",
      "|    learning_rate        | 0.00492      |\n",
      "|    loss                 | -0.00021     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000347    |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 5.36e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 714      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80640        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033466944 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | -4.25        |\n",
      "|    learning_rate        | 0.00488      |\n",
      "|    loss                 | 0.00131      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000623    |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 5.34e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 711      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029056023 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | -4.18        |\n",
      "|    learning_rate        | 0.00485      |\n",
      "|    loss                 | -0.00126     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000761    |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 1.98e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028542068 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | -4.21        |\n",
      "|    learning_rate        | 0.00482      |\n",
      "|    loss                 | -0.00157     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00033     |\n",
      "|    std                  | 0.936        |\n",
      "|    value_loss           | 1.12e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 708      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 141120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010886057 |\n",
      "|    clip_fraction        | 0.0037       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | -4.16        |\n",
      "|    learning_rate        | 0.00478      |\n",
      "|    loss                 | -0.000993    |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000249    |\n",
      "|    std                  | 0.922        |\n",
      "|    value_loss           | 5.26e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 707      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 161280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012754799 |\n",
      "|    clip_fraction        | 0.00541      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | -4.22        |\n",
      "|    learning_rate        | 0.00475      |\n",
      "|    loss                 | -0.000961    |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000332    |\n",
      "|    std                  | 0.91         |\n",
      "|    value_loss           | 3.78e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019236521 |\n",
      "|    clip_fraction        | 0.00646      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | -3.52        |\n",
      "|    learning_rate        | 0.00471      |\n",
      "|    loss                 | -0.00147     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000538    |\n",
      "|    std                  | 0.889        |\n",
      "|    value_loss           | 7.66e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 201600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006119399 |\n",
      "|    clip_fraction        | 0.000694     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | -3.89        |\n",
      "|    learning_rate        | 0.00468      |\n",
      "|    loss                 | -0.000565    |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -3.15e-05    |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 5.5e-10      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 706      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 221760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003612093 |\n",
      "|    clip_fraction        | 5.95e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | -4.21        |\n",
      "|    learning_rate        | 0.00465      |\n",
      "|    loss                 | 0.000555     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -3.84e-07    |\n",
      "|    std                  | 0.846        |\n",
      "|    value_loss           | 1.14e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 241920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047402503 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.46        |\n",
      "|    explained_variance   | -4.24        |\n",
      "|    learning_rate        | 0.00461      |\n",
      "|    loss                 | -0.00164     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000954    |\n",
      "|    std                  | 0.828        |\n",
      "|    value_loss           | 1.04e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 705      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 343      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045124525 |\n",
      "|    clip_fraction        | 0.00966      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | -4.12        |\n",
      "|    learning_rate        | 0.00458      |\n",
      "|    loss                 | 0.00162      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000679    |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 2.61e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 371      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282240, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 282240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021406626 |\n",
      "|    clip_fraction        | 0.00537      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.36        |\n",
      "|    explained_variance   | -4.34        |\n",
      "|    learning_rate        | 0.00455      |\n",
      "|    loss                 | -0.000936    |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000342    |\n",
      "|    std                  | 0.785        |\n",
      "|    value_loss           | 1.08e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 400      |\n",
      "|    total_timesteps | 282240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.26e+03      |\n",
      "|    mean_reward          | 0             |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 302400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094887475 |\n",
      "|    clip_fraction        | 0.00247       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.31         |\n",
      "|    explained_variance   | -4.09         |\n",
      "|    learning_rate        | 0.00451       |\n",
      "|    loss                 | -0.000468     |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.00012      |\n",
      "|    std                  | 0.774         |\n",
      "|    value_loss           | 1.02e-10      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 429      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322560, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 322560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015904314 |\n",
      "|    clip_fraction        | 0.00813      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.31        |\n",
      "|    explained_variance   | -4.12        |\n",
      "|    learning_rate        | 0.00448      |\n",
      "|    loss                 | 0.00102      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0004      |\n",
      "|    std                  | 0.767        |\n",
      "|    value_loss           | 1.63e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 457      |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342720, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 342720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031029081 |\n",
      "|    clip_fraction        | 0.00525      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.28        |\n",
      "|    explained_variance   | -4.23        |\n",
      "|    learning_rate        | 0.00445      |\n",
      "|    loss                 | -0.000598    |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000357    |\n",
      "|    std                  | 0.76         |\n",
      "|    value_loss           | 7.64e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 486      |\n",
      "|    total_timesteps | 342720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 362880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013279462 |\n",
      "|    clip_fraction        | 0.00514      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.29        |\n",
      "|    explained_variance   | -3.59        |\n",
      "|    learning_rate        | 0.00441      |\n",
      "|    loss                 | 0.000316     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00031     |\n",
      "|    std                  | 0.762        |\n",
      "|    value_loss           | 5.46e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 704      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383040, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003120291 |\n",
      "|    clip_fraction        | 0.00926     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.26       |\n",
      "|    explained_variance   | -4.04       |\n",
      "|    learning_rate        | 0.00438     |\n",
      "|    loss                 | -0.00132    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.000602   |\n",
      "|    std                  | 0.755       |\n",
      "|    value_loss           | 5.63e-10    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 544      |\n",
      "|    total_timesteps | 383040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403200, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 403200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041017774 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.26        |\n",
      "|    explained_variance   | -4.32        |\n",
      "|    learning_rate        | 0.00434      |\n",
      "|    loss                 | -0.00233     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000873    |\n",
      "|    std                  | 0.748        |\n",
      "|    value_loss           | 7.83e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 572      |\n",
      "|    total_timesteps | 403200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003664996 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | -1.94       |\n",
      "|    learning_rate        | 0.00431     |\n",
      "|    loss                 | -0.000389   |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.000954   |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 4.56e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 601      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443520, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 443520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032953746 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.17        |\n",
      "|    explained_variance   | -4.31        |\n",
      "|    learning_rate        | 0.00428      |\n",
      "|    loss                 | -0.00129     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.000986    |\n",
      "|    std                  | 0.715        |\n",
      "|    value_loss           | 1.56e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 630      |\n",
      "|    total_timesteps | 443520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463680, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038675948 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.14        |\n",
      "|    explained_variance   | -4.33        |\n",
      "|    learning_rate        | 0.00424      |\n",
      "|    loss                 | -0.00295     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 0.707        |\n",
      "|    value_loss           | 2.32e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 658      |\n",
      "|    total_timesteps | 463680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 483840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009845321 |\n",
      "|    clip_fraction        | 0.00864      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.17        |\n",
      "|    explained_variance   | -4.38        |\n",
      "|    learning_rate        | 0.00421      |\n",
      "|    loss                 | 0.000225     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000531    |\n",
      "|    std                  | 0.719        |\n",
      "|    value_loss           | 2.54e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 687      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002945783 |\n",
      "|    clip_fraction        | 0.0163      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | -4.12       |\n",
      "|    learning_rate        | 0.00418     |\n",
      "|    loss                 | -0.00234    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    std                  | 0.722       |\n",
      "|    value_loss           | 7.08e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 716      |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524160, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 524160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027757129 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.14        |\n",
      "|    explained_variance   | -4.73        |\n",
      "|    learning_rate        | 0.00414      |\n",
      "|    loss                 | -0.000425    |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 1.43e-07     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 745      |\n",
      "|    total_timesteps | 524160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042443043 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.14        |\n",
      "|    explained_variance   | -3.87        |\n",
      "|    learning_rate        | 0.00411      |\n",
      "|    loss                 | -0.00162     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.706        |\n",
      "|    value_loss           | 5.82e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 773      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564480, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 564480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001659063 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | -4.52       |\n",
      "|    learning_rate        | 0.00408     |\n",
      "|    loss                 | -0.00219    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.000777   |\n",
      "|    std                  | 0.683       |\n",
      "|    value_loss           | 2.96e-10    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 802      |\n",
      "|    total_timesteps | 564480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584640, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 584640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038574012 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.02        |\n",
      "|    explained_variance   | -2.69        |\n",
      "|    learning_rate        | 0.00404      |\n",
      "|    loss                 | -0.00341     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    std                  | 0.666        |\n",
      "|    value_loss           | 5.48e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 831      |\n",
      "|    total_timesteps | 584640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024790845 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.01        |\n",
      "|    explained_variance   | -4.02        |\n",
      "|    learning_rate        | 0.00401      |\n",
      "|    loss                 | -0.000158    |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000583    |\n",
      "|    std                  | 0.662        |\n",
      "|    value_loss           | 2.68e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 860      |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624960, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024723453 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | -4.81        |\n",
      "|    learning_rate        | 0.00398      |\n",
      "|    loss                 | -0.000452    |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00062     |\n",
      "|    std                  | 0.631        |\n",
      "|    value_loss           | 3.26e-10     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 888      |\n",
      "|    total_timesteps | 624960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645120, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 645120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035410312 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | -5.08        |\n",
      "|    learning_rate        | 0.00394      |\n",
      "|    loss                 | 0.000203     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    std                  | 0.638        |\n",
      "|    value_loss           | 5.46e-08     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 917      |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 665280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023749126 |\n",
      "|    clip_fraction        | 0.00935      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | -4.36        |\n",
      "|    learning_rate        | 0.00391      |\n",
      "|    loss                 | 0.000908     |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000349    |\n",
      "|    std                  | 0.643        |\n",
      "|    value_loss           | 4.11e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 948      |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Reward l2\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small', obs = 'xs', reward = '2a'),\n",
    "    lambda: tradingEng(paths2,action = 'small', obs = 'xs', reward = '2a')\n",
    "]),filename='logsL2-train')\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small-More-Trust', obs = 'xs', reward = '2a'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelL2',\n",
    "    log_path='./logs/eval_logsL2/ev',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=20160, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0806      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042122398 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.417       |\n",
      "|    learning_rate        | 0.00498      |\n",
      "|    loss                 | -0.0274      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 0.000769     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.411   |\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 20160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40320, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0959     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006711756 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.67       |\n",
      "|    explained_variance   | -0.0504     |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.0322     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    std                  | 0.916       |\n",
      "|    value_loss           | 2.38e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.395   |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 40320    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60480, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.105      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017662158 |\n",
      "|    clip_fraction        | 0.0809      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.0236      |\n",
      "|    learning_rate        | 0.00492     |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    std                  | 0.815       |\n",
      "|    value_loss           | 1.72e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.374   |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80640, episode_reward=-0.11 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.106      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80640       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014625925 |\n",
      "|    clip_fraction        | 0.0606      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.0462      |\n",
      "|    learning_rate        | 0.00488     |\n",
      "|    loss                 | -0.0264     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 1.57e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.358   |\n",
      "| time/              |          |\n",
      "|    fps             | 511      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100800, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0909     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023721876 |\n",
      "|    clip_fraction        | 0.0846      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.01       |\n",
      "|    explained_variance   | 0.016       |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.0238     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    std                  | 0.68        |\n",
      "|    value_loss           | 6.83e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.336   |\n",
      "| time/              |          |\n",
      "|    fps             | 521      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 100800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0987     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018850528 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.0221      |\n",
      "|    learning_rate        | 0.00482     |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 5.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.319   |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141120, episode_reward=-0.09 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0926     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014746396 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.0269      |\n",
      "|    learning_rate        | 0.00478     |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    std                  | 0.584       |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.285   |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 141120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=161280, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 161280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007621401 |\n",
      "|    clip_fraction        | 0.0377      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.0698      |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00296    |\n",
      "|    std                  | 0.53        |\n",
      "|    value_loss           | 1.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.259   |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009456141 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0384      |\n",
      "|    learning_rate        | 0.00471     |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    std                  | 0.491       |\n",
      "|    value_loss           | 2.51e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.231   |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=-0.10 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0986     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009415196 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.845      |\n",
      "|    explained_variance   | 0.081       |\n",
      "|    learning_rate        | 0.00468     |\n",
      "|    loss                 | -0.0153     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    std                  | 0.444       |\n",
      "|    value_loss           | 6.33e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.204   |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 201600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=221760, episode_reward=-0.11 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.111      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 221760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013109084 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.059       |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 0.404       |\n",
      "|    value_loss           | 4.8e-07     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.188   |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 410      |\n",
      "|    total_timesteps | 221760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.10 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0989    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 241920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01115942 |\n",
      "|    clip_fraction        | 0.0737     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.167      |\n",
      "|    learning_rate        | 0.00461    |\n",
      "|    loss                 | -0.00744   |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00559   |\n",
      "|    std                  | 0.354      |\n",
      "|    value_loss           | 1.52e-07   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.173   |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 452      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262080, episode_reward=-0.09 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0906     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 262080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014491947 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | 0.031       |\n",
      "|    learning_rate        | 0.00458     |\n",
      "|    loss                 | -0.0106     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    std                  | 0.307       |\n",
      "|    value_loss           | 2.35e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.161   |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 488      |\n",
      "|    total_timesteps | 262080   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add ent\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small', obs = 'xs')\n",
    "]),filename='logs-TrainEnt')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelEnt',\n",
    "    log_path='./logs/eval_logs/evEnt',\n",
    "    eval_freq=252*8*5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*2*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*4*5, normalize_advantage=True, gamma = 0.9, verbose = 1,ent_coef=0.01) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations 0.7\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "with open(\"0.7Corr1Half.pkl\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "\n",
    "# Load Paths\n",
    "with open(\"0.7Corr2Half.pkl\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "\n",
    "with open(\"0.7CorrTest.pkl\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n",
    "\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small', obs = 'xs')\n",
    "]),filename='logs07-train')\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_model07',\n",
    "    log_path='./logs/eval_logs07',\n",
    "    eval_freq=252*8*5*20,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0015), policy_kwargs=policy_kwargs, n_steps=252*8*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=1e7, log_interval=2, callback=eval_callback) \n",
    "\n",
    "# Correlations -0.7\n",
    "#t = start_and_release(paths1,action='small-More-Trust', obs = 'auto')\n",
    "with open(\"NegCorrpt0.pkl\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"NegCorrpt2.pkl\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "# Load Paths\n",
    "with open(\"NegCorrpt4.pkl\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"NegCorrpt3.pkl\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "with open(\"n0.7CorrTest.pkl\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n",
    "\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'small', obs = 'xs'),\n",
    "    lambda: tradingEng(paths2,action = 'small', obs = 'xs')\n",
    "]),filename='logsm07-train')\n",
    "\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'small', obs = 'xs'),\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/best_modelm07',\n",
    "    log_path='./logs/eval_logsm07',\n",
    "    eval_freq=252*8*5*20,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 8\n",
    ")\n",
    "\n",
    "# Instantiate the agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128,64,64,64,64,36,18], vf=[512,512,256,128,64,64,64,64,36,18], optimizers_class = th.optim.Adam, log_std_init = 0.005)) #\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*4*5, learning_rate=linear_schedule(0.0015), policy_kwargs=policy_kwargs, n_steps=252*8*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=1e7, log_interval=2, callback=eval_callback) \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
