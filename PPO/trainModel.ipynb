{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "## Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Paths\n",
    "with open(\"../ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"../ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths2 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"../ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"../ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"../ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear LR schedule, see SB3 Documentation at https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "## Policy Kwargs\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128], vf=[512,512,256,128], optimizers_class = th.optim.Adam)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabellebyman/opt/anaconda3/envs/exjobb/lib/python3.13/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=50400, episode_reward=-0.10 +/- 0.06\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.26e+03 |\n",
      "|    mean_reward     | -0.101   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50400    |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Training section  #\n",
    "#####################\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs', reward='1b'), # <- Set action and obs\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs', reward='1b')  # <- Set action and obs\n",
    "]),filename='logs-xsbig')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs', reward='1b'),    # <- Set action and obs\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/models/xsbig_best', # <- Remember to name best model save path\n",
    "    log_path='./logs/eval_logs/xsbig', # Remeber to name eval data path\n",
    "    eval_freq=252*20*5, \n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*10*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*20*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "model.save('./logs/models/xsbig_final') # <- Save final model in case we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
