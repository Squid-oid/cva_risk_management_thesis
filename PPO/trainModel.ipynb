{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import gymnasium as gym\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "import torch as th\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "import pickle\n",
    "\n",
    "## Import Our environment\n",
    "from dev_env import tradingEng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Paths\n",
    "with open(\"ZeroCorrFrs1Half\",\"rb\") as fp:\n",
    "    paths1 = pickle.load(fp)\n",
    "with open(\"ZeroCorrFrs2Half\",\"rb\") as fp:\n",
    "    paths1 = paths1 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrSnd1Half\",\"rb\") as fp:\n",
    "    paths2 = pickle.load(fp)\n",
    "with open(\"ZeroCorrSnd2Half\",\"rb\") as fp:\n",
    "    paths2 = paths2 + pickle.load(fp)\n",
    "\n",
    "with open(\"ZeroCorrTest\",\"rb\") as fp:\n",
    "    paths_ev = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear LR schedule, see SB3 Documentation at https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "## Policy Kwargs\n",
    "policy_kwargs = dict(activation_fn=th.nn.LeakyReLU,\n",
    "                     net_arch=dict(pi=[512,512,256,128], vf=[512,512,256,128], optimizers_class = th.optim.Adam)) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=30240, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.26e+03 |\n",
      "|    mean_reward     | -0.0789  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30240    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60480, episode_reward=-0.08 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0841     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007330171 |\n",
      "|    clip_fraction        | 0.0772      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.7       |\n",
      "|    explained_variance   | -2.03       |\n",
      "|    learning_rate        | 0.00495     |\n",
      "|    loss                 | -0.00773    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0052      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.317   |\n",
      "| time/              |          |\n",
      "|    fps             | 671      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 60480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90720, episode_reward=-0.08 +/- 0.04\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0821    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 90720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00830939 |\n",
      "|    clip_fraction        | 0.0945     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -56.7      |\n",
      "|    explained_variance   | -0.0797    |\n",
      "|    learning_rate        | 0.0049     |\n",
      "|    loss                 | -0.0055    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00475   |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 4.36e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120960, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.068      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008135615 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.7       |\n",
      "|    explained_variance   | -0.0268     |\n",
      "|    learning_rate        | 0.00485     |\n",
      "|    loss                 | -0.00618    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 1.94e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.322   |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 120960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151200, episode_reward=-0.07 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0745      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 151200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075655263 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -56.7        |\n",
      "|    explained_variance   | -0.0129      |\n",
      "|    learning_rate        | 0.0048       |\n",
      "|    loss                 | -0.0046      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00461     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 1.67e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=181440, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0486     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008484222 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.6       |\n",
      "|    explained_variance   | 0.00796     |\n",
      "|    learning_rate        | 0.00475     |\n",
      "|    loss                 | -0.00301    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00286    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.35e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.308   |\n",
      "| time/              |          |\n",
      "|    fps             | 571      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 181440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211680, episode_reward=-0.08 +/- 0.05\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0753    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 211680     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00928805 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -56.5      |\n",
      "|    explained_variance   | 0.00405    |\n",
      "|    learning_rate        | 0.0047     |\n",
      "|    loss                 | -0.00631   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00394   |\n",
      "|    std                  | 0.992      |\n",
      "|    value_loss           | 2e-06      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=241920, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.05       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 241920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009608399 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.4       |\n",
      "|    explained_variance   | 0.00888     |\n",
      "|    learning_rate        | 0.00465     |\n",
      "|    loss                 | -0.00728    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 2.29e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.301   |\n",
      "| time/              |          |\n",
      "|    fps             | 572      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 422      |\n",
      "|    total_timesteps | 241920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272160, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0584      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100185545 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -56.3        |\n",
      "|    explained_variance   | 0.0314       |\n",
      "|    learning_rate        | 0.0046       |\n",
      "|    loss                 | -0.00546     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00548     |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 1.71e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=302400, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0444     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010524778 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.2       |\n",
      "|    explained_variance   | 0.0419      |\n",
      "|    learning_rate        | 0.00455     |\n",
      "|    loss                 | -0.00753    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 1.36e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.302   |\n",
      "| time/              |          |\n",
      "|    fps             | 573      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 302400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332640, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0623    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 332640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32302254 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -56.2      |\n",
      "|    explained_variance   | 0.0612     |\n",
      "|    learning_rate        | 0.0045     |\n",
      "|    loss                 | -0.00674   |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00546   |\n",
      "|    std                  | 0.986      |\n",
      "|    value_loss           | 2.14e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=362880, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0543     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021577213 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.2       |\n",
      "|    explained_variance   | 0.0664      |\n",
      "|    learning_rate        | 0.00445     |\n",
      "|    loss                 | 0.00118     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 1.16e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.297   |\n",
      "| time/              |          |\n",
      "|    fps             | 576      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 629      |\n",
      "|    total_timesteps | 362880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393120, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.045      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 393120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010147646 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -56.2       |\n",
      "|    explained_variance   | 0.0562      |\n",
      "|    learning_rate        | 0.0044      |\n",
      "|    loss                 | -0.00797    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 1.85e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=423360, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0507    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 423360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00970206 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -55.9      |\n",
      "|    explained_variance   | 0.0717     |\n",
      "|    learning_rate        | 0.00434    |\n",
      "|    loss                 | -0.00685   |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00444   |\n",
      "|    std                  | 0.98       |\n",
      "|    value_loss           | 1.47e-06   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.299   |\n",
      "| time/              |          |\n",
      "|    fps             | 577      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 733      |\n",
      "|    total_timesteps | 423360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453600, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0494     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 453600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007662693 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.9       |\n",
      "|    explained_variance   | 0.0588      |\n",
      "|    learning_rate        | 0.00429     |\n",
      "|    loss                 | -0.00722    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00549    |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 7.35e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=483840, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0494     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 483840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009806551 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.8       |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.00424     |\n",
      "|    loss                 | -0.00511    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 8.31e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 835      |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514080, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0559     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 514080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007910083 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.7       |\n",
      "|    explained_variance   | 0.0832      |\n",
      "|    learning_rate        | 0.00419     |\n",
      "|    loss                 | -0.00709    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 1.31e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=544320, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0438      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 544320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102031715 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -55.6        |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.00414      |\n",
      "|    loss                 | -0.00462     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 1.1e-06      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.291   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 939      |\n",
      "|    total_timesteps | 544320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574560, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.26e+03   |\n",
      "|    mean_reward          | -0.0585    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 574560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01906465 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -55.6      |\n",
      "|    explained_variance   | 0.066      |\n",
      "|    learning_rate        | 0.00409    |\n",
      "|    loss                 | -0.0019    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.00127   |\n",
      "|    std                  | 0.971      |\n",
      "|    value_loss           | 3.76e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=604800, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0459     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 604800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009747056 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.4       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00404     |\n",
      "|    loss                 | -0.00662    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 6.84e-07    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.286   |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1043     |\n",
      "|    total_timesteps | 604800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635040, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0476     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 635040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008751389 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.3       |\n",
      "|    explained_variance   | 0.0711      |\n",
      "|    learning_rate        | 0.00399     |\n",
      "|    loss                 | -0.00556    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 8.95e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=665280, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.0461     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 665280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009209889 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.2       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00394     |\n",
      "|    loss                 | -0.00632    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    std                  | 0.963       |\n",
      "|    value_loss           | 1.12e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.294   |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 1146     |\n",
      "|    total_timesteps | 665280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695520, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.048      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 695520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010512443 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55.2       |\n",
      "|    explained_variance   | 0.0973      |\n",
      "|    learning_rate        | 0.00389     |\n",
      "|    loss                 | -0.00291    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 3.13e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=725760, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.26e+03     |\n",
      "|    mean_reward          | -0.0536      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 725760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087628225 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -55.1        |\n",
      "|    explained_variance   | 0.0672       |\n",
      "|    learning_rate        | 0.00384      |\n",
      "|    loss                 | -0.00525     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 1.82e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | -0.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 579      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1252     |\n",
      "|    total_timesteps | 725760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 1259.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.26e+03    |\n",
      "|    mean_reward          | -0.046      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 756000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008895234 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -55         |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.00379     |\n",
      "|    loss                 | -0.00714    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00486    |\n",
      "|    std                  | 0.957       |\n",
      "|    value_loss           | 1.61e-06    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Training section  #\n",
    "#####################\n",
    "envs = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths1,action = 'big', obs = 'xs'), # <- Set action and obs\n",
    "    lambda: tradingEng(paths2,action = 'big', obs = 'xs')  # <- Set action and obs\n",
    "]),filename='logs-xsbig')\n",
    "ev_env = VecMonitor(DummyVecEnv([\n",
    "    lambda: tradingEng(paths_ev,action = 'big', obs = 'xs'),    # <- Set action and obs\n",
    "]))\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    ev_env,\n",
    "    best_model_save_path='./logs/models/xsbig_best', # <- Remember to name best model save path\n",
    "    log_path='./logs/eval_logs/xsbig', # Remeber to name eval data path\n",
    "    eval_freq=252*20*5, \n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose = True,\n",
    "    n_eval_episodes = 16\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", envs, batch_size = 252*10*5, learning_rate=linear_schedule(0.005), policy_kwargs=policy_kwargs, n_steps=252*20*5, normalize_advantage=True, gamma = 0.9, verbose = 1) \n",
    "\n",
    "model.learn(total_timesteps=3e6, log_interval=2, callback=eval_callback) \n",
    "model.save('./logs/models/xsbig_final') # <- Save final model in case we need it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
